{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual network\n",
    "\n",
    "This notebook trains the ResNet-20 based on\n",
    "\n",
    "\n",
    "After training, model is serialized and uploaded to W&B project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "def load_data(run : wandb.sdk.wandb_run.Run) -> pathlib.Path:\n",
    "    \"\"\"\n",
    "    Unpacks data from an artifact into a folder and returns the path to the folder.\n",
    "    \"\"\"\n",
    "\n",
    "    artifact_name = f\"letters_splits\"\n",
    "    artifact = run.use_artifact(f\"master-thesis/{artifact_name}:latest\")\n",
    "    artifact_dir = pathlib.Path(f\"./artifacts/{artifact.name.replace(':', '-')}\").resolve()\n",
    "    if not artifact_dir.exists():\n",
    "        artifact_dir = artifact.download()\n",
    "        artifact_dir = pathlib.Path(artifact_dir).resolve()\n",
    "        for split_file in artifact_dir.iterdir():\n",
    "            if split_file.name.endswith(\".tar.gz\"):\n",
    "                split = split_file.name.replace(\".tar.gz\", \"\")\n",
    "                shutil.unpack_archive(split_file, artifact_dir / split, format=\"gztar\")\n",
    "                \n",
    "    return [ artifact_dir / split for split in [\"train\", \"test\", \"val\"]]\n",
    "\n",
    "def get_number_of_classes(ds: tf.data.Dataset) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of classes in a dataset.\n",
    "    \"\"\"\n",
    "    return len(ds.class_names)\n",
    "\n",
    "def create_tf_dataset(split_path: pathlib.Path, batch_size: int = 32):\n",
    "    \"\"\"\n",
    "    Creates a tf dataset from path containing a folder for each class.\n",
    "    \"\"\"\n",
    "    ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        split_path, \n",
    "        image_size=(32,32), \n",
    "        batch_size=batch_size,\n",
    "        color_mode='grayscale',\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "def preprocess_dataset(ds: tf.data.Dataset, cache: bool = True) -> tf.data.Dataset :\n",
    "    ds = ds.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y)) # normalize\n",
    "    if cache:\n",
    "        ds = ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def calculate_model_size_on_disk(path: str) -> int:\n",
    "    return pathlib.Path(path).stat().st_size    \n",
    "\n",
    "def calculate_model_num_parameters(model: tf.keras.Model) -> int:\n",
    "    return model.count_params()\n",
    "\n",
    "def calculate_model_flops(model: tf.keras.Model) -> str:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([9, 12, 18, 9]) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1mi1hdmf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fresh-sun-28</strong>: <a href=\"https://wandb.ai/gratkadlafana/master-thesis/runs/1mi1hdmf\" target=\"_blank\">https://wandb.ai/gratkadlafana/master-thesis/runs/1mi1hdmf</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230111_225152-1mi1hdmf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1mi1hdmf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Wiktor\\code\\master-thesis\\notebooks\\wandb\\run-20230111_225503-fjn7v5z6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/gratkadlafana/master-thesis/runs/fjn7v5z6\" target=\"_blank\">ResNetv1</a></strong> to <a href=\"https://wandb.ai/gratkadlafana/master-thesis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "defaults = dict(\n",
    "    batch_size=32*4,\n",
    "    epochs=100,    \n",
    "    optimizer=\"sgd\",\n",
    "    learning_rate=0.01,\n",
    "    momentum=0.9,\n",
    ")\n",
    "\n",
    "RESNET_DEPTHS = [3, 4, 6, 3]\n",
    "MODEL_NAME = f\"ResNet-{sum(RESNET_DEPTHS) + 2}\"\n",
    "run = wandb.init(project=\"master-thesis\", job_type=\"training\", name=MODEL_NAME, config=defaults,)\n",
    "split_paths = load_data(run=run)\n",
    "\n",
    "# hyperparameters\n",
    "\n",
    "opt_name = wandb.config.optimizer\n",
    "lr = wandb.config.learning_rate\n",
    "momentum = wandb.config.momentum\n",
    "bs = wandb.config.batch_size\n",
    "epochs = wandb.config.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 446491 files belonging to 89 classes.\n",
      "Found 55773 files belonging to 89 classes.\n",
      "Found 55773 files belonging to 89 classes.\n",
      "There are 89 classes\n",
      "Training set has 3489 batches\n",
      "Test set has 436 batches\n",
      "Validation set has 436 batches\n"
     ]
    }
   ],
   "source": [
    "ds_train, ds_test, ds_val = [\n",
    "    create_tf_dataset(split_path, batch_size=bs) for split_path in split_paths\n",
    "    ]\n",
    "\n",
    "num_classes = len(ds_train.class_names)\n",
    "\n",
    "print(f\"There are {num_classes} classes\")\n",
    "print(f\"Training set has {len(ds_train)} batches\")\n",
    "print(f\"Test set has {len(ds_test)} batches\")\n",
    "print(f\"Validation set has {len(ds_val)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = preprocess_dataset(ds_train)\n",
    "ds_val = preprocess_dataset(ds_val)\n",
    "ds_test = preprocess_dataset(ds_test, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
    "        super(ResidualBlock, self).__init__(**kwargs)\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            tf.keras.layers.Conv2D(filters, strides=strides, kernel_size=3,  padding=\"same\", use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            tf.keras.layers.Conv2D(filters, strides=1, kernel_size=3, padding=\"same\", use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization()\n",
    "        ]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                tf.keras.layers.Conv2D(filters, strides=strides, kernel_size=1, padding=\"same\", use_bias=False),\n",
    "                tf.keras.layers.BatchNormalization()\n",
    "            ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.main_layers:\n",
    "            x = layer(x)\n",
    "        skip_x = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_x = layer(skip_x)\n",
    "        return self.activation(x + skip_x)\n",
    "\n",
    "class ResNet(tf.keras.Model):\n",
    "    def __init__(self, block_design = [3,4,6,3], input_shape=[32, 32, 1], num_classes=88, **kwargs):\n",
    "        super(ResNet, self).__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "        self.input_conv = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, input_shape=input_shape, padding=\"same\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation(\"relu\"),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"),\n",
    "        ])\n",
    "        self.resnet_blocks = tf.keras.models.Sequential()\n",
    "        prev_filters = 64\n",
    "        for filters in np.repeat(np.array([64, 128, 256, 512]), block_design):\n",
    "            strides = 1 if filters == prev_filters else 2\n",
    "            self.resnet_blocks.add(ResidualBlock(filters, strides=strides))\n",
    "            prev_filters = filters\n",
    "        self.avg_pool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.input_conv(inputs)\n",
    "        x = self.resnet_blocks(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "def get_resnet_model(input_shape, block_design, num_classes) -> tf.keras.Sequential:\n",
    "    model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, input_shape=input_shape, padding=\"same\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation(\"relu\"),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"),\n",
    "        ])\n",
    "    resnet_blocks = tf.keras.models.Sequential()\n",
    "    for filters in np.repeat(np.array([64, 128, 256, 512]), block_design):\n",
    "            strides = 1 if filters == prev_filters else 2\n",
    "            resnet_blocks.add(ResidualBlock(filters, strides=strides))\n",
    "            prev_filters = filters\n",
    "    model.add(resnet_blocks)\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_resnet_model(input_shape=[32, 32, 1], block_design=RESNET_DEPTHS, num_classes=num_classes)\n",
    "opt = tf.keras.optimizers.get({\n",
    "    'class_name': wandb.config.optimizer,\n",
    "    'config': {\n",
    "        'learning_rate': lr,\n",
    "        'momentum': momentum\n",
    "    }\n",
    "})\n",
    "\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "wandb_callback = wandb.keras.WandbCallback(\n",
    "    save_model=False,\n",
    "    compute_flops=True,\n",
    ")\n",
    "\n",
    "# save the best model\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=f\"./artifacts/{MODEL_NAME}.h5\",\n",
    "    save_weights_only=False,\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=ds_val,\n",
    "    callbacks=[wandb_callback, checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "epochs = range(1, len(history.history[\"loss\"]) + 1)\n",
    "ax.plot(epochs, history.history[\"accuracy\"], label=\"accuracy\")\n",
    "ax.plot(epochs, history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate model size on disk, flops and number of parameters\n",
    "\n",
    "flops = wandb.run.summary[\"GFLOPs\"]\n",
    "disk_size = calculate_model_size_on_disk(f\"./artifacts/{MODEL_NAME}.h5\")\n",
    "num_parameters = calculate_model_num_parameters(model)\n",
    "\n",
    "# evaluate model on ds_test and log to wandb\n",
    "test_loss, test_acc = model.evaluate(ds_test)\n",
    "\n",
    "wandb.log({\n",
    "    \"test loss\": test_loss, \n",
    "    \"test accuracy\": test_acc, \n",
    "    \"number of parameters\": num_parameters,\n",
    "    \"disk size\": disk_size, \n",
    "    \"model flops\": flops\n",
    "    })\n",
    "\n",
    "\n",
    "diacritics = {\n",
    "    62: \"ą\",\n",
    "    63: \"ć\",\n",
    "    64: \"ę\",\n",
    "    65: \"ł\",\n",
    "    66: \"ń\",\n",
    "    67: \"ó\",\n",
    "    68: \"ś\",\n",
    "    69: \"ź\",\n",
    "    70: \"ż\",\n",
    "    71: \"Ą\",\n",
    "    72: \"Ć\",\n",
    "    73: \"Ę\",\n",
    "    74: \"Ł\",\n",
    "    75: \"Ń\",\n",
    "    76: \"Ó\",\n",
    "    77: \"Ś\",\n",
    "    78: \"Ź\",\n",
    "    79: \"Ż\"\n",
    "}\n",
    "\n",
    "# log test accuracy on these classes separately to wandb\n",
    "\n",
    "diacritics_acc = {}\n",
    "for diacritic_label in diacritics.keys():\n",
    "    ds_test_diacritic = ds_test.filter(lambda x, y: y == diacritic_label)\n",
    "    test_loss, test_acc = model.evaluate(ds_test_diacritic)\n",
    "    diacritics_acc[diacritic_label] = {\n",
    "        \"loss\": test_loss,\n",
    "        \"accuracy\": test_acc,\n",
    "        \"label\": diacritics[diacritic_label],\n",
    "    }\n",
    "\n",
    "wandb.log(diacritics_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save artifact to wandb\n",
    "artifact = wandb.Artifact(\n",
    "    name=MODEL_NAME,\n",
    "    type=\"model\"\n",
    ")\n",
    "\n",
    "# save best model to artifact\n",
    "artifact.add_file(f\"./artifacts/{MODEL_NAME}.h5\")\n",
    "run.log_artifact(artifact)\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df5f531b08a468c0a2a3591ee4fa2c1150ed1bbbe823daf85795b21510fb4a25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
