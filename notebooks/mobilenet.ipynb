{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better CNN model\n",
    "\n",
    "Here I explore very similar architectures to the baseline (CNN - Flatten - Dense layers)\n",
    "\n",
    "Hyperparameters experimented with:\n",
    "\n",
    "- number of CNN and dense layers\n",
    "- Dropout\n",
    "- Activation function\n",
    "- Other max pooling strategies\n",
    "- learning rate scheduling\n",
    "\n",
    "\n",
    "After training, model is serialized and uploaded to W&B project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import shutil\n",
    "import math\n",
    "from typing import List\n",
    "\n",
    "def load_data(run, artifact_name = f\"phcd_paper_splits_tfds\") -> List[tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Downloads datasets from a wandb artifact and loads them into a list of tf.data.Datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    artifact = run.use_artifact(f\"master-thesis/{artifact_name}:latest\")\n",
    "    artifact_dir = pathlib.Path(\n",
    "        f\"./artifacts/{artifact.name.replace(':', '-')}\"\n",
    "    ).resolve()\n",
    "    if not artifact_dir.exists():\n",
    "        artifact_dir = artifact.download()\n",
    "        artifact_dir = pathlib.Path(artifact_dir).resolve()\n",
    "\n",
    "    # if tf.__version__ minor is less than 10, use\n",
    "    # tf.data.experimental.load instead of tf.data.Dataset.load\n",
    "\n",
    "    if int(tf.__version__.split(\".\")[1]) < 10:\n",
    "        load_function = tf.data.experimental.load\n",
    "    else:\n",
    "        load_function = tf.data.Dataset.load\n",
    "    \n",
    "    output_list = []\n",
    "    for split in [\"train\", \"test\", \"val\"]:\n",
    "        ds = load_function(str(artifact_dir / split), compression=\"GZIP\")\n",
    "        output_list.append(ds)\n",
    "    \n",
    "    return output_list\n",
    "\n",
    "def get_number_of_classes(ds: tf.data.Dataset) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of classes in a dataset.\n",
    "    \"\"\"\n",
    "    labels_iterator= ds.map(lambda x, y: y).as_numpy_iterator()\n",
    "    labels = np.concatenate(list(labels_iterator))\n",
    "    return len(np.unique(labels))\n",
    "\n",
    "def preprocess_dataset(ds: tf.data.Dataset, batch_size: int, cache: bool = True) -> tf.data.Dataset:\n",
    "    ds = ds.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y))  # normalize\n",
    "    ds = ds.unbatch().batch(batch_size)\n",
    "    if cache:\n",
    "        ds = ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def calculate_model_size_on_disk(path: str) -> int:\n",
    "    return pathlib.Path(path).stat().st_size    \n",
    "\n",
    "def calculate_model_num_parameters(model: tf.keras.Model) -> int:\n",
    "    return model.count_params()\n",
    "\n",
    "def calculate_model_flops(summary) -> float:\n",
    "    # from run.summary get GFLOPs or GFLOPS whichever is available\n",
    "    if \"GFLOPs\" in summary:\n",
    "        return summary.get(\"GFLOPs\")\n",
    "    elif \"GFLOPS\" in summary:\n",
    "        return summary.get(\"GFLOPS\")\n",
    "\n",
    "\n",
    "class Terminate_slow_convergence(tf.keras.callbacks.Callback):\n",
    "    ACCURACY_THRESHOLD =  0.9\n",
    "    EPOCH = 6\n",
    "    MAX_VAL_ACCURACY_THRESHOLD = 0.995\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        # if model is not converging fast, stop training\n",
    "        if(logs.get('accuracy') < self.ACCURACY_THRESHOLD and epoch > self.EPOCH):\n",
    "            print(f\"Model accuracy is {logs.get('accuracy')} and is below {self.ACCURACY_THRESHOLD} at epoch {epoch}. Terminating training.\")\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "        # if model converged, stop training\n",
    "        #if(logs.get('val_accuracy') >= self.MAX_VAL_ACCURACY_THRESHOLD and epoch > 2):\n",
    "        #    self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Available devices: \", tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "def get_mobilenet_model(input_shape=(32,32,1), num_classes=89, mobilenet_version = 2 ) -> tf.keras.Sequential:\n",
    "    base_model = None\n",
    "\n",
    "    if mobilenet_version == 2:\n",
    "        base_model = MobileNetV2(include_top=False, input_shape=input_shape, weights=None, pooling = 'max')\n",
    "    else:\n",
    "        base_model = MobileNet(include_top=False, input_shape=input_shape, weights=None, pooling = 'max')\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.Dense(num_classes),\n",
    "        tf.keras.layers.Activation('softmax', dtype='float32', name='predictions') # this is needed for mixed precision training\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_name, config_defaults, artifact_name, num_classes=89):\n",
    "    print(\"Initializing wandb\")\n",
    "    with wandb.init(\n",
    "        project=\"master-thesis\",\n",
    "        job_type=\"training\",\n",
    "        name=model_name,\n",
    "        config=config_defaults,\n",
    "    ) as run:\n",
    "        ds_train, ds_test, ds_val = load_data(run, artifact_name)\n",
    "\n",
    "        num_classes = get_number_of_classes(ds_val)\n",
    "\n",
    "        ds_train = preprocess_dataset(ds_train, batch_size=wandb.config.batch_size)\n",
    "        ds_val = preprocess_dataset(ds_val, batch_size=wandb.config.batch_size)\n",
    "        ds_test = preprocess_dataset(ds_test, batch_size=wandb.config.batch_size, cache=False)\n",
    "        \n",
    "        print(f\"Model name: {model_name}\")\n",
    "        model = get_mobilenet_model(num_classes=num_classes, mobilenet_version=2)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "\n",
    "        # save the best model\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=f\"./artifacts/{model_name}.h5\",\n",
    "            save_weights_only=False,\n",
    "            monitor=\"val_accuracy\",\n",
    "            mode=\"max\",\n",
    "            save_best_only=True,\n",
    "        )\n",
    "\n",
    "        wandb_callback = wandb.keras.WandbCallback(\n",
    "            save_model=False,\n",
    "            compute_flops=True,\n",
    "        )\n",
    "\n",
    "        # learning rate scheduler\n",
    "        initial_learning_rate = wandb.config.epochs\n",
    "        epochs = wandb.config.epochs\n",
    "        decay = initial_learning_rate / epochs\n",
    "\n",
    "        def lr_time_based_decay(epoch, lr):\n",
    "            return lr * 1 / (1 + decay * epoch)\n",
    "\n",
    "        def lr_step_decay(epoch, lr):\n",
    "            drop_rate = 0.5\n",
    "            epochs_drop = 10.0\n",
    "            return initial_learning_rate * math.pow(drop_rate, math.floor(epoch/epochs_drop))\n",
    "\n",
    "        lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_time_based_decay)\n",
    "\n",
    "        slow_convergence_callback = Terminate_slow_convergence()\n",
    "        history = model.fit(\n",
    "            ds_train,\n",
    "            epochs=wandb.config.epochs,\n",
    "            validation_data=ds_val,\n",
    "            callbacks=[wandb_callback, checkpoint_callback, lr_scheduler],\n",
    "        )\n",
    "        print(\"Training finished\")\n",
    "\n",
    "        # calculate model size on disk, flops and number of parameters\n",
    "        flops = calculate_model_flops(wandb.run.summary)\n",
    "     \n",
    "        disk_size = calculate_model_size_on_disk(f\"./artifacts/{model_name}.h5\")\n",
    "        num_parameters = calculate_model_num_parameters(model)\n",
    "\n",
    "        # evaluate model on ds_test and log to wandb\n",
    "        test_loss, test_acc = model.evaluate(ds_test)\n",
    "\n",
    "        wandb.log({\n",
    "            \"test loss\": test_loss, \n",
    "            \"test accuracy\": test_acc, \n",
    "            \"number of parameters\": num_parameters,\n",
    "            \"disk size\": disk_size, \n",
    "            \"model flops\": flops\n",
    "            })\n",
    "        # save artifact to wandb\n",
    "        artifact = wandb.Artifact(name=model_name, type=\"model\")\n",
    "\n",
    "        # save best model to artifact\n",
    "        artifact.add_file(f\"./artifacts/{model_name}.h5\")\n",
    "        run.log_artifact(artifact)\n",
    "        run.finish()\n",
    "        print(\"Evaluation finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = dict(\n",
    "    batch_size=128,\n",
    "    epochs=100,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "train(\"mobilenet\", defaults, artifact_name=\"uppercase_splits_tfds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Jan 14 2023, 19:35:44) [GCC 11.3.0]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "d798f8ad6d6c53bff9e7c2dca27b60374f042a436c567b9cf87bf4fc98c22b9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
