{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRicHmH8vnLY"
   },
   "source": [
    "# Baseline network + pruning + quantization\n",
    "\n",
    "This notebook trains the baseline network with exact same architecture as the one in paper [Recognition of handwritten Latin characters with diacritics using CNN](https://journals.pan.pl/dlibra/publication/136210/edition/119099/content/bulletin-of-the-polish-academy-of-sciences-technical-sciences-recognition-of-handwritten-latin-characters-with-diacritics-using-cnn-lukasik-edyta-charytanowicz-malgorzata-milosz-marek-tokovarov-michail-kaczorowska-monika-czerwinski-dariusz-zientarski-tomasz-2021-69-no-1?language=en)\n",
    "\n",
    "Model architecture description:\n",
    "\n",
    "\"The architecture of the concrete CNN is shown in Fig. 2.\n",
    "The input is a 32x32 binarized matrix. The input is then prop-\n",
    "agated through 12 adaptable layers. First come two convolu-\n",
    "tional layers having 32 filters with the size of 3x3 and stride 1.\n",
    "Secondly, the output of the convolutional layer is fed to the\n",
    "ReLU function. The output is down-sampled using a max-pool-\n",
    "ing operation with a 2x2 stride. Next, the dropout technique is\n",
    "used with the coefficient 0.25. The four operations (two con-\n",
    "volutions, nonlinearity, max-pooling, and dropout) are repeated,\n",
    "using 64 filters for the convolutional layers. The output of the\n",
    "last layer is then flattened and fed through a fully connected\n",
    "layer with 256 neurons and ReLU nonlinearities, dropped out\n",
    "with the 0.25 coefficient, and a final output layer is fully con-\n",
    "nected with a Softmax activation function. The Adam optimizer\n",
    "and the cross-entropy loss function were used in the network. \n",
    "The output is a probability distribution over 89 classes.\"\n",
    "\n",
    "\n",
    "Additionally, this network was pruned and quantized after training.\n",
    "\n",
    "\n",
    "\n",
    "After training, model is serialized and uploaded to W&B project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard_plugin_profile in /usr/local/lib/python3.8/dist-packages (2.11.1)\n",
      "Requirement already satisfied: gviz-api>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard_plugin_profile) (1.10.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorboard_plugin_profile) (1.14.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard_plugin_profile) (3.19.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard_plugin_profile) (2.2.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard_plugin_profile) (65.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=0.11.15->tensorboard_plugin_profile) (2.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -U tensorboard_plugin_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XhFWPE1BvnLZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 12:33:27.021136: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import shutil\n",
    "import datetime\n",
    "import numpy as np\n",
    "import wandb\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def load_data(run, artifact_name = \"phcd_paper_splits_tfds\") -> List[tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Downloads datasets from a wandb artifact and loads them into a list of tf.data.Datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    artifact = run.use_artifact(f\"master-thesis/{artifact_name}:latest\")\n",
    "    artifact_dir = pathlib.Path(\n",
    "        f\"./artifacts/{artifact.name.replace(':', '-')}\"\n",
    "    ).resolve()\n",
    "    if not artifact_dir.exists():\n",
    "        artifact_dir = artifact.download()\n",
    "        artifact_dir = pathlib.Path(artifact_dir).resolve()\n",
    "\n",
    "    # if tf.__version__ minor is less than 10, use\n",
    "    # tf.data.experimental.load instead of tf.data.Dataset.load\n",
    "\n",
    "    if int(tf.__version__.split(\".\")[1]) < 10:\n",
    "        load_function = tf.data.experimental.load\n",
    "    else:\n",
    "        load_function = tf.data.Dataset.load\n",
    "    \n",
    "    output_list = []\n",
    "    for split in [\"train\", \"test\", \"val\"]:\n",
    "        ds = load_function(str(artifact_dir / split), compression=\"GZIP\")\n",
    "        output_list.append(ds)\n",
    "    \n",
    "    return output_list\n",
    "\n",
    "def get_number_of_classes(ds: tf.data.Dataset) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of classes in a dataset.\n",
    "    \"\"\"\n",
    "    labels_iterator= ds.map(lambda x, y: y).as_numpy_iterator()\n",
    "    labels = np.concatenate(list(labels_iterator))\n",
    "    return len(np.unique(labels))\n",
    "\n",
    "def get_number_of_examples(ds: tf.data.Dataset) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of examples in a dataset.\n",
    "    \"\"\"\n",
    "    return sum(1 for _ in ds)\n",
    "\n",
    "def preprocess_dataset(ds: tf.data.Dataset, batch_size: int, cache: bool = True) -> tf.data.Dataset:\n",
    "    ds = ds.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y))  # normalize\n",
    "    ds = ds.unbatch().batch(batch_size)\n",
    "    if cache:\n",
    "        ds = ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def calculate_model_size_on_disk(path: str) -> int:\n",
    "    return pathlib.Path(path).stat().st_size    \n",
    "\n",
    "def calculate_model_num_parameters(model: tf.keras.Model) -> int:\n",
    "    return model.count_params()\n",
    "\n",
    "def calculate_model_flops(summary) -> float:\n",
    "    # from run.summary get GFLOPs or GFLOPS whichever is available\n",
    "    if \"GFLOPs\" in summary.keys():\n",
    "        return summary.get(\"GFLOPs\")\n",
    "    elif \"GFLOPS\" in summary.keys():\n",
    "        return summary.get(\"GFLOPS\")\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rgJaTJFlvnLb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Available devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 12:33:28.387356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-29 12:33:28.389668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-29 12:33:28.389876: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Available devices: \", tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgratkadlafana\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20230129_123329-tvhh7ub2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/gratkadlafana/master-thesis/runs/tvhh7ub2\" target=\"_blank\">baseline-pruned-numbers</a></strong> to <a href=\"https://wandb.ai/gratkadlafana/master-thesis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/gratkadlafana/master-thesis\" target=\"_blank\">https://wandb.ai/gratkadlafana/master-thesis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/gratkadlafana/master-thesis/runs/tvhh7ub2\" target=\"_blank\">https://wandb.ai/gratkadlafana/master-thesis/runs/tvhh7ub2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact phcd_paper_splits_tfds:latest, 163.87MB. 27 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   27 of 27 files downloaded.  \n",
      "Done. 0:0:0.0\n",
      "2023-01-29 12:33:31.855235: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 12:33:31.855977: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-29 12:33:31.856297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-29 12:33:31.856576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-29 12:33:32.342387: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-29 12:33:32.342754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-29 12:33:32.342768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-01-29 12:33:32.342991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-29 12:33:32.343032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2075 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 SUPER, pci bus id: 0000:26:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "defaults = dict(\n",
    "    batch_size=32*2,\n",
    "    epochs=30,    \n",
    "    optimizer=\"adam\"\n",
    ")\n",
    "\n",
    "model_name = \"baseline-pruned-numbers\"\n",
    "\n",
    "artifact_name = \"phcd_paper_splits_tfds\"\n",
    "run = wandb.init(project=\"master-thesis\", job_type=\"training\", name=model_name, config=defaults, tags=[artifact_name])\n",
    "    \n",
    "# hyperparameters\n",
    "epochs = wandb.config.epochs\n",
    "bs = wandb.config.batch_size\n",
    "\n",
    "ds_train, ds_test, ds_val = load_data(run, artifact_name=artifact_name)\n",
    "\n",
    "num_classes = get_number_of_classes(ds_val)\n",
    "\n",
    "ds_train = preprocess_dataset(ds_train, batch_size=bs)\n",
    "ds_val = preprocess_dataset(ds_val, batch_size=bs)\n",
    "ds_test = preprocess_dataset(ds_test, batch_size=bs, cache=False)\n",
    "\n",
    "'''\n",
    "The architecture of the concrete CNN is shown in Fig. 2.\n",
    "The input is a 32x32 binarized matrix. \n",
    "The input is then propagated through 12 adaptable layers. \n",
    "First come two convolutional layers having 32 filters with the size of 3x3 and stride 1.\n",
    "Secondly, the output of the convolutional layer is fed to the\n",
    "ReLU function. The output is down-sampled using a max-pool-\n",
    "ing operation with a 2x2 stride. Next, the dropout technique is\n",
    "used with the coefficient 0.25. The four operations (two con-\n",
    "volutions, nonlinearity, max-pooling, and dropout) are repeated,\n",
    "using 64 filters for the convolutional layers. The output of the\n",
    "last layer is then flattened and fed through a fully connected\n",
    "layer with 256 neurons and ReLU nonlinearities, dropped out\n",
    "with the 0.25 coefficient, and a final output layer is fully con-\n",
    "nected with a Softmax activation function. The Adam optimizer\n",
    "and the cross-entropy loss function were used in the network\n",
    "[24]. The output is a probability distribution over 89 classes.\n",
    "'''\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.InputLayer(input_shape=(32, 32, 1)),\n",
    "        \n",
    "        # 2 Convolutional layers with 32 filters, 3x3 size, and stride 1\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), strides=1, activation='relu'),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), strides=1, activation='relu'),\n",
    "        \n",
    "        # Max-pooling operation with 2x2 stride\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "        # Dropout with coefficient 0.25\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        \n",
    "        # Repeat above 4 operations using 64 filters\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), strides=1, activation='relu'),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), strides=1, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        \n",
    "        # Flatten the output and feed through fully connected layer with 256 neurons\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "\n",
    "        # Dropout with coefficient 0.25\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 10, 10, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 5, 5, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               409856    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 89)                22873     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 497,721\n",
      "Trainable params: 497,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# TF profiler stats - FLOPS calculation?\\n\\nProfileOptionBuilder = tf.profiler.ProfileOptionBuilder\\n\\nparam_stats = tf.profiler.profile(\\n    tf.get_default_graph(),\\n    options=ProfileOptionBuilder.trainable_variables_parameter())\\n\\n# Use code view to associate statistics with Python codes.\\nopts = ProfileOptionBuilder(\\n    ProfileOptionBuilder.trainable_variables_parameter()\\n    ).with_node_names(show_name_regexes=['.*my_code1.py.*', '.*my_code2.py.*']\\n    ).build()\\nparam_stats = tf.profiler.profile(\\n    tf.get_default_graph(),\\n    cmd='code',\\n    options=opts)\\n\\n# param_stats can be tensorflow.tfprof.GraphNodeProto or\\n# tensorflow.tfprof.MultiGraphNodeProto, depending on the view.\\n# Let's print the root below.\\nsys.stdout.write('total_params: %d\\n' % param_stats.total_parameters)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# TF profiler stats - FLOPS calculation?\n",
    "\n",
    "ProfileOptionBuilder = tf.profiler.ProfileOptionBuilder\n",
    "\n",
    "param_stats = tf.profiler.profile(\n",
    "    tf.get_default_graph(),\n",
    "    options=ProfileOptionBuilder.trainable_variables_parameter())\n",
    "\n",
    "# Use code view to associate statistics with Python codes.\n",
    "opts = ProfileOptionBuilder(\n",
    "    ProfileOptionBuilder.trainable_variables_parameter()\n",
    "    ).with_node_names(show_name_regexes=['.*my_code1.py.*', '.*my_code2.py.*']\n",
    "    ).build()\n",
    "param_stats = tf.profiler.profile(\n",
    "    tf.get_default_graph(),\n",
    "    cmd='code',\n",
    "    options=opts)\n",
    "\n",
    "# param_stats can be tensorflow.tfprof.GraphNodeProto or\n",
    "# tensorflow.tfprof.MultiGraphNodeProto, depending on the view.\n",
    "# Let's print the root below.\n",
    "sys.stdout.write('total_params: %d\\n' % param_stats.total_parameters)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 12:33:33.021188: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-01-29 12:33:33.021212: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-01-29 12:33:33.021246: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1664] Profiler found 1 GPUs\n",
      "2023-01-29 12:33:33.036178: E tensorflow/core/profiler/backends/gpu/cupti_error_manager.cc:192] cuptiSubscribe: error 15: CUPTI_ERROR_NOT_INITIALIZED\n",
      "2023-01-29 12:33:33.036224: E tensorflow/core/profiler/backends/gpu/cupti_error_manager.cc:457] cuptiGetResultString: ignored due to a previous error.\n",
      "2023-01-29 12:33:33.036235: E tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1715] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error \n",
      "2023-01-29 12:33:33.036392: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-01-29 12:33:33.036434: E tensorflow/core/profiler/backends/gpu/cupti_error_manager.cc:140] cuptiFinalize: ignored due to a previous error.\n",
      "2023-01-29 12:33:33.036439: E tensorflow/core/profiler/backends/gpu/cupti_error_manager.cc:457] cuptiGetResultString: ignored due to a previous error.\n",
      "2023-01-29 12:33:33.036442: E tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1807] function cupti_interface_->Finalize()failed with error \n",
      "2023-01-29 12:33:33.036558: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-01-29 12:33:33.036564: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-01-29 12:33:33.036583: E tensorflow/core/profiler/backends/gpu/cupti_error_manager.cc:133] cuptiGetTimestamp: ignored due to a previous error.\n",
      "2023-01-29 12:33:33.036588: E tensorflow/core/profiler/backends/gpu/cupti_error_manager.cc:184] cuptiSubscribe: ignored due to a previous error.\n",
      "2023-01-29 12:33:33.036592: E tensorflow/core/profiler/backends/gpu/cupti_error_manager.cc:457] cuptiGetResultString: ignored due to a previous error.\n",
      "2023-01-29 12:33:33.036595: E tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1715] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error \n",
      "/usr/local/lib/python3.8/dist-packages/keras/backend.py:5585: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "2023-01-29 12:33:33.724352: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-01-29 12:33:34.198880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2023-01-29 12:33:35.160774: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f537cf1ab40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-29 12:33:35.160823: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce GTX 1650 SUPER, Compute Capability 7.5\n",
      "2023-01-29 12:33:35.164800: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-01-29 12:33:35.265910: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6/Unknown - 4s 12ms/step - loss: 4.4938 - accuracy: 0.0156  ERROR:tensorflow:Failed to start profiler: Another profiler is running.\n",
      "6977/6977 [==============================] - 92s 13ms/step - loss: 0.8413 - accuracy: 0.7228 - val_loss: 0.5238 - val_accuracy: 0.8042\n",
      "Epoch 2/30\n",
      "6977/6977 [==============================] - 104s 15ms/step - loss: 0.5995 - accuracy: 0.7851 - val_loss: 0.4745 - val_accuracy: 0.8192\n",
      "Epoch 3/30\n",
      "6977/6977 [==============================] - 91s 13ms/step - loss: 0.5582 - accuracy: 0.7961 - val_loss: 0.4533 - val_accuracy: 0.8256\n",
      "Epoch 4/30\n",
      "6977/6977 [==============================] - 81s 12ms/step - loss: 0.5367 - accuracy: 0.8027 - val_loss: 0.4349 - val_accuracy: 0.8315\n",
      "Epoch 5/30\n",
      "6977/6977 [==============================] - 81s 12ms/step - loss: 0.5227 - accuracy: 0.8065 - val_loss: 0.4300 - val_accuracy: 0.8319\n",
      "Epoch 6/30\n",
      "6977/6977 [==============================] - 83s 12ms/step - loss: 0.5141 - accuracy: 0.8091 - val_loss: 0.4222 - val_accuracy: 0.8348\n",
      "Epoch 7/30\n",
      "6977/6977 [==============================] - 117s 17ms/step - loss: 0.5069 - accuracy: 0.8107 - val_loss: 0.4136 - val_accuracy: 0.8370\n",
      "Epoch 8/30\n",
      "6977/6977 [==============================] - 147s 21ms/step - loss: 0.4995 - accuracy: 0.8131 - val_loss: 0.4061 - val_accuracy: 0.8403\n",
      "Epoch 9/30\n",
      "6977/6977 [==============================] - 96s 14ms/step - loss: 0.4958 - accuracy: 0.8139 - val_loss: 0.4071 - val_accuracy: 0.8383\n",
      "Epoch 10/30\n",
      "6977/6977 [==============================] - 84s 12ms/step - loss: 0.4924 - accuracy: 0.8150 - val_loss: 0.4013 - val_accuracy: 0.8409\n",
      "Epoch 11/30\n",
      "6977/6977 [==============================] - 84s 12ms/step - loss: 0.4942 - accuracy: 0.8141 - val_loss: 0.3935 - val_accuracy: 0.8437\n",
      "Epoch 12/30\n",
      "1424/6977 [=====>........................] - ETA: 1:19:25 - loss: 0.4839 - accuracy: 0.8181"
     ]
    }
   ],
   "source": [
    "# save the best model\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=f\"./artifacts/{model_name}.h5\",\n",
    "    save_weights_only=False,\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, profile_batch='10, 20')\n",
    "\n",
    "tf.profiler.experimental.start('logdir')\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=ds_val,\n",
    "    callbacks=[tb_callback],\n",
    ")\n",
    "# Train the model here\n",
    "tf.profiler.experimental.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Stop right there criminal scum!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate model size on disk, flops and number of parameters\n",
    "\n",
    "flops = calculate_model_flops(wandb.run.summary)\n",
    "disk_size = calculate_model_size_on_disk(f\"./artifacts/{model_name}.h5\")\n",
    "num_parameters = calculate_model_num_parameters(model)\n",
    "\n",
    "# evaluate model on ds_test and log to wandb\n",
    "test_loss_before, test_acc_before = model.evaluate(ds_test)\n",
    "\n",
    "wandb.log({\n",
    "    \"test loss\": test_loss_before, \n",
    "    \"test accuracy\": test_acc_before, \n",
    "    \"number of parameters\": num_parameters,\n",
    "    \"disk size\": disk_size, \n",
    "    \"model flops\": flops,\n",
    "    })\n",
    "\n",
    "# save base model to wandb\n",
    "artifact = wandb.Artifact(\n",
    "    name=model_name,\n",
    "    type=\"model\"\n",
    ")\n",
    "\n",
    "# save best model to artifact\n",
    "artifact.add_file(f\"./artifacts/{model_name}.h5\")\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "\n",
    "# pruning\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Compute end step to finish pruning after 2 epochs.\n",
    "num_images = get_number_of_examples(ds_train)\n",
    "end_step = np.ceil(num_images / bs).astype(np.int32) * epochs\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                            final_sparsity=0.80,\n",
    "                                                            begin_step=0,\n",
    "                                                            end_step=end_step)\n",
    "}\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    wandb_callback,\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=f\"./artifacts/{model_name}_after.h5\",\n",
    "        save_weights_only=False,\n",
    "        monitor=\"val_accuracy\",\n",
    "        mode=\"max\",\n",
    "        save_best_only=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "history = model_for_pruning.fit(\n",
    "    ds_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=ds_val,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "\n",
    "# calculate model size on disk, flops and number of parameters\n",
    "\n",
    "flops = calculate_model_flops(wandb.run.summary)\n",
    "disk_size = calculate_model_size_on_disk(f\"./artifacts/{model_name}_after.h5\")\n",
    "num_parameters = calculate_model_num_parameters(model)\n",
    "\n",
    "# evaluate model on ds_test and log to wandb\n",
    "test_loss_after, test_acc_after = model.evaluate(ds_test)\n",
    "\n",
    "wandb.log({\n",
    "    \"test loss\": test_loss_after, \n",
    "    \"test accuracy\": test_acc_after, \n",
    "    \"number of parameters\": num_parameters,\n",
    "    \"disk size\": disk_size, \n",
    "    \"model flops\": flops,\n",
    "    })\n",
    "\n",
    "\n",
    "print('Baseline test accuracy:', test_acc_before) \n",
    "print('Pruned test accuracy:', test_acc_after)\n",
    "\n",
    "# save base model to wandb\n",
    "artifact = wandb.Artifact(\n",
    "    name=f\"{model_name}_after\",\n",
    "    type=\"model\"\n",
    ")\n",
    "\n",
    "# save best model to artifact\n",
    "artifact.add_file(f\"./artifacts/{model_name}_after.h5\")\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMjD5hwsvnLd"
   },
   "outputs": [],
   "source": [
    "defaults = dict(\n",
    "    batch_size=32*2,\n",
    "    epochs=30,    \n",
    "    optimizer=\"adam\"\n",
    ")\n",
    "train(\"baseline-pruned-numbers\", defaults, artifact_name=\"numbers_splits_tfds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Stop right there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\"baseline-pruned-all-characters\", defaults, artifact_name=\"phcd_paper_splits_tfds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\"baseline-pruned-uppercase-diacritics\", defaults, artifact_name=\"uppercase_splits_tfds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYHU5y_rvrRm"
   },
   "outputs": [],
   "source": [
    "train(\"baseline-pruned-uppercase\", defaults, artifact_name=\"uppercase_no_diacritics_splits_tfds\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d798f8ad6d6c53bff9e7c2dca27b60374f042a436c567b9cf87bf4fc98c22b9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
