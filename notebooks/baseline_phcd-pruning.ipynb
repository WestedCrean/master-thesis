{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRicHmH8vnLY"
   },
   "source": [
    "# Baseline network + pruning + quantization\n",
    "\n",
    "This notebook trains the baseline network with exact same architecture as the one in paper [Recognition of handwritten Latin characters with diacritics using CNN](https://journals.pan.pl/dlibra/publication/136210/edition/119099/content/bulletin-of-the-polish-academy-of-sciences-technical-sciences-recognition-of-handwritten-latin-characters-with-diacritics-using-cnn-lukasik-edyta-charytanowicz-malgorzata-milosz-marek-tokovarov-michail-kaczorowska-monika-czerwinski-dariusz-zientarski-tomasz-2021-69-no-1?language=en)\n",
    "\n",
    "Model architecture description:\n",
    "\n",
    "\"The architecture of the concrete CNN is shown in Fig. 2.\n",
    "The input is a 32x32 binarized matrix. The input is then prop-\n",
    "agated through 12 adaptable layers. First come two convolu-\n",
    "tional layers having 32 filters with the size of 3x3 and stride 1.\n",
    "Secondly, the output of the convolutional layer is fed to the\n",
    "ReLU function. The output is down-sampled using a max-pool-\n",
    "ing operation with a 2x2 stride. Next, the dropout technique is\n",
    "used with the coefficient 0.25. The four operations (two con-\n",
    "volutions, nonlinearity, max-pooling, and dropout) are repeated,\n",
    "using 64 filters for the convolutional layers. The output of the\n",
    "last layer is then flattened and fed through a fully connected\n",
    "layer with 256 neurons and ReLU nonlinearities, dropped out\n",
    "with the 0.25 coefficient, and a final output layer is fully con-\n",
    "nected with a Softmax activation function. The Adam optimizer\n",
    "and the cross-entropy loss function were used in the network. \n",
    "The output is a probability distribution over 89 classes.\"\n",
    "\n",
    "\n",
    "Additionally, this network was pruned and quantized after training.\n",
    "\n",
    "\n",
    "\n",
    "After training, model is serialized and uploaded to W&B project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U tensorboard_plugin_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhFWPE1BvnLZ"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import shutil\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def load_data(run, artifact_name = \"phcd_paper_splits_tfds\") -> List[tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Downloads datasets from a wandb artifact and loads them into a list of tf.data.Datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    artifact = run.use_artifact(f\"master-thesis/{artifact_name}:latest\")\n",
    "    artifact_dir = pathlib.Path(\n",
    "        f\"./artifacts/{artifact.name.replace(':', '-')}\"\n",
    "    ).resolve()\n",
    "    if not artifact_dir.exists():\n",
    "        artifact_dir = artifact.download()\n",
    "        artifact_dir = pathlib.Path(artifact_dir).resolve()\n",
    "\n",
    "    # if tf._version_ minor is less than 10, use\n",
    "    # tf.data.experimental.load instead of tf.data.Dataset.load\n",
    "\n",
    "    if int(tf._version_.split(\".\")[1]) < 10:\n",
    "        load_function = tf.data.experimental.load\n",
    "    else:\n",
    "        load_function = tf.data.Dataset.load\n",
    "    \n",
    "    output_list = []\n",
    "    for split in [\"train\", \"test\", \"val\"]:\n",
    "        ds = load_function(str(artifact_dir / split), compression=\"GZIP\")\n",
    "        output_list.append(ds)\n",
    "    \n",
    "    return output_list\n",
    "\n",
    "def get_number_of_classes(ds: tf.data.Dataset) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of classes in a dataset.\n",
    "    \"\"\"\n",
    "    labels_iterator= ds.map(lambda x, y: y).as_numpy_iterator()\n",
    "    labels = np.concatenate(list(labels_iterator))\n",
    "    return len(np.unique(labels))\n",
    "\n",
    "def get_number_of_examples(ds: tf.data.Dataset) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of examples in a dataset.\n",
    "    \"\"\"\n",
    "    return sum(1 for _ in ds)\n",
    "\n",
    "def preprocess_dataset(ds: tf.data.Dataset, batch_size: int, cache: bool = True) -> tf.data.Dataset:\n",
    "    ds = ds.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y))  # normalize\n",
    "    ds = ds.unbatch().batch(batch_size)\n",
    "    if cache:\n",
    "        ds = ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def calculate_model_size_on_disk(path: str) -> int:\n",
    "    return pathlib.Path(path).stat().st_size    \n",
    "\n",
    "def calculate_model_num_parameters(model: tf.keras.Model) -> int:\n",
    "    return model.count_params()\n",
    "\n",
    "def calculate_model_flops(summary) -> float:\n",
    "    # from run.summary get GFLOPs or GFLOPS whichever is available\n",
    "    if \"GFLOPs\" in summary.keys():\n",
    "        return summary.get(\"GFLOPs\")\n",
    "    elif \"GFLOPS\" in summary.keys():\n",
    "        return summary.get(\"GFLOPS\")\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_flops(model_h5_path):\n",
    "    session = tf.compat.v1.Session()\n",
    "    graph = tf.compat.v1.get_default_graph()\n",
    "    \n",
    "    # silence tensorflow warnings and info messages\n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "    with graph.as_default():\n",
    "        with session.as_default():\n",
    "            model = tf.keras.models.load_model(model_h5_path)\n",
    "\n",
    "            run_meta = tf.compat.v1.RunMetadata()\n",
    "            opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "        \n",
    "            # We use the Keras session graph in the call to the profiler.\n",
    "            flops = tf.compat.v1.profiler.profile(graph=graph,\n",
    "                                                  run_meta=run_meta, cmd='op', options=opts)\n",
    "        \n",
    "            return flops.total_float_ops\n",
    "\n",
    "def plot_history(history, title):\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.suptitle(title)\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['accuracy'], label='train')\n",
    "    plt.plot(history.history['val_accuracy'], label='val')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='val')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rgJaTJFlvnLb"
   },
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Available devices: \", tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = dict(\n",
    "    batch_size=32*2,\n",
    "    epochs=100,    \n",
    "    optimizer=\"adam\"\n",
    ")\n",
    "\n",
    "model_name = \"baseline\"\n",
    "\n",
    "artifact_name = \"phcd_paper_splits_tfds\"\n",
    "run = wandb.init(project=\"master-thesis\", job_type=\"training\", name=model_name, config=defaults, tags=[artifact_name])\n",
    "    \n",
    "# hyperparameters\n",
    "epochs = wandb.config.epochs\n",
    "bs = wandb.config.batch_size\n",
    "\n",
    "ds_train, ds_test, ds_val = load_data(run, artifact_name=artifact_name)\n",
    "\n",
    "num_classes = get_number_of_classes(ds_val)\n",
    "\n",
    "ds_train = preprocess_dataset(ds_train, batch_size=bs)\n",
    "ds_val = preprocess_dataset(ds_val, batch_size=bs)\n",
    "ds_test = preprocess_dataset(ds_test, batch_size=bs, cache=False)\n",
    "\n",
    "'''\n",
    "The architecture of the concrete CNN is shown in Fig. 2.\n",
    "The input is a 32x32 binarized matrix. \n",
    "The input is then propagated through 12 adaptable layers. \n",
    "First come two convolutional layers having 32 filters with the size of 3x3 and stride 1.\n",
    "Secondly, the output of the convolutional layer is fed to the\n",
    "ReLU function. The output is down-sampled using a max-pool-\n",
    "ing operation with a 2x2 stride. Next, the dropout technique is\n",
    "used with the coefficient 0.25. The four operations (two con-\n",
    "volutions, nonlinearity, max-pooling, and dropout) are repeated,\n",
    "using 64 filters for the convolutional layers. The output of the\n",
    "last layer is then flattened and fed through a fully connected\n",
    "layer with 256 neurons and ReLU nonlinearities, dropped out\n",
    "with the 0.25 coefficient, and a final output layer is fully con-\n",
    "nected with a Softmax activation function. The Adam optimizer\n",
    "and the cross-entropy loss function were used in the network\n",
    "[24]. The output is a probability distribution over 89 classes.\n",
    "'''\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.InputLayer(input_shape=(32, 32, 1)),\n",
    "        \n",
    "        # 2 Convolutional layers with 32 filters, 3x3 size, and stride 1\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), strides=1, activation='relu'),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), strides=1, activation='relu'),\n",
    "        \n",
    "        # Max-pooling operation with 2x2 stride\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "        # Dropout with coefficient 0.25\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        \n",
    "        # Repeat above 4 operations using 64 filters\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), strides=1, activation='relu'),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), strides=1, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        \n",
    "        # Flatten the output and feed through fully connected layer with 256 neurons\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "\n",
    "        # Dropout with coefficient 0.25\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    ds_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=ds_val,\n",
    "    callbacks=[\n",
    "        WandbCallback(\n",
    "            compute_flops=True, \n",
    "            save_model=False, \n",
    "            log_weights=False, \n",
    "            log_gradients=False\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "plot_history(history, \"Baseline\")\n",
    "tf.keras.models.save_model(model, 'model_baseline.h5', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate model size on disk, flops and number of parameters\n",
    "\n",
    "flops = get_flops('model_baseline.h5')\n",
    "disk_size = calculate_model_size_on_disk('model_baseline.h5')\n",
    "num_parameters = calculate_model_num_parameters(model)\n",
    "\n",
    "# evaluate model on ds_test and log to wandb\n",
    "test_loss_before, test_acc_before = model.evaluate(ds_test)\n",
    "\n",
    "data_to_log = {\n",
    "    \"test loss\": test_loss_before, \n",
    "    \"test accuracy\": test_acc_before, \n",
    "    \"number of parameters\": num_parameters,\n",
    "    \"disk size\": disk_size, \n",
    "    \"model flops\": flops,\n",
    "    }\n",
    "print(data_to_log)\n",
    "wandb.log(data_to_log)\n",
    "run.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization - allows for better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"master-thesis\", job_type=\"training\", name=f\"{model_name}_quantized\", config=defaults, tags=[artifact_name])\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "quant_epochs = 2\n",
    "base_model = tf.keras.models.load_model('model_baseline.h5')\n",
    "\n",
    "# q_aware stands for for quantization aware.\n",
    "q_aware_model = quantize_model(base_model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = q_aware_model.fit(\n",
    "    ds_train,\n",
    "    epochs=quant_epochs,\n",
    "    validation_data=ds_val,\n",
    "    callbacks=[\n",
    "        WandbCallback(\n",
    "            compute_flops=True, \n",
    "            save_model=False, \n",
    "            log_weights=False, \n",
    "            log_gradients=False\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "plot_history(history, \"Quantized\")\n",
    "tf.keras.models.save_model(model, 'model_quantized.h5', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate model size on disk, flops and number of parameters\n",
    "\n",
    "flops = get_flops('model_quantized.h5')\n",
    "disk_size = calculate_model_size_on_disk('model_quantized.h5')\n",
    "num_parameters = calculate_model_num_parameters(q_aware_model)\n",
    "\n",
    "# evaluate model on ds_test and log to wandb\n",
    "test_loss_after, test_acc_after = model.evaluate(ds_test)\n",
    "\n",
    "data_to_log = {\n",
    "    \"test loss\": test_loss_after, \n",
    "    \"test accuracy\": test_acc_after, \n",
    "    \"number of parameters\": num_parameters,\n",
    "    \"disk size\": disk_size, \n",
    "    \"model flops\": flops,\n",
    "    }\n",
    "print(data_to_log)\n",
    "wandb.log(data_to_log)\n",
    "\n",
    "test_acc_quantize = test_acc_after\n",
    "run.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning model - allows for smaller size (after compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"master-thesis\", job_type=\"training\", name=f\"{model_name}_pruned\", config=defaults, tags=[artifact_name])\n",
    "# pruning\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "prune_epochs = 4\n",
    "num_images = get_number_of_examples(ds_train)\n",
    "end_step = np.ceil(num_images / prune_epochs).astype(np.int32) * prune_epochs\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                            final_sparsity=0.80,\n",
    "                                                            begin_step=0,\n",
    "                                                            end_step=end_step,\n",
    "                                                            frequency=100)\n",
    "}\n",
    "\n",
    "base_model = tf.keras.models.load_model('model_baseline.h5')\n",
    "\n",
    "model_sparse = prune_low_magnitude(base_model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_sparse.compile(optimizer='adam',\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "model_sparse.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    tfmot.sparsity.keras.PruningSummaries(log_dir='./prune_summary'),\n",
    "    WandbCallback(\n",
    "            compute_flops=True, \n",
    "            save_model=False, \n",
    "            log_weights=False, \n",
    "            log_gradients=False\n",
    "        )\n",
    "]\n",
    "\n",
    "history = model_sparse.fit(\n",
    "    ds_train,\n",
    "    epochs=prune_epochs,\n",
    "    validation_data=ds_val,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "plot_history(history, \"Pruned\")\n",
    "tf.keras.models.save_model(model_sparse, 'model_sparse.h5', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate model size on disk, flops and number of parameters\n",
    "\n",
    "flops = get_flops('model_sparse.h5')\n",
    "disk_size = calculate_model_size_on_disk('model_sparse.h5')\n",
    "num_parameters = calculate_model_num_parameters(model_sparse)\n",
    "\n",
    "# evaluate model on ds_test and log to wandb\n",
    "test_loss_after, test_acc_after = model_sparse.evaluate(ds_test)\n",
    "\n",
    "data_to_log = {\n",
    "    \"test loss\": test_loss_before, \n",
    "    \"test accuracy\": test_acc_before, \n",
    "    \"number of parameters\": num_parameters,\n",
    "    \"disk size\": disk_size, \n",
    "    \"model flops\": flops,\n",
    "    }\n",
    "print(data_to_log)\n",
    "wandb.log(data_to_log)\n",
    "\n",
    "test_acc_prune = test_acc_after\n",
    "run.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PQAT - pruning preserving quantization aware training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"master-thesis\", job_type=\"training\", name=f\"{model_name}_pqat\", config=defaults, tags=[artifact_name])\n",
    "\n",
    "# PQAT\n",
    "quant_aware_annotate_model = tfmot.quantization.keras.quantize_annotate_model(\n",
    "              model_sparse)\n",
    "pqat_model = tfmot.quantization.keras.quantize_apply(\n",
    "              quant_aware_annotate_model,\n",
    "              tfmot.experimental.combine.Default8BitPrunePreserveQuantizeScheme())\n",
    "\n",
    "pqat_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    tfmot.sparsity.keras.PruningSummaries(log_dir='./prune_summary'),\n",
    "    WandbCallback(\n",
    "            compute_flops=True, \n",
    "            save_model=False, \n",
    "            log_weights=False, \n",
    "            log_gradients=False\n",
    "        )\n",
    "]\n",
    "\n",
    "history = quant_aware_annotate_model.fit(\n",
    "    ds_train,\n",
    "    epochs=prune_epochs,\n",
    "    validation_data=ds_val,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "plot_history(history, \"Pruned model + PQAT\")\n",
    "tf.keras.models.save_model(quant_aware_annotate_model, 'model_quantized_sparse.h5', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate model size on disk, flops and number of parameters\n",
    "flops = get_flops('model_quantized_sparse.h5')\n",
    "disk_size = calculate_model_size_on_disk('model_quantized_sparse.h5')\n",
    "num_parameters = calculate_model_num_parameters(quant_aware_annotate_model)\n",
    "\n",
    "# evaluate model on ds_test and log to wandb\n",
    "test_loss_after, test_acc_after = quant_aware_annotate_model.evaluate(ds_test)\n",
    "\n",
    "data_to_log = {\n",
    "    \"test loss\": test_loss_before, \n",
    "    \"test accuracy\": test_acc_before, \n",
    "    \"number of parameters\": num_parameters,\n",
    "    \"disk size\": disk_size, \n",
    "    \"model flops\": flops,\n",
    "    }\n",
    "print(data_to_log)\n",
    "wandb.log(data_to_log)\n",
    "\n",
    "test_acc_pqat = test_acc_after\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary & Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Baseline test accuracy:', test_acc_before) \n",
    "print('Quantized test accuracy:', test_acc_quantize)\n",
    "print('Pruned test accuracy:', test_acc_prune)\n",
    "print('PQAT test accuracy:', test_acc_pqat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('model_baseline.zip', 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write('model_baseline.h5')\n",
    "\n",
    "with zipfile.ZipFile('model_quantized.zip', 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write('model_quantized.h5')\n",
    "    \n",
    "with zipfile.ZipFile('model_sparse.zip', 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write('model_sparse.h5')\n",
    "\n",
    "with zipfile.ZipFile('model_quantized_sparse.zip', 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write('model_quantized_sparse.h5')\n",
    "\n",
    "print('Zipped model files size:\\n')\n",
    "b_size = os.path.getsize('model_baseline.zip')\n",
    "q_size = os.path.getsize('model_quantized.zip')\n",
    "s_size = os.path.getsize('model_sparse.zip')\n",
    "q_s_size = os.path.getsize('model_quantized_sparse.zip')\n",
    "print('Baseline: {} bytes'.format(b_size))\n",
    "print('Quantized: {} bytes'.format(q_size))\n",
    "print('Sparse:   {} bytes'.format(s_size))\n",
    "print('PQAT:     {} bytes'.format(q_s_size))\n",
    "\n",
    "print(f'Quantized model is {((b_size-q_size)/b_size * 100):.2f}% smaller')\n",
    "print(f'Sparse model is {((b_size-s_size)/b_size * 100):.2f}% smaller')\n",
    "print(f'PQAT model is {((b_size-q_s_size)/b_size * 100):.2f}% smaller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir prune_summary"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "d798f8ad6d6c53bff9e7c2dca27b60374f042a436c567b9cf87bf4fc98c22b9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
