{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset overview\n",
    "\n",
    "In this notebook we review class counts in train and validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pathlib\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def load_data(run: wandb.sdk.wandb_run.Run) -> pathlib.Path:\n",
    "    \"\"\"\n",
    "    Unpacks data from an artifact into a folder and returns the path to the folder.\n",
    "    \"\"\"\n",
    "\n",
    "    artifact_name = f\"letters_splits\"\n",
    "    artifact = run.use_artifact(f\"master-thesis/{artifact_name}:latest\")\n",
    "    artifact_dir = pathlib.Path(\n",
    "        f\"./artifacts/{artifact.name.replace(':', '-')}\"\n",
    "    ).resolve()\n",
    "    if not artifact_dir.exists():\n",
    "        artifact_dir = artifact.download()\n",
    "        artifact_dir = pathlib.Path(artifact_dir).resolve()\n",
    "        for split_file in artifact_dir.iterdir():\n",
    "            if split_file.name.endswith(\".tar.gz\"):\n",
    "                split = split_file.name.replace(\".tar.gz\", \"\")\n",
    "                shutil.unpack_archive(split_file, artifact_dir / split, format=\"gztar\")\n",
    "\n",
    "    return [artifact_dir / split for split in [\"train\", \"test\", \"val\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"master-thesis\", job_type=\"preprocessing\")\n",
    "split_paths = load_data(run=run)\n",
    "\n",
    "ds_train = tf.keras.utils.image_dataset_from_directory(\n",
    "        split_paths[0],\n",
    "        image_size=(32, 32),\n",
    "        color_mode=\"grayscale\",\n",
    "    )\n",
    "\n",
    "ds_test = tf.keras.utils.image_dataset_from_directory(\n",
    "        split_paths[1],\n",
    "        image_size=(32, 32),\n",
    "        color_mode=\"grayscale\",\n",
    "    )\n",
    "\n",
    "ds_val = tf.keras.utils.image_dataset_from_directory(\n",
    "        split_paths[2],\n",
    "        image_size=(32, 32),\n",
    "        color_mode=\"grayscale\",\n",
    "    )\n",
    "\n",
    "number_of_classes = len(ds_train.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_id_to_class_name = {\n",
    "        0: \"0\",\n",
    "        1: \"1\",\n",
    "        2: \"2\",\n",
    "        3: \"3\",\n",
    "        4: \"4\",\n",
    "        5: \"5\",\n",
    "        6: \"6\",\n",
    "        7: \"7\",\n",
    "        8: \"8\",\n",
    "        9: \"9\",\n",
    "        10: \"a\",\n",
    "        11: \"b\",\n",
    "        12: \"c\",\n",
    "        13: \"d\",\n",
    "        14: \"e\",\n",
    "        15: \"f\",\n",
    "        16: \"g\",\n",
    "        17: \"h\",\n",
    "        18: \"i\",\n",
    "        19: \"j\",\n",
    "        20: \"k\",\n",
    "        21: \"l\",\n",
    "        22: \"m\",\n",
    "        23: \"n\",\n",
    "        24: \"o\",\n",
    "        25: \"p\",\n",
    "        26: \"q\",\n",
    "        27: \"r\",\n",
    "        28: \"s\",\n",
    "        29: \"t\",\n",
    "        30: \"u\",\n",
    "        31: \"v\",\n",
    "        32: \"w\",\n",
    "        33: \"x\",\n",
    "        34: \"y\",\n",
    "        35: \"z\",\n",
    "        36: \"A\",\n",
    "        37: \"B\",\n",
    "        38: \"C\",\n",
    "        39: \"D\",\n",
    "        40: \"E\",\n",
    "        41: \"F\",\n",
    "        42: \"G\",\n",
    "        43: \"H\",\n",
    "        44: \"I\",\n",
    "        45: \"J\",\n",
    "        46: \"K\",\n",
    "        47: \"L\",\n",
    "        48: \"M\",\n",
    "        49: \"N\",\n",
    "        50: \"O\",\n",
    "        51: \"P\",\n",
    "        52: \"Q\",\n",
    "        53: \"R\",\n",
    "        54: \"S\",\n",
    "        55: \"T\",\n",
    "        56: \"U\",\n",
    "        57: \"V\",\n",
    "        58: \"W\",\n",
    "        59: \"X\",\n",
    "        60: \"Y\",\n",
    "        61: \"Z\",\n",
    "        # then lowercase letters of the Polish alphabet: ą, ć, ę, ł, ń, ó, ś, ź, ż\n",
    "        62: \"ą\",\n",
    "        63: \"ć\",\n",
    "        64: \"ę\",\n",
    "        65: \"ł\",\n",
    "        66: \"ń\",\n",
    "        67: \"ó\",\n",
    "        68: \"ś\",\n",
    "        69: \"ź\",\n",
    "        70: \"ż\",\n",
    "        # then uppercase letters of the Polish alphabet: Ą, Ć, Ę, Ł, Ń, Ó, Ś, Ź, Ż\n",
    "        71: \"Ą\",\n",
    "        72: \"Ć\",\n",
    "        73: \"Ę\",\n",
    "        74: \"Ł\",\n",
    "        75: \"Ń\",\n",
    "        76: \"Ó\",\n",
    "        77: \"Ś\",\n",
    "        78: \"Ź\",\n",
    "        79: \"Ż\",\n",
    "        # then special characters: + - : ; $ ! ? @\n",
    "        80: \"+\",\n",
    "        81: \"-\",\n",
    "        82: \":\",\n",
    "        83: \";\",\n",
    "        84: \"$\",\n",
    "        85: \"!\",\n",
    "        86: \"?\",\n",
    "        87: \"@\",\n",
    "        88: \".\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "CLASSES = list(char_id_to_class_name.keys())\n",
    "\n",
    "# TFRecord util functions\n",
    "\n",
    "def decode_jpeg_and_label(filename):\n",
    "  bits = tf.io.read_file(filename)\n",
    "  image = tf.io.decode_jpeg(bits)\n",
    "  # parse flower name from containing directory\n",
    "  label = tf.strings.split(tf.expand_dims(filename, axis=-1), sep='/')\n",
    "  label = label.values[-2]\n",
    "  return image, label\n",
    "\n",
    "def recompress_image(image, label):\n",
    "  height = tf.shape(image)[0]\n",
    "  width = tf.shape(image)[1]\n",
    "  image = tf.cast(image, tf.uint8)\n",
    "  image = tf.image.encode_jpeg(image, optimize_size=True, chroma_downsampling=False)\n",
    "  return image, label, height, width\n",
    "\n",
    "# Three types of data can be stored in TFRecords: bytestrings, integers and floats\n",
    "# They are always stored as lists, a single data element will be a list of size 1\n",
    "\n",
    "def _bytestring_feature(list_of_bytestrings):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))\n",
    "\n",
    "def _int_feature(list_of_ints): # int64\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\n",
    "\n",
    "def _float_feature(list_of_floats): # float32\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))\n",
    "\n",
    "def to_tfrecord(tfrec_filewriter, img_bytes, label, height, width):  \n",
    "  class_num = np.argmax(np.array(CLASSES)==label) # 'roses' => 2 (order defined in CLASSES)\n",
    "  one_hot_class = np.eye(len(CLASSES))[class_num]     # [0, 0, 1, 0, 0] for class #2, roses\n",
    "\n",
    "  feature = {\n",
    "      \"image\": _bytestring_feature([img_bytes]), # one image in the list\n",
    "      \"class\": _int_feature([class_num]),        # one class in the list\n",
    "      \n",
    "      # additional (not very useful) fields to demonstrate TFRecord writing/reading of different types of data\n",
    "      \"label\":         _bytestring_feature([label]),          # fixed length (1) list of strings, the text label\n",
    "      \"size\":          _int_feature([height, width]),         # fixed length (2) list of ints\n",
    "      \"one_hot_class\": _float_feature(one_hot_class.tolist()) # variable length  list of floats, n=len(CLASSES)\n",
    "  }\n",
    "  return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "def write_tfrecords(split_path, data_dir, shards=16, output_dir=\"./datasets_tfrecords\"):\n",
    "  output_dir = pathlib.Path(output_dir).resolve()\n",
    "  output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "  DATA_OUTPUT = str(output_dir) \n",
    "  DATA_PATTERN = f\"{split_paths}/*/*.png\" \n",
    "  SHARDS = shards\n",
    "  AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "  nb_images = len(tf.io.gfile.glob(DATA_PATTERN))\n",
    "  shard_size = math.ceil(1.0 * nb_images / SHARDS)\n",
    "  print(\"Pattern matches {} images which will be rewritten as {} .tfrec files containing {} images each.\".format(nb_images, SHARDS, shard_size))\n",
    "\n",
    "  filenames = tf.data.Dataset.list_files(DATA_PATTERN, seed=35155) # This also shuffles the images\n",
    "  dataset1 = filenames.map(decode_jpeg_and_label, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "  dataset3 = dataset1.map(recompress_image, num_parallel_calls=AUTOTUNE)\n",
    "  dataset3 = dataset3.batch(shard_size) # sharding: there will be one \"batch\" of images per file \n",
    "\n",
    "  print(\"Writing TFRecords\")\n",
    "  for shard, (image, label, height, width) in enumerate(dataset3):\n",
    "    # batch size used as shard size here\n",
    "    shard_size = image.numpy().shape[0]\n",
    "    # good practice to have the number of records in the filename\n",
    "    filename = DATA_OUTPUT + \"{:02d}-{}.tfrec\".format(shard, shard_size)\n",
    "    \n",
    "    with tf.io.TFRecordWriter(filename) as out_file:\n",
    "      for i in range(shard_size):\n",
    "        example = to_tfrecord(out_file,\n",
    "                              image.numpy()[i], # re-compressed image: already a byte string\n",
    "                              label.numpy()[i],\n",
    "                              height.numpy()[i],\n",
    "                              width.numpy()[i])\n",
    "        out_file.write(example.SerializeToString())\n",
    "      print(\"Wrote file {} containing {} records\".format(filename, shard_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as TFRecords and upload to WandB\n",
    "\n",
    "tfrecord_dir = \"./datasets_tfrecords\"\n",
    "for split_path in split_paths:\n",
    "  write_tfrecords(split_path, tfrecord_dir)\n",
    "\n",
    "artifact = wandb.Artifact(\"letters_splits_tfds\", type=\"dataset\", description=\"Dataset splits in tf.data.TFRecord format\")\n",
    "artifact.add_dir(tfrecord_dir)\n",
    "run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save datasets on disk then upload to wandb as artifacts\n",
    "\n",
    "output_dir = pathlib.Path(\"./datasets\").resolve()\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "ds_train.save(str(output_dir / \"train\"), compression=\"GZIP\")\n",
    "ds_val.save(str(output_dir / \"val\"), compression=\"GZIP\")\n",
    "ds_test.save(str(output_dir / \"test\"), compression=\"GZIP\")\n",
    "\n",
    "artifact = wandb.Artifact(\"letters_splits_tfds\", type=\"dataset\", description=\"Dataset splits in tf.data.Dataset format\")\n",
    "artifact.add_dir(output_dir)\n",
    "run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate class count for each split\n",
    "train_class_count = np.zeros(number_of_classes)\n",
    "for _, label in ds_train:\n",
    "    train_class_count += tf.math.bincount(label, minlength=number_of_classes)\n",
    "\n",
    "val_class_count = np.zeros(number_of_classes)\n",
    "for _, label in ds_val:\n",
    "    val_class_count += tf.math.bincount(label, minlength=number_of_classes)\n",
    "\n",
    "# plot class count for each split\n",
    "plt.bar(ds_train.class_names, train_class_count)\n",
    "plt.title(\"Train\")\n",
    "plt.show()\n",
    "\n",
    "plt.bar(ds_val.class_names, val_class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log class count for each split to wandb\n",
    "\n",
    "wandb.log({\"train_class_count\": wandb.Histogram(train_class_count)})\n",
    "wandb.log({\"val_class_count\": wandb.Histogram(val_class_count)})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:22:47) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50e1e42dfe1d658956852bd8112c3a0f458e21bda0a35307ca1649a3ad9140a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
