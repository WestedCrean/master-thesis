{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Dataset overview\n",
                "\n",
                "In this notebook we review class counts in train and validation splits"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "import wandb\n",
                "import pathlib\n",
                "import shutil\n",
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from typing import List\n",
                "\n",
                "\n",
                "def load_raw_image_data(run) -> pathlib.Path:\n",
                "    \"\"\"\n",
                "    Unpacks data from an artifact into a folder and returns the path to the folder.\n",
                "    \"\"\"\n",
                "\n",
                "    artifact_name = f\"letters_splits\"\n",
                "    artifact = run.use_artifact(f\"master-thesis/{artifact_name}:latest\")\n",
                "    artifact_dir = pathlib.Path(\n",
                "        f\"./artifacts/{artifact.name.replace(':', '-')}\"\n",
                "    ).resolve()\n",
                "    if not artifact_dir.exists():\n",
                "        artifact_dir = artifact.download()\n",
                "        artifact_dir = pathlib.Path(artifact_dir).resolve()\n",
                "        for split_file in artifact_dir.iterdir():\n",
                "            if split_file.name.endswith(\".tar.gz\"):\n",
                "                split = split_file.name.replace(\".tar.gz\", \"\")\n",
                "                shutil.unpack_archive(split_file, artifact_dir / split, format=\"gztar\")\n",
                "\n",
                "    return [artifact_dir / split for split in [\"train\", \"test\", \"val\"]]\n",
                "\n",
                "def load_tfdataset_data(run) -> List[tf.data.Dataset]:\n",
                "    \"\"\"\n",
                "    Downloads datasets from a wandb artifact and loads them into a list of tf.data.Datasets.\n",
                "    \"\"\"\n",
                "\n",
                "    artifact_name = f\"letters_splits_tfds\"\n",
                "    artifact = run.use_artifact(f\"master-thesis/{artifact_name}:latest\")\n",
                "    artifact_dir = pathlib.Path(\n",
                "        f\"./artifacts/{artifact.name.replace(':', '-')}\"\n",
                "    ).resolve()\n",
                "    if not artifact_dir.exists():\n",
                "        artifact_dir = artifact.download()\n",
                "        artifact_dir = pathlib.Path(artifact_dir).resolve()\n",
                "\n",
                "    # if tf.__version__ minor is less than 10, use\n",
                "    # tf.data.experimental.load instead of tf.data.Dataset.load\n",
                "\n",
                "    if int(tf.__version__.split(\".\")[1]) < 10:\n",
                "        load_function = tf.data.experimental.load\n",
                "    else:\n",
                "        load_function = tf.data.Dataset.load\n",
                "    \n",
                "    output_list = []\n",
                "    for split in [\"train\", \"test\", \"val\"]:\n",
                "        ds = load_function(str(artifact_dir / split), compression=\"GZIP\")\n",
                "        output_list.append(ds)\n",
                "    \n",
                "    return output_list"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "char_id_to_class_name = {\n",
                "        0: \"0\",\n",
                "        1: \"1\",\n",
                "        2: \"2\",\n",
                "        3: \"3\",\n",
                "        4: \"4\",\n",
                "        5: \"5\",\n",
                "        6: \"6\",\n",
                "        7: \"7\",\n",
                "        8: \"8\",\n",
                "        9: \"9\",\n",
                "        10: \"a\",\n",
                "        11: \"b\",\n",
                "        12: \"c\",\n",
                "        13: \"d\",\n",
                "        14: \"e\",\n",
                "        15: \"f\",\n",
                "        16: \"g\",\n",
                "        17: \"h\",\n",
                "        18: \"i\",\n",
                "        19: \"j\",\n",
                "        20: \"k\",\n",
                "        21: \"l\",\n",
                "        22: \"m\",\n",
                "        23: \"n\",\n",
                "        24: \"o\",\n",
                "        25: \"p\",\n",
                "        26: \"q\",\n",
                "        27: \"r\",\n",
                "        28: \"s\",\n",
                "        29: \"t\",\n",
                "        30: \"u\",\n",
                "        31: \"v\",\n",
                "        32: \"w\",\n",
                "        33: \"x\",\n",
                "        34: \"y\",\n",
                "        35: \"z\",\n",
                "        36: \"A\",\n",
                "        37: \"B\",\n",
                "        38: \"C\",\n",
                "        39: \"D\",\n",
                "        40: \"E\",\n",
                "        41: \"F\",\n",
                "        42: \"G\",\n",
                "        43: \"H\",\n",
                "        44: \"I\",\n",
                "        45: \"J\",\n",
                "        46: \"K\",\n",
                "        47: \"L\",\n",
                "        48: \"M\",\n",
                "        49: \"N\",\n",
                "        50: \"O\",\n",
                "        51: \"P\",\n",
                "        52: \"Q\",\n",
                "        53: \"R\",\n",
                "        54: \"S\",\n",
                "        55: \"T\",\n",
                "        56: \"U\",\n",
                "        57: \"V\",\n",
                "        58: \"W\",\n",
                "        59: \"X\",\n",
                "        60: \"Y\",\n",
                "        61: \"Z\",\n",
                "        # then lowercase letters of the Polish alphabet: ą, ć, ę, ł, ń, ó, ś, ź, ż\n",
                "        62: \"ą\",\n",
                "        63: \"ć\",\n",
                "        64: \"ę\",\n",
                "        65: \"ł\",\n",
                "        66: \"ń\",\n",
                "        67: \"ó\",\n",
                "        68: \"ś\",\n",
                "        69: \"ź\",\n",
                "        70: \"ż\",\n",
                "        # then uppercase letters of the Polish alphabet: Ą, Ć, Ę, Ł, Ń, Ó, Ś, Ź, Ż\n",
                "        71: \"Ą\",\n",
                "        72: \"Ć\",\n",
                "        73: \"Ę\",\n",
                "        74: \"Ł\",\n",
                "        75: \"Ń\",\n",
                "        76: \"Ó\",\n",
                "        77: \"Ś\",\n",
                "        78: \"Ź\",\n",
                "        79: \"Ż\",\n",
                "        # then special characters: + - : ; $ ! ? @\n",
                "        80: \"+\",\n",
                "        81: \"-\",\n",
                "        82: \":\",\n",
                "        83: \";\",\n",
                "        84: \"$\",\n",
                "        85: \"!\",\n",
                "        86: \"?\",\n",
                "        87: \"@\",\n",
                "        88: \".\",\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load data from raw images (.png files)\n",
                "\n",
                "run = wandb.init(project=\"master-thesis\", job_type=\"preprocessing\")\n",
                "split_paths = load_raw_image_data(run=run)\n",
                "\n",
                "ds_train = tf.keras.utils.image_dataset_from_directory(\n",
                "        split_paths[0],\n",
                "        image_size=(32, 32),\n",
                "        color_mode=\"grayscale\",\n",
                "    )\n",
                "\n",
                "ds_test = tf.keras.utils.image_dataset_from_directory(\n",
                "        split_paths[1],\n",
                "        image_size=(32, 32),\n",
                "        color_mode=\"grayscale\",\n",
                "    )\n",
                "\n",
                "ds_val = tf.keras.utils.image_dataset_from_directory(\n",
                "        split_paths[2],\n",
                "        image_size=(32, 32),\n",
                "        color_mode=\"grayscale\",\n",
                "    )\n",
                "\n",
                "number_of_classes = len(ds_train.class_names)\n",
                "\n",
                "# save datasets on disk then upload to wandb as artifacts\n",
                "\n",
                "output_dir = pathlib.Path(\"./datasets\").resolve()\n",
                "output_dir.mkdir(exist_ok=True)\n",
                "\n",
                "ds_train.save(str(output_dir / \"train\"), compression=\"GZIP\")\n",
                "ds_val.save(str(output_dir / \"val\"), compression=\"GZIP\")\n",
                "ds_test.save(str(output_dir / \"test\"), compression=\"GZIP\")\n",
                "\n",
                "artifact = wandb.Artifact(\"letters_splits_tfds\", type=\"dataset\", description=\"Dataset splits in tf.data.Dataset format\")\n",
                "artifact.add_dir(output_dir)\n",
                "run.log_artifact(artifact)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mwandb\u001b[0m:   9 of 9 files downloaded.  \n"
                    ]
                }
            ],
            "source": [
                "# load data from tf.data.Dataset artifacts\n",
                "\n",
                "ds_train, ds_test, ds_val = load_tfdataset_data(run)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/home/wflis/code/master-thesis/notebooks/datasets)... Done. 0.1s\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<wandb.sdk.wandb_artifacts.Artifact at 0x7ff925acc0a0>"
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# save datasets on disk then upload to wandb as artifacts\n",
                "\n",
                "output_dir = pathlib.Path(\"./datasets\").resolve()\n",
                "output_dir.mkdir(exist_ok=True)\n",
                "\n",
                "ds_train.save(str(output_dir / \"train\"), compression=\"GZIP\")\n",
                "ds_val.save(str(output_dir / \"val\"), compression=\"GZIP\")\n",
                "ds_test.save(str(output_dir / \"test\"), compression=\"GZIP\")\n",
                "\n",
                "artifact = wandb.Artifact(\"letters_splits_tfds\", type=\"dataset\", description=\"Dataset splits in tf.data.Dataset format\")\n",
                "artifact.add_dir(output_dir)\n",
                "run.log_artifact(artifact)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "\n",
                "CLASSES = list(char_id_to_class_name.keys())\n",
                "\n",
                "# TFRecord util functions\n",
                "\n",
                "def decode_jpeg_and_label(filename):\n",
                "  bits = tf.io.read_file(filename)\n",
                "  image = tf.io.decode_jpeg(bits)\n",
                "  # parse flower name from containing directory\n",
                "  label = tf.strings.split(tf.expand_dims(filename, axis=-1), sep='/')\n",
                "  label = label.values[-2]\n",
                "  return image, label\n",
                "\n",
                "def recompress_image(image, label):\n",
                "  height = tf.shape(image)[0]\n",
                "  width = tf.shape(image)[1]\n",
                "  image = tf.cast(image, tf.uint8)\n",
                "  image = tf.image.encode_jpeg(image, optimize_size=True, chroma_downsampling=False)\n",
                "  return image, label, height, width\n",
                "\n",
                "# Three types of data can be stored in TFRecords: bytestrings, integers and floats\n",
                "# They are always stored as lists, a single data element will be a list of size 1\n",
                "\n",
                "def _bytestring_feature(list_of_bytestrings):\n",
                "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))\n",
                "\n",
                "def _int_feature(list_of_ints): # int64\n",
                "  return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\n",
                "\n",
                "def _float_feature(list_of_floats): # float32\n",
                "  return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))\n",
                "\n",
                "def to_tfrecord(tfrec_filewriter, img_bytes, label, height, width):  \n",
                "  class_num = np.argmax(np.array(CLASSES)==label) # 'roses' => 2 (order defined in CLASSES)\n",
                "  one_hot_class = np.eye(len(CLASSES))[class_num]     # [0, 0, 1, 0, 0] for class #2, roses\n",
                "\n",
                "  feature = {\n",
                "      \"image\": _bytestring_feature([img_bytes]), # one image in the list\n",
                "      \"class\": _int_feature([class_num]),        # one class in the list\n",
                "      \n",
                "      # additional (not very useful) fields to demonstrate TFRecord writing/reading of different types of data\n",
                "      \"label\":         _bytestring_feature([label]),          # fixed length (1) list of strings, the text label\n",
                "      \"size\":          _int_feature([height, width]),         # fixed length (2) list of ints\n",
                "      \"one_hot_class\": _float_feature(one_hot_class.tolist()) # variable length  list of floats, n=len(CLASSES)\n",
                "  }\n",
                "  return tf.train.Example(features=tf.train.Features(feature=feature))\n",
                "\n",
                "def write_tfrecords(split_path, data_dir, shards=16, output_dir=\"./datasets_tfrecords\"):\n",
                "  output_dir = pathlib.Path(output_dir).resolve()\n",
                "  output_dir.mkdir(exist_ok=True)\n",
                "\n",
                "  DATA_OUTPUT = str(output_dir) \n",
                "  DATA_PATTERN = f\"{split_path}/*/*.png\" \n",
                "  SHARDS = shards\n",
                "  AUTOTUNE = tf.data.AUTOTUNE\n",
                "\n",
                "  nb_images = len(tf.io.gfile.glob(DATA_PATTERN))\n",
                "  shard_size = math.ceil(1.0 * nb_images / SHARDS)\n",
                "  print(\"Pattern matches {} images which will be rewritten as {} .tfrec files containing {} images each.\".format(nb_images, SHARDS, shard_size))\n",
                "\n",
                "  filenames = tf.data.Dataset.list_files(DATA_PATTERN, seed=35155) # This also shuffles the images\n",
                "  dataset1 = filenames.map(decode_jpeg_and_label, num_parallel_calls=AUTOTUNE)\n",
                "\n",
                "  dataset3 = dataset1.map(recompress_image, num_parallel_calls=AUTOTUNE)\n",
                "  dataset3 = dataset3.batch(shard_size) # sharding: there will be one \"batch\" of images per file \n",
                "\n",
                "  print(\"Writing TFRecords\")\n",
                "  for shard, (image, label, height, width) in enumerate(dataset3):\n",
                "    # batch size used as shard size here\n",
                "    shard_size = image.numpy().shape[0]\n",
                "    # good practice to have the number of records in the filename\n",
                "    filename = DATA_OUTPUT + \"{:02d}-{}.tfrec\".format(shard, shard_size)\n",
                "    \n",
                "    with tf.io.TFRecordWriter(filename) as out_file:\n",
                "      for i in range(shard_size):\n",
                "        example = to_tfrecord(out_file,\n",
                "                              image.numpy()[i], # re-compressed image: already a byte string\n",
                "                              label.numpy()[i],\n",
                "                              height.numpy()[i],\n",
                "                              width.numpy()[i])\n",
                "        out_file.write(example.SerializeToString())\n",
                "      print(\"Wrote file {} containing {} records\".format(filename, shard_size))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "output_dir = pathlib.Path(\"./datasets_tfrecords\").resolve()\n",
                "output_dir.mkdir(exist_ok=True)\n",
                "\n",
                "shards=16\n",
                "output_dir=\"./datasets_tfrecords\"\n",
                "\n",
                "split_path = split_paths[0]\n",
                "DATA_OUTPUT = str(output_dir) \n",
                "DATA_PATTERN = f\"{split_path}/*/*.png\" \n",
                "SHARDS = shards\n",
                "AUTOTUNE = tf.data.AUTOTUNE\n",
                "\n",
                "nb_images = len(tf.io.gfile.glob(DATA_PATTERN))\n",
                "shard_size = math.ceil(1.0 * nb_images / SHARDS)\n",
                "print(\"Pattern matches {} images which will be rewritten as {} .tfrec files containing {} images each.\".format(nb_images, SHARDS, shard_size))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "filenames = tf.data.Dataset.list_files(DATA_PATTERN, seed=35155) # This also shuffles the images\n",
                "dataset1 = filenames.map(decode_jpeg_and_label, num_parallel_calls=AUTOTUNE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset3 = dataset1.map(recompress_image, num_parallel_calls=AUTOTUNE)\n",
                "dataset3 = dataset3.batch(shard_size) # sharding: there will be one \"batch\" of images per file "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Writing TFRecords\")\n",
                "for shard, (image, label, height, width) in enumerate(dataset3):\n",
                "    # batch size used as shard size here\n",
                "    shard_size = image.numpy().shape[0]\n",
                "    # good practice to have the number of records in the filename\n",
                "    filename = DATA_OUTPUT + \"/{:02d}-{}.tfrec\".format(shard, shard_size)\n",
                "\n",
                "\n",
                "    with tf.io.TFRecordWriter(filename) as out_file:\n",
                "        for i in range(shard_size):\n",
                "            '''\n",
                "            example = to_tfrecord(out_file,\n",
                "                                    image.numpy()[i], # re-compressed image: already a byte string\n",
                "                                    label.numpy()[i],\n",
                "                                    height.numpy()[i],\n",
                "                                    width.numpy()[i])\n",
                "            '''\n",
                "            #out_file.write(example.SerializeToString())\n",
                "            print(\"Wrote file {} containing {} records\".format(filename, shard_size))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save as TFRecords and upload to WandB\n",
                "\n",
                "tfrecord_dir = \"./datasets_tfrecords\"\n",
                "for split_path in split_paths:\n",
                "  write_tfrecords(split_path, tfrecord_dir)\n",
                "\n",
                "artifact = wandb.Artifact(\"letters_splits_tfds\", type=\"dataset\", description=\"Dataset splits in tf.data.TFRecord format\")\n",
                "artifact.add_dir(tfrecord_dir)\n",
                "run.log_artifact(artifact)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# calculate class count for each split\n",
                "train_class_count = np.zeros(number_of_classes)\n",
                "for _, label in ds_train:\n",
                "    train_class_count += tf.math.bincount(label, minlength=number_of_classes)\n",
                "\n",
                "val_class_count = np.zeros(number_of_classes)\n",
                "for _, label in ds_val:\n",
                "    val_class_count += tf.math.bincount(label, minlength=number_of_classes)\n",
                "\n",
                "# plot class count for each split\n",
                "plt.bar(ds_train.class_names, train_class_count)\n",
                "plt.title(\"Train\")\n",
                "plt.show()\n",
                "\n",
                "plt.bar(ds_val.class_names, val_class_count)\n",
                "\n",
                "# log class count for each split to wandb\n",
                "\n",
                "wandb.log({\"train_class_count\": wandb.Histogram(train_class_count)})\n",
                "wandb.log({\"val_class_count\": wandb.Histogram(val_class_count)})\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "master-thesis",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.8"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "e25b199ea3671ac14fcd9257931c2d883fc8c97101fad7f035094360c4bcdb79"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
