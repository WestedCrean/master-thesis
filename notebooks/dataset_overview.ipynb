{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset overview\n",
    "\n",
    "In this notebook we review class counts in train and validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 16:42:31.186380: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import pathlib\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def load_raw_image_data(run) -> pathlib.Path:\n",
    "    \"\"\"\n",
    "    Unpacks data from an artifact into a folder and returns the path to the folder.\n",
    "    \"\"\"\n",
    "\n",
    "    artifact_name = f\"letters_splits\"\n",
    "    artifact = run.use_artifact(f\"master-thesis/{artifact_name}:latest\")\n",
    "    artifact_dir = pathlib.Path(\n",
    "        f\"./artifacts/{artifact.name.replace(':', '-')}\"\n",
    "    ).resolve()\n",
    "    if not artifact_dir.exists():\n",
    "        artifact_dir = artifact.download()\n",
    "        artifact_dir = pathlib.Path(artifact_dir).resolve()\n",
    "        for split_file in artifact_dir.iterdir():\n",
    "            if split_file.name.endswith(\".tar.gz\"):\n",
    "                split = split_file.name.replace(\".tar.gz\", \"\")\n",
    "                shutil.unpack_archive(split_file, artifact_dir / split, format=\"gztar\")\n",
    "\n",
    "    return [artifact_dir / split for split in [\"train\", \"test\", \"val\"]]\n",
    "\n",
    "def load_tfdataset_data(run) -> List[tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Downloads datasets from a wandb artifact and loads them into a list of tf.data.Datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    artifact_name = f\"letters_splits_tfds\"\n",
    "    artifact = run.use_artifact(f\"master-thesis/{artifact_name}:latest\")\n",
    "    artifact_dir = pathlib.Path(\n",
    "        f\"./artifacts/{artifact.name.replace(':', '-')}\"\n",
    "    ).resolve()\n",
    "    if not artifact_dir.exists():\n",
    "        artifact_dir = artifact.download()\n",
    "        artifact_dir = pathlib.Path(artifact_dir).resolve()\n",
    "\n",
    "    # if tf.__version__ minor is less than 10, use\n",
    "    # tf.data.experimental.load instead of tf.data.Dataset.load\n",
    "\n",
    "    if int(tf.__version__.split(\".\")[1]) < 10:\n",
    "        load_function = tf.data.experimental.load\n",
    "    else:\n",
    "        load_function = tf.data.Dataset.load\n",
    "    \n",
    "    output_list = []\n",
    "    for split in [\"train\", \"test\", \"val\"]:\n",
    "        ds = load_function(str(artifact_dir / split), compression=\"GZIP\")\n",
    "        output_list.append(ds)\n",
    "    \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_id_to_class_name = {\n",
    "        0: \"0\",\n",
    "        1: \"1\",\n",
    "        2: \"2\",\n",
    "        3: \"3\",\n",
    "        4: \"4\",\n",
    "        5: \"5\",\n",
    "        6: \"6\",\n",
    "        7: \"7\",\n",
    "        8: \"8\",\n",
    "        9: \"9\",\n",
    "        10: \"a\",\n",
    "        11: \"b\",\n",
    "        12: \"c\",\n",
    "        13: \"d\",\n",
    "        14: \"e\",\n",
    "        15: \"f\",\n",
    "        16: \"g\",\n",
    "        17: \"h\",\n",
    "        18: \"i\",\n",
    "        19: \"j\",\n",
    "        20: \"k\",\n",
    "        21: \"l\",\n",
    "        22: \"m\",\n",
    "        23: \"n\",\n",
    "        24: \"o\",\n",
    "        25: \"p\",\n",
    "        26: \"q\",\n",
    "        27: \"r\",\n",
    "        28: \"s\",\n",
    "        29: \"t\",\n",
    "        30: \"u\",\n",
    "        31: \"v\",\n",
    "        32: \"w\",\n",
    "        33: \"x\",\n",
    "        34: \"y\",\n",
    "        35: \"z\",\n",
    "        36: \"A\",\n",
    "        37: \"B\",\n",
    "        38: \"C\",\n",
    "        39: \"D\",\n",
    "        40: \"E\",\n",
    "        41: \"F\",\n",
    "        42: \"G\",\n",
    "        43: \"H\",\n",
    "        44: \"I\",\n",
    "        45: \"J\",\n",
    "        46: \"K\",\n",
    "        47: \"L\",\n",
    "        48: \"M\",\n",
    "        49: \"N\",\n",
    "        50: \"O\",\n",
    "        51: \"P\",\n",
    "        52: \"Q\",\n",
    "        53: \"R\",\n",
    "        54: \"S\",\n",
    "        55: \"T\",\n",
    "        56: \"U\",\n",
    "        57: \"V\",\n",
    "        58: \"W\",\n",
    "        59: \"X\",\n",
    "        60: \"Y\",\n",
    "        61: \"Z\",\n",
    "        # then lowercase letters of the Polish alphabet: ą, ć, ę, ł, ń, ó, ś, ź, ż\n",
    "        62: \"ą\",\n",
    "        63: \"ć\",\n",
    "        64: \"ę\",\n",
    "        65: \"ł\",\n",
    "        66: \"ń\",\n",
    "        67: \"ó\",\n",
    "        68: \"ś\",\n",
    "        69: \"ź\",\n",
    "        70: \"ż\",\n",
    "        # then uppercase letters of the Polish alphabet: Ą, Ć, Ę, Ł, Ń, Ó, Ś, Ź, Ż\n",
    "        71: \"Ą\",\n",
    "        72: \"Ć\",\n",
    "        73: \"Ę\",\n",
    "        74: \"Ł\",\n",
    "        75: \"Ń\",\n",
    "        76: \"Ó\",\n",
    "        77: \"Ś\",\n",
    "        78: \"Ź\",\n",
    "        79: \"Ż\",\n",
    "        # then special characters: + - : ; $ ! ? @\n",
    "        80: \"+\",\n",
    "        81: \"-\",\n",
    "        82: \":\",\n",
    "        83: \";\",\n",
    "        84: \"$\",\n",
    "        85: \"!\",\n",
    "        86: \"?\",\n",
    "        87: \"@\",\n",
    "        88: \".\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgratkadlafana\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20230122_164249-o0d422wn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/gratkadlafana/master-thesis/runs/o0d422wn\" target=\"_blank\">gallant-bird-176</a></strong> to <a href=\"https://wandb.ai/gratkadlafana/master-thesis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/gratkadlafana/master-thesis\" target=\"_blank\">https://wandb.ai/gratkadlafana/master-thesis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/gratkadlafana/master-thesis/runs/o0d422wn\" target=\"_blank\">https://wandb.ai/gratkadlafana/master-thesis/runs/o0d422wn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"master-thesis\", job_type=\"preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from raw images (.png files)\n",
    "split_paths = load_raw_image_data(run=run)\n",
    "\n",
    "ds_train = tf.keras.utils.image_dataset_from_directory(\n",
    "        split_paths[0],\n",
    "        image_size=(32, 32),\n",
    "        color_mode=\"grayscale\",\n",
    "    )\n",
    "\n",
    "ds_test = tf.keras.utils.image_dataset_from_directory(\n",
    "        split_paths[1],\n",
    "        image_size=(32, 32),\n",
    "        color_mode=\"grayscale\",\n",
    "    )\n",
    "\n",
    "ds_val = tf.keras.utils.image_dataset_from_directory(\n",
    "        split_paths[2],\n",
    "        image_size=(32, 32),\n",
    "        color_mode=\"grayscale\",\n",
    "    )\n",
    "\n",
    "number_of_classes = len(ds_train.class_names)\n",
    "\n",
    "# save datasets on disk then upload to wandb as artifacts\n",
    "\n",
    "output_dir = pathlib.Path(\"./datasets\").resolve()\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "ds_train.save(str(output_dir / \"train\"), compression=\"GZIP\")\n",
    "ds_val.save(str(output_dir / \"val\"), compression=\"GZIP\")\n",
    "ds_test.save(str(output_dir / \"test\"), compression=\"GZIP\")\n",
    "\n",
    "artifact = wandb.Artifact(\"letters_splits_tfds\", type=\"dataset\", description=\"Dataset splits in tf.data.Dataset format\")\n",
    "artifact.add_dir(output_dir)\n",
    "run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact letters_splits_tfds:latest, 54.88MB. 18 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   18 of 18 files downloaded.  \n",
      "Done. 0:0:0.0\n",
      "2023-01-22 16:42:55.959841: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-22 16:42:55.963268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-22 16:42:55.963520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-22 16:42:55.963816: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-22 16:42:55.964346: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-22 16:42:55.964584: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-22 16:42:55.964808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-22 16:42:56.485625: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-22 16:42:56.485873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-22 16:42:56.485887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-01-22 16:42:56.486070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-22 16:42:56.486109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1616 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 SUPER, pci bus id: 0000:26:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# load data from tf.data.Dataset artifacts\n",
    "\n",
    "ds_train, ds_test, ds_val = load_tfdataset_data(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.8.2-py3-none-any.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (1.14.1)\n",
      "Collecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (2.28.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (1.23.4)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (5.10.2)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting etils[enp,epath]>=0.9.0\n",
      "  Downloading etils-1.0.0-py3-none-any.whl (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.5/146.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (3.19.6)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (2.1.0)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.12.0-py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (5.9.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (8.1.3)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (1.3.0)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.8/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (4.4.0)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.8/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (3.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from promise->tensorflow-datasets) (1.14.0)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.58.0-py2.py3-none-any.whl (223 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.0/223.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21485 sha256=44464148027313ca52cecd7ddaee3c7539caf3d3372b8d4442a9df01f0811902\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/fe/dc/a7b3e03dfd0afb3a19691905bbafac1fbaebb704a02a4daeb2\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, tqdm, toml, promise, googleapis-common-protos, etils, dill, tensorflow-metadata, tensorflow-datasets\n",
      "Successfully installed dill-0.3.6 dm-tree-0.1.8 etils-1.0.0 googleapis-common-protos-1.58.0 promise-2.3 tensorflow-datasets-4.8.2 tensorflow-metadata-1.12.0 toml-0.10.2 tqdm-4.64.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************ Summary ************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1377it [00:01, 1300.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples/sec (First included) 162587.17 ex/sec (total: 176384 ex, 1.08 sec)\n",
      "Examples/sec (First only) 4062.55 ex/sec (total: 128 ex, 0.03 sec)\n",
      "Examples/sec (First excluded) 167328.88 ex/sec (total: 176256 ex, 1.05 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>BenchmarkResult:</strong><br/><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>num_examples</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>first+lasts</th>\n",
       "      <td>1.084858</td>\n",
       "      <td>176384</td>\n",
       "      <td>162587.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>0.031507</td>\n",
       "      <td>128</td>\n",
       "      <td>4062.549036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lasts</th>\n",
       "      <td>1.053351</td>\n",
       "      <td>176256</td>\n",
       "      <td>167328.877389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "BenchmarkResult(stats=             duration  num_examples            avg\n",
       "first+lasts  1.084858        176384  162587.166667\n",
       "first        0.031507           128    4062.549036\n",
       "lasts        1.053351        176256  167328.877389, raw_stats=                      duration\n",
       "start_time        11173.783236\n",
       "first_batch_time  11173.814743\n",
       "end_time          11174.868094\n",
       "num_iter           1377.000000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "def preprocess_dataset(ds: tf.data.Dataset, batch_size: int, cache: bool = True) -> tf.data.Dataset:\n",
    "    ds = ds.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y))  # normalize\n",
    "    ds = ds.unbatch().batch(batch_size)\n",
    "    if cache:\n",
    "        ds = ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "ds_train_not_cached = preprocess_dataset(ds_train, 128, cache=False)\n",
    "\n",
    "tfds.benchmark(ds_train_not_cached, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************ Summary ************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "345it [00:01, 316.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples/sec (First included) 79659.94 ex/sec (total: 88576 ex, 1.11 sec)\n",
      "Examples/sec (First only) 8668.86 ex/sec (total: 256 ex, 0.03 sec)\n",
      "Examples/sec (First excluded) 81596.79 ex/sec (total: 88320 ex, 1.08 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>BenchmarkResult:</strong><br/><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>num_examples</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>first+lasts</th>\n",
       "      <td>1.111927</td>\n",
       "      <td>88576</td>\n",
       "      <td>79659.936250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>0.029531</td>\n",
       "      <td>256</td>\n",
       "      <td>8668.856162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lasts</th>\n",
       "      <td>1.082396</td>\n",
       "      <td>88320</td>\n",
       "      <td>81596.786058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "BenchmarkResult(stats=             duration  num_examples           avg\n",
       "first+lasts  1.111927         88576  79659.936250\n",
       "first        0.029531           256   8668.856162\n",
       "lasts        1.082396         88320  81596.786058, raw_stats=                      duration\n",
       "start_time        11268.091955\n",
       "first_batch_time  11268.121486\n",
       "end_time          11269.203881\n",
       "num_iter            345.000000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train_cached = preprocess_dataset(ds_train, 128*4, cache=True)\n",
    "\n",
    "tfds.benchmark(ds_train_cached, batch_size=128*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************ Summary ************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "345it [00:00, 5092.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples/sec (First included) 2502097.77 ex/sec (total: 177152 ex, 0.07 sec)\n",
      "Examples/sec (First only) 114783.97 ex/sec (total: 512 ex, 0.00 sec)\n",
      "Examples/sec (First excluded) 2662613.38 ex/sec (total: 176640 ex, 0.07 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>BenchmarkResult:</strong><br/><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>num_examples</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>first+lasts</th>\n",
       "      <td>0.070801</td>\n",
       "      <td>177152</td>\n",
       "      <td>2.502098e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>0.004461</td>\n",
       "      <td>512</td>\n",
       "      <td>1.147840e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lasts</th>\n",
       "      <td>0.066341</td>\n",
       "      <td>176640</td>\n",
       "      <td>2.662613e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "BenchmarkResult(stats=             duration  num_examples           avg\n",
       "first+lasts  0.070801        177152  2.502098e+06\n",
       "first        0.004461           512  1.147840e+05\n",
       "lasts        0.066341        176640  2.662613e+06, raw_stats=                      duration\n",
       "start_time        11271.670544\n",
       "first_batch_time  11271.675004\n",
       "end_time          11271.741345\n",
       "num_iter            345.000000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.benchmark(ds_train_cached, batch_size=128*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/home/wflis/code/master-thesis/notebooks/datasets)... Done. 0.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_artifacts.Artifact at 0x7ff925acc0a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save datasets on disk then upload to wandb as artifacts\n",
    "\n",
    "output_dir = pathlib.Path(\"./datasets\").resolve()\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "ds_train.save(str(output_dir / \"train\"), compression=\"GZIP\")\n",
    "ds_val.save(str(output_dir / \"val\"), compression=\"GZIP\")\n",
    "ds_test.save(str(output_dir / \"test\"), compression=\"GZIP\")\n",
    "\n",
    "artifact = wandb.Artifact(\"letters_splits_tfds\", type=\"dataset\", description=\"Dataset splits in tf.data.Dataset format\")\n",
    "artifact.add_dir(output_dir)\n",
    "run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "CLASSES = list(char_id_to_class_name.keys())\n",
    "\n",
    "# TFRecord util functions\n",
    "\n",
    "def decode_jpeg_and_label(filename):\n",
    "  bits = tf.io.read_file(filename)\n",
    "  image = tf.io.decode_jpeg(bits)\n",
    "  # parse flower name from containing directory\n",
    "  label = tf.strings.split(tf.expand_dims(filename, axis=-1), sep='/')\n",
    "  label = label.values[-2]\n",
    "  return image, label\n",
    "\n",
    "def recompress_image(image, label):\n",
    "  height = tf.shape(image)[0]\n",
    "  width = tf.shape(image)[1]\n",
    "  image = tf.cast(image, tf.uint8)\n",
    "  image = tf.image.encode_jpeg(image, optimize_size=True, chroma_downsampling=False)\n",
    "  return image, label, height, width\n",
    "\n",
    "# Three types of data can be stored in TFRecords: bytestrings, integers and floats\n",
    "# They are always stored as lists, a single data element will be a list of size 1\n",
    "\n",
    "def _bytestring_feature(list_of_bytestrings):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))\n",
    "\n",
    "def _int_feature(list_of_ints): # int64\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\n",
    "\n",
    "def _float_feature(list_of_floats): # float32\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))\n",
    "\n",
    "def to_tfrecord(tfrec_filewriter, img_bytes, label, height, width):  \n",
    "  class_num = np.argmax(np.array(CLASSES)==label) # 'roses' => 2 (order defined in CLASSES)\n",
    "  one_hot_class = np.eye(len(CLASSES))[class_num]     # [0, 0, 1, 0, 0] for class #2, roses\n",
    "\n",
    "  feature = {\n",
    "      \"image\": _bytestring_feature([img_bytes]), # one image in the list\n",
    "      \"class\": _int_feature([class_num]),        # one class in the list\n",
    "      \n",
    "      # additional (not very useful) fields to demonstrate TFRecord writing/reading of different types of data\n",
    "      \"label\":         _bytestring_feature([label]),          # fixed length (1) list of strings, the text label\n",
    "      \"size\":          _int_feature([height, width]),         # fixed length (2) list of ints\n",
    "      \"one_hot_class\": _float_feature(one_hot_class.tolist()) # variable length  list of floats, n=len(CLASSES)\n",
    "  }\n",
    "  return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "def write_tfrecords(split_path, data_dir, shards=16, output_dir=\"./datasets_tfrecords\"):\n",
    "  output_dir = pathlib.Path(output_dir).resolve()\n",
    "  output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "  DATA_OUTPUT = str(output_dir) \n",
    "  DATA_PATTERN = f\"{split_path}/*/*.png\" \n",
    "  SHARDS = shards\n",
    "  AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "  nb_images = len(tf.io.gfile.glob(DATA_PATTERN))\n",
    "  shard_size = math.ceil(1.0 * nb_images / SHARDS)\n",
    "  print(\"Pattern matches {} images which will be rewritten as {} .tfrec files containing {} images each.\".format(nb_images, SHARDS, shard_size))\n",
    "\n",
    "  filenames = tf.data.Dataset.list_files(DATA_PATTERN, seed=35155) # This also shuffles the images\n",
    "  dataset1 = filenames.map(decode_jpeg_and_label, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "  dataset3 = dataset1.map(recompress_image, num_parallel_calls=AUTOTUNE)\n",
    "  dataset3 = dataset3.batch(shard_size) # sharding: there will be one \"batch\" of images per file \n",
    "\n",
    "  print(\"Writing TFRecords\")\n",
    "  for shard, (image, label, height, width) in enumerate(dataset3):\n",
    "    # batch size used as shard size here\n",
    "    shard_size = image.numpy().shape[0]\n",
    "    # good practice to have the number of records in the filename\n",
    "    filename = DATA_OUTPUT + \"{:02d}-{}.tfrec\".format(shard, shard_size)\n",
    "    \n",
    "    with tf.io.TFRecordWriter(filename) as out_file:\n",
    "      for i in range(shard_size):\n",
    "        example = to_tfrecord(out_file,\n",
    "                              image.numpy()[i], # re-compressed image: already a byte string\n",
    "                              label.numpy()[i],\n",
    "                              height.numpy()[i],\n",
    "                              width.numpy()[i])\n",
    "        out_file.write(example.SerializeToString())\n",
    "      print(\"Wrote file {} containing {} records\".format(filename, shard_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = pathlib.Path(\"./datasets_tfrecords\").resolve()\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "shards=16\n",
    "output_dir=\"./datasets_tfrecords\"\n",
    "\n",
    "split_path = split_paths[0]\n",
    "DATA_OUTPUT = str(output_dir) \n",
    "DATA_PATTERN = f\"{split_path}/*/*.png\" \n",
    "SHARDS = shards\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "nb_images = len(tf.io.gfile.glob(DATA_PATTERN))\n",
    "shard_size = math.ceil(1.0 * nb_images / SHARDS)\n",
    "print(\"Pattern matches {} images which will be rewritten as {} .tfrec files containing {} images each.\".format(nb_images, SHARDS, shard_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = tf.data.Dataset.list_files(DATA_PATTERN, seed=35155) # This also shuffles the images\n",
    "dataset1 = filenames.map(decode_jpeg_and_label, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = dataset1.map(recompress_image, num_parallel_calls=AUTOTUNE)\n",
    "dataset3 = dataset3.batch(shard_size) # sharding: there will be one \"batch\" of images per file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing TFRecords\")\n",
    "for shard, (image, label, height, width) in enumerate(dataset3):\n",
    "    # batch size used as shard size here\n",
    "    shard_size = image.numpy().shape[0]\n",
    "    # good practice to have the number of records in the filename\n",
    "    filename = DATA_OUTPUT + \"/{:02d}-{}.tfrec\".format(shard, shard_size)\n",
    "\n",
    "\n",
    "    with tf.io.TFRecordWriter(filename) as out_file:\n",
    "        for i in range(shard_size):\n",
    "            '''\n",
    "            example = to_tfrecord(out_file,\n",
    "                                    image.numpy()[i], # re-compressed image: already a byte string\n",
    "                                    label.numpy()[i],\n",
    "                                    height.numpy()[i],\n",
    "                                    width.numpy()[i])\n",
    "            '''\n",
    "            #out_file.write(example.SerializeToString())\n",
    "            print(\"Wrote file {} containing {} records\".format(filename, shard_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as TFRecords and upload to WandB\n",
    "\n",
    "tfrecord_dir = \"./datasets_tfrecords\"\n",
    "for split_path in split_paths:\n",
    "  write_tfrecords(split_path, tfrecord_dir)\n",
    "\n",
    "artifact = wandb.Artifact(\"letters_splits_tfds\", type=\"dataset\", description=\"Dataset splits in tf.data.TFRecord format\")\n",
    "artifact.add_dir(tfrecord_dir)\n",
    "run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate class count for each split\n",
    "train_class_count = np.zeros(number_of_classes)\n",
    "for _, label in ds_train:\n",
    "    train_class_count += tf.math.bincount(label, minlength=number_of_classes)\n",
    "\n",
    "val_class_count = np.zeros(number_of_classes)\n",
    "for _, label in ds_val:\n",
    "    val_class_count += tf.math.bincount(label, minlength=number_of_classes)\n",
    "\n",
    "# plot class count for each split\n",
    "plt.bar(ds_train.class_names, train_class_count)\n",
    "plt.title(\"Train\")\n",
    "plt.show()\n",
    "\n",
    "plt.bar(ds_val.class_names, val_class_count)\n",
    "\n",
    "# log class count for each split to wandb\n",
    "\n",
    "wandb.log({\"train_class_count\": wandb.Histogram(train_class_count)})\n",
    "wandb.log({\"val_class_count\": wandb.Histogram(val_class_count)})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e25b199ea3671ac14fcd9257931c2d883fc8c97101fad7f035094360c4bcdb79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
