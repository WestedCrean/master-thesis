{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils functions\n",
    "\n",
    "import pathlib\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import wandb\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def load_data(run, artifact_name = \"phcd_paper_splits_tfds\") -> List[tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Downloads datasets from a wandb artifact and loads them into a list of tf.data.Datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    artifact = run.use_artifact(f\"master-thesis/{artifact_name}:latest\")\n",
    "    artifact_dir = pathlib.Path(\n",
    "        f\"./artifacts/{artifact.name.replace(':', '-')}\"\n",
    "    ).resolve()\n",
    "    if not artifact_dir.exists():\n",
    "        artifact_dir = artifact.download()\n",
    "        artifact_dir = pathlib.Path(artifact_dir).resolve()\n",
    "\n",
    "    # if tf.__version__ minor is less than 10, use\n",
    "    # tf.data.experimental.load instead of tf.data.Dataset.load\n",
    "\n",
    "    if int(tf.__version__.split(\".\")[1]) < 10:\n",
    "        load_function = tf.data.experimental.load\n",
    "    else:\n",
    "        load_function = tf.data.Dataset.load\n",
    "    \n",
    "    output_list = []\n",
    "    for split in [\"train\", \"test\", \"val\"]:\n",
    "        ds = load_function(str(artifact_dir / split), compression=\"GZIP\")\n",
    "        output_list.append(ds)\n",
    "    \n",
    "    return output_list\n",
    "\n",
    "def get_readable_class_labels(subset = 'phcd_paper'):\n",
    "    if subset == 'phcd_paper':\n",
    "        return ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c',\n",
    "        'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n",
    "        'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C',\n",
    "        'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P',\n",
    "        'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'ą', 'ć', 'ę',\n",
    "        'ł', 'ń', 'ó', 'ś', 'ź', 'ż', 'Ą', 'Ć', 'Ę', 'Ł', 'Ń', 'Ó', 'Ś',\n",
    "        'Ź', 'Ż', '+', '-', ':', ';', '$', '!', '?', '@', '.']\n",
    "    elif subset == 'uppercase':\n",
    "        return ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'Ą', 'Ć', \n",
    "        'Ę', 'Ł', 'Ń', 'Ó', 'Ś', 'Ź', 'Ż']\n",
    "    elif subset == 'lowercase':\n",
    "        return ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "        'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ą', 'ć',\n",
    "        'ę', 'ł', 'ń', 'ó', 'ś', 'ź', 'ż']\n",
    "    elif subset == 'numbers':\n",
    "        return ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    elif subset == 'uppercase_no_diacritics':\n",
    "        return ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
    "        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "    elif subset == 'lowercase_no_diacritics':\n",
    "        return ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "        'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "def calculate_accuracy_per_class(model, test_dataset, test_dataset_name):\n",
    "    '''\n",
    "    Calculates the accuracy per class for a given model and test dataset.\n",
    "\n",
    "    Returns dict with class labels as keys and accuracy as values.\n",
    "    '''\n",
    "        \n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    # get labels\n",
    "    y_true = test_dataset.map(lambda x, y: y).as_numpy_iterator()\n",
    "    y_true = np.concatenate(list(y_true))\n",
    "    # calculate accuracy per class\n",
    "    labels = get_readable_class_labels(test_dataset_name)\n",
    "    class_accuracy = np.zeros(len(labels))\n",
    "    for i, label in enumerate(labels):\n",
    "        class_accuracy[i] = np.sum(y_pred[y_true == i] == i) / np.sum(y_true == i)\n",
    "    return { label: acc for label, acc in zip(labels, class_accuracy) }\n",
    "    \n",
    "\n",
    "def plot_accuracy_per_class(class_accuracy_dict):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    labels = list(class_accuracy_dict.keys())\n",
    "    class_accuracy = list(class_accuracy_dict.values())\n",
    "    plt.bar(labels, class_accuracy)\n",
    "    plt.xticks(labels)\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy per class\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def accuracy_table(class_accuracy_dict):\n",
    "    labels = list(class_accuracy_dict.keys())\n",
    "    class_accuracy = list(class_accuracy_dict.values())\n",
    "    return wandb.Table(columns=[\"Class\", \"Accuracy\"], data=list(zip(labels, class_accuracy)))\n",
    "\n",
    "def get_number_of_classes(ds: tf.data.Dataset) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of classes in a dataset.\n",
    "    \"\"\"\n",
    "    labels_iterator= ds.map(lambda x, y: y).as_numpy_iterator()\n",
    "    labels = np.concatenate(list(labels_iterator))\n",
    "    return len(np.unique(labels))\n",
    "\n",
    "def get_number_of_examples(ds: tf.data.Dataset) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of examples in a dataset.\n",
    "    \"\"\"\n",
    "    return sum(1 for _ in ds)\n",
    "\n",
    "def preprocess_dataset(ds: tf.data.Dataset, batch_size: int, cache: bool = True) -> tf.data.Dataset:\n",
    "    ds = ds.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y))  # normalize\n",
    "    ds = ds.unbatch().batch(batch_size)\n",
    "    if cache:\n",
    "        ds = ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def calculate_model_compressed_size_on_disk(path: str) -> int:\n",
    "    compressed_path = path + \".zip\"\n",
    "    with zipfile.ZipFile(compressed_path, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(path)\n",
    "    return pathlib.Path(compressed_path).stat().st_size    \n",
    "\n",
    "def calculate_model_size_on_disk(path: str) -> int:\n",
    "    return pathlib.Path(path).stat().st_size\n",
    "\n",
    "def calculate_model_num_parameters(model: tf.keras.Model) -> int:\n",
    "    return model.count_params()\n",
    "\n",
    "def calculate_model_flops(summary) -> float:\n",
    "    # from run.summary get GFLOPs or GFLOPS whichever is available\n",
    "    if \"GFLOPs\" in summary.keys():\n",
    "        return summary.get(\"GFLOPs\")\n",
    "    elif \"GFLOPS\" in summary.keys():\n",
    "        return summary.get(\"GFLOPS\")\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "project_name = \"master-thesis\"\n",
    "run_id = \"h8uj5bw9\"\n",
    "\n",
    "def get_model_from_run(run_id, project_name=\"master-thesis\"):\n",
    "    # from project run_id artifacts get file_name\n",
    "    api = wandb.Api()\n",
    "    run = api.run(f\"{project_name}/{run_id}\")\n",
    "    artifact = run.file(\"model_baseline.h5\").download(replace=True)\n",
    "    model = tf.keras.models.load_model(artifact.name)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model, pathlib.Path(artifact.name)\n",
    "\n",
    "model, model_path = get_model_from_run(run_id, project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = dict(\n",
    "    batch_size=32*2,\n",
    "    epochs=60,    \n",
    "    optimizer=\"adam\"\n",
    ")\n",
    "\n",
    "artifact_base_name = \"phcd_paper\"\n",
    "artifact_name = f\"{artifact_base_name}_splits_tfds\" # \"phcd_paper_splits_tfds\n",
    "run = wandb.init(project=\"master-thesis\", job_type=\"optimization\",  config=defaults, tags=[\"optimization\"])\n",
    "\n",
    "# hyperparameters\n",
    "epochs = wandb.config.epochs\n",
    "bs = wandb.config.batch_size\n",
    "\n",
    "ds_train, ds_test, ds_val = load_data(run, artifact_name=artifact_name)\n",
    "\n",
    "num_classes = get_number_of_classes(ds_val)\n",
    "\n",
    "ds_train = preprocess_dataset(ds_train, batch_size=bs)\n",
    "ds_val = preprocess_dataset(ds_val, batch_size=bs)\n",
    "ds_test = preprocess_dataset(ds_test, batch_size=bs, cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# before optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(bench_model, bench_model_path, benech_dataset):\n",
    "    test_loss_before, test_acc_before = bench_model.evaluate(ds_test)\n",
    "    num_parameters = calculate_model_num_parameters(bench_model)\n",
    "    compressed_disk_size = calculate_model_compressed_size_on_disk(bench_model_path)\n",
    "\n",
    "    print(f\"Test accuracy: {test_acc_before}\")\n",
    "    print(f\"Number of parameters: {num_parameters}\")\n",
    "    print(f\"Compressed disk size: {compressed_disk_size}\")\n",
    "\n",
    "print(f\"Benchmark before optimization:\")\n",
    "benchmark(model, str(model_path), ds_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import openvino.runtime as ov\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from keras.utils import data_utils\n",
    "from openvino.tools import mo\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nncf\n",
    "\n",
    "DATASET_CLASSES = 89\n",
    "\n",
    "def validate(model: ov.Model, \n",
    "             val_loader: tf.data.Dataset) -> tf.Tensor:\n",
    "    compiled_model = ov.compile_model(model)\n",
    "    output = compiled_model.outputs[0]\n",
    "\n",
    "    metric = tf.keras.metrics.CategoricalAccuracy(name='acc@1')\n",
    "    for images, labels in tqdm(val_loader):\n",
    "        pred = compiled_model(images)[output]\n",
    "        metric.update_state(labels, pred) \n",
    "   \n",
    "    return metric.result()\n",
    "\n",
    "def run_benchmark(model_path: str, shape: Optional[List[int]] = None, \n",
    "                  verbose: bool = True) -> float:\n",
    "    command = f'benchmark_app -m {model_path} -d CPU -api async -t 15'\n",
    "    if shape is not None:\n",
    "        command += f' -shape [{\",\".join(str(x) for x in shape)}]'\n",
    "    cmd_output = subprocess.check_output(command, shell=True) # nosec\n",
    "    if verbose:\n",
    "        print(*str(cmd_output).split('\\\\n')[-9:-1], sep='\\n')\n",
    "    match = re.search(r\"Throughput\\: (.+?) FPS\", str(cmd_output))\n",
    "    return float(match.group(1))\n",
    "\n",
    "\n",
    "def get_model_size(ir_path: str, m_type: str = 'Mb', \n",
    "                   verbose: bool = True) -> float:\n",
    "    xml_size = os.path.getsize(ir_path)\n",
    "    bin_size = os.path.getsize(os.path.splitext(ir_path)[0] + '.bin')\n",
    "    for t in ['bytes', 'Kb', 'Mb']:\n",
    "        if m_type == t:\n",
    "            break\n",
    "        xml_size /= 1024\n",
    "        bin_size /= 1024\n",
    "    model_size = xml_size + bin_size \n",
    "    if verbose:\n",
    "        print(f'Model graph (xml):   {xml_size:.3f} Mb')\n",
    "        print(f'Model weights (bin): {bin_size:.3f} Mb')\n",
    "        print(f'Model size:          {model_size:.3f} Mb')      \n",
    "    return model_size\n",
    "\n",
    "def transform_fn(data_item):\n",
    "    images, _ = data_item\n",
    "    return images\n",
    "\n",
    "\n",
    "calibration_dataset = nncf.Dataset(ds_val, transform_fn)\n",
    "quantized_model = nncf.quantize(model, calibration_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_model = mo.convert_model(model)\n",
    "ov_quantized_model = mo.convert_model(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path().parent.resolve()\n",
    "model_name = \"arch-6\"\n",
    "\n",
    "fp32_ir_path = f'{ROOT}/models/{model_name}_fp32.xml'\n",
    "ov.serialize(ov_model, fp32_ir_path)\n",
    "print(f'[1/7] Save FP32 model: {fp32_ir_path}')\n",
    "fp32_model_size = get_model_size(fp32_ir_path, verbose=True)\n",
    "\n",
    "int8_ir_path = f'{ROOT}/models/{model_name}_int8.xml'\n",
    "ov.serialize(ov_quantized_model, int8_ir_path)\n",
    "print(f'[2/7] Save INT8 model: {int8_ir_path}')\n",
    "int8_model_size = get_model_size(int8_ir_path, verbose=True)\n",
    "\n",
    "print('[3/7] Benchmark FP32 model:')\n",
    "fp32_fps = run_benchmark(fp32_ir_path, shape=[1,32,32,1], verbose=True)\n",
    "print('[4/7] Benchmark INT8 model:')\n",
    "int8_fps = run_benchmark(int8_ir_path, shape=[1,32,32,1], verbose=True)\n",
    "\n",
    "print('[5/7] Validate OpenVINO FP32 model:')\n",
    "fp32_top1 = validate(ov_model, calibration_dataset)\n",
    "print(f'Accuracy @ top1: {fp32_top1:.3f}')\n",
    "\n",
    "print('[6/7] Validate OpenVINO INT8 model:')\n",
    "int8_top1 = validate(ov_quantized_model, calibration_dataset)\n",
    "print(f'Accuracy @ top1: {int8_top1:.3f}')\n",
    "\n",
    "print('[7/7] Report:')\n",
    "print(f'Accuracy drop: {fp32_top1 - int8_top1:.3f}')\n",
    "print(f'Model compression rate: {fp32_model_size / int8_model_size:.3f}')\n",
    "# https://docs.openvino.ai/latest/openvino_docs_optimization_guide_dldt_optimization_guide.html\n",
    "print(f'Performance speed up (throughput mode): {int8_fps / fp32_fps:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nncf import NNCFConfig\n",
    "from nncf.tensorflow.helpers.model_creation import create_compressed_model\n",
    "from nncf.tensorflow.initialization import register_default_init_args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nncf_config_dict = {\n",
    "    \"input_info\": {\"sample_size\": [1, 3] + list(IMG_SIZE)},\n",
    "    \"log_dir\": str(OUTPUT_DIR),  # The log directory for NNCF-specific logging outputs.\n",
    "    \"compression\": {\n",
    "        \"algorithm\": \"quantization\",  # Specify the algorithm here.\n",
    "    },\n",
    "}\n",
    "nncf_config = NNCFConfig.from_dict(nncf_config_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Benchmark after optimization:\")\n",
    "benchmark(optimized_model, optimized_model_path, ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
