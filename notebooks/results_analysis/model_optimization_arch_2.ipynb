{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow-model-optimization wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils functions\n",
    "\n",
    "import pathlib\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import datetime \n",
    "import numpy as np\n",
    "import wandb\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def load_data(run, artifact_name = \"phcd_paper_splits_tfds\") -> List[tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Downloads datasets from a wandb artifact and loads them into a list of tf.data.Datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    artifact = run.use_artifact(f\"master-thesis/{artifact_name}:latest\")\n",
    "    artifact_dir = pathlib.Path(\n",
    "        f\"./artifacts/{artifact.name.replace(':', '-')}\"\n",
    "    ).resolve()\n",
    "    if not artifact_dir.exists():\n",
    "        artifact_dir = artifact.download()\n",
    "        artifact_dir = pathlib.Path(artifact_dir).resolve()\n",
    "\n",
    "    # if tf.__version__ minor is less than 10, use\n",
    "    # tf.data.experimental.load instead of tf.data.Dataset.load\n",
    "\n",
    "    if int(tf.__version__.split(\".\")[1]) < 10:\n",
    "        load_function = tf.data.experimental.load\n",
    "    else:\n",
    "        load_function = tf.data.Dataset.load\n",
    "    \n",
    "    output_list = []\n",
    "    for split in [\"train\", \"test\", \"val\"]:\n",
    "        ds = load_function(str(artifact_dir / split), compression=\"GZIP\")\n",
    "        output_list.append(ds)\n",
    "    \n",
    "    return output_list\n",
    "\n",
    "def get_readable_class_labels(subset = 'phcd_paper'):\n",
    "    if subset == 'phcd_paper':\n",
    "        return ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c',\n",
    "        'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n",
    "        'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C',\n",
    "        'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P',\n",
    "        'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'ą', 'ć', 'ę',\n",
    "        'ł', 'ń', 'ó', 'ś', 'ź', 'ż', 'Ą', 'Ć', 'Ę', 'Ł', 'Ń', 'Ó', 'Ś',\n",
    "        'Ź', 'Ż', '+', '-', ':', ';', '$', '!', '?', '@', '.']\n",
    "    elif subset == 'uppercase':\n",
    "        return ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'Ą', 'Ć', \n",
    "        'Ę', 'Ł', 'Ń', 'Ó', 'Ś', 'Ź', 'Ż']\n",
    "    elif subset == 'lowercase':\n",
    "        return ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "        'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ą', 'ć',\n",
    "        'ę', 'ł', 'ń', 'ó', 'ś', 'ź', 'ż']\n",
    "    elif subset == 'numbers':\n",
    "        return ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    elif subset == 'uppercase_no_diacritics':\n",
    "        return ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
    "        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "    elif subset == 'lowercase_no_diacritics':\n",
    "        return ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "        'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "def calculate_accuracy_per_class(model, test_dataset, test_dataset_name):\n",
    "    '''\n",
    "    Calculates the accuracy per class for a given model and test dataset.\n",
    "\n",
    "    Returns dict with class labels as keys and accuracy as values.\n",
    "    '''\n",
    "        \n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    # get labels\n",
    "    y_true = test_dataset.map(lambda x, y: y).as_numpy_iterator()\n",
    "    y_true = np.concatenate(list(y_true))\n",
    "    # calculate accuracy per class\n",
    "    labels = get_readable_class_labels(test_dataset_name)\n",
    "    class_accuracy = np.zeros(len(labels))\n",
    "    for i, label in enumerate(labels):\n",
    "        class_accuracy[i] = np.sum(y_pred[y_true == i] == i) / np.sum(y_true == i)\n",
    "    return { label: acc for label, acc in zip(labels, class_accuracy) }\n",
    "    \n",
    "\n",
    "def plot_accuracy_per_class(class_accuracy_dict):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    labels = list(class_accuracy_dict.keys())\n",
    "    class_accuracy = list(class_accuracy_dict.values())\n",
    "    plt.bar(labels, class_accuracy)\n",
    "    plt.xticks(labels)\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy per class\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def accuracy_table(class_accuracy_dict):\n",
    "    labels = list(class_accuracy_dict.keys())\n",
    "    class_accuracy = list(class_accuracy_dict.values())\n",
    "    return wandb.Table(columns=[\"Class\", \"Accuracy\"], data=list(zip(labels, class_accuracy)))\n",
    "\n",
    "def get_number_of_classes(ds: tf.data.Dataset) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of classes in a dataset.\n",
    "    \"\"\"\n",
    "    labels_iterator= ds.map(lambda x, y: y).as_numpy_iterator()\n",
    "    labels = np.concatenate(list(labels_iterator))\n",
    "    return len(np.unique(labels))\n",
    "\n",
    "def get_number_of_examples(ds: tf.data.Dataset) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of examples in a dataset.\n",
    "    \"\"\"\n",
    "    return sum(1 for _ in ds)\n",
    "\n",
    "def preprocess_dataset(ds: tf.data.Dataset, batch_size: int, cache: bool = True) -> tf.data.Dataset:\n",
    "    ds = ds.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y))  # normalize\n",
    "    ds = ds.unbatch().batch(batch_size)\n",
    "    if cache:\n",
    "        ds = ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def calculate_model_compressed_size_on_disk(path: str) -> int:\n",
    "    compressed_path = path + \".zip\"\n",
    "    with zipfile.ZipFile(compressed_path, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(path)\n",
    "    return pathlib.Path(compressed_path).stat().st_size    \n",
    "\n",
    "def calculate_model_size_on_disk(path: str) -> int:\n",
    "    return pathlib.Path(path).stat().st_size\n",
    "\n",
    "def calculate_model_num_parameters(model: tf.keras.Model) -> int:\n",
    "    return model.count_params()\n",
    "\n",
    "def calculate_model_flops(summary) -> float:\n",
    "    # from run.summary get GFLOPs or GFLOPS whichever is available\n",
    "    if \"GFLOPs\" in summary.keys():\n",
    "        return summary.get(\"GFLOPs\")\n",
    "    elif \"GFLOPS\" in summary.keys():\n",
    "        return summary.get(\"GFLOPS\")\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def plot_history(history, title):\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.suptitle(title)\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['accuracy'], label='train')\n",
    "    plt.plot(history.history['val_accuracy'], label='val')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='val')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "\n",
    "project_name = \"master-thesis\"\n",
    "run_name = \"architecture-2\"\n",
    "\n",
    "def get_runs(project_name, run_name):\n",
    "    \"\"\"\n",
    "    Returns all runs with a given name in a given project.\n",
    "    \"\"\"\n",
    "    api = wandb.Api()\n",
    "    runs = api.runs(\n",
    "        project_name, {\n",
    "            \"$and\": [\n",
    "                {\"displayName\": run_name},\n",
    "                {\"state\": \"finished\"},\n",
    "                {\"tags\": \"phcd_paper_splits_tfds\"}\n",
    "                ]\n",
    "            })\n",
    "    return [run.id for run in runs]\n",
    "\n",
    "def get_model_from_run(run_id, project_name=\"master-thesis\"):\n",
    "    # from project run_id artifacts get file_name\n",
    "    api = wandb.Api()\n",
    "    run = api.run(f\"{project_name}/{run_id}\")\n",
    "    # download file config.yaml\n",
    "    run.file(\"config.yaml\").download(replace=True)\n",
    "    artifact = run.file(\"model_baseline.h5\").download(replace=True)\n",
    "    model = tf.keras.models.load_model(artifact.name, compile=False)\n",
    "    return model, pathlib.Path(artifact.name)\n",
    "\n",
    "run_ids = get_runs(project_name, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = dict(\n",
    "    batch_size=32*2,\n",
    "    epochs=60,    \n",
    "    optimizer=\"adam\"\n",
    ")\n",
    "\n",
    "artifact_base_name = \"phcd_paper\"\n",
    "artifact_name = f\"{artifact_base_name}_splits_tfds\" # \"phcd_paper_splits_tfds\n",
    "arch_short_name = \"arch-2\"\n",
    "run = wandb.init(project=\"master-thesis\", job_type=\"model-optimization\",  config=defaults, tags=[\"optimization\"])\n",
    "\n",
    "# hyperparameters\n",
    "epochs = wandb.config.epochs\n",
    "bs = wandb.config.batch_size\n",
    "\n",
    "ds_train, ds_test, ds_val = load_data(run, artifact_name=artifact_name)\n",
    "\n",
    "num_classes = get_number_of_classes(ds_val)\n",
    "\n",
    "ds_train_non_normalized = ds_train.unbatch().batch(1).cache().prefetch(buffer_size=tf.data.AUTOTUNE).take(2000)\n",
    "ds_test_non_normalized = ds_test.unbatch().batch(1).cache().prefetch(buffer_size=tf.data.AUTOTUNE).take(1743)\n",
    "\n",
    "ds_train = preprocess_dataset(ds_train, batch_size=bs)\n",
    "ds_val = preprocess_dataset(ds_val, batch_size=bs)\n",
    "ds_test = preprocess_dataset(ds_test, batch_size=bs, cache=False)\n",
    "ds_test = ds_test.take(1743)\n",
    "#ds_test = preprocess_dataset(ds_test, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# before optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_as_tfrecords(ds: tf.data.Dataset, path):\n",
    "    def serialize_example(image, label):\n",
    "        image = tf.io.serialize_tensor(image)\n",
    "        label = tf.io.serialize_tensor(label)\n",
    "        feature = {\n",
    "            \"image\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[image.numpy()])),\n",
    "            \"label\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[label.numpy()])),\n",
    "        }\n",
    "        example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    writer = tf.data.experimental.TFRecordWriter(path)\n",
    "    writer.write(ds.map(serialize_example))\n",
    "\n",
    "def benchmark(bench_model, bench_model_path, bench_dataset):\n",
    "    test_loss, test_acc = bench_model.evaluate(bench_dataset)\n",
    "    num_parameters = calculate_model_num_parameters(bench_model)\n",
    "    compressed_disk_size = calculate_model_compressed_size_on_disk(bench_model_path)\n",
    "\n",
    "    print(f\"Test accuracy: {test_acc}\")\n",
    "    \n",
    "    print(f\"Number of parameters: {num_parameters}\")\n",
    "    print(f\"Compressed disk size (MB): {compressed_disk_size / 1e6}\")\n",
    "    return test_acc, num_parameters, compressed_disk_size\n",
    "\n",
    "def benchmark_tflite(bench_model, bench_model_path, bench_dataset):\n",
    "    test_loss, test_acc = bench_model.evaluate(bench_dataset)\n",
    "    num_parameters = calculate_model_num_parameters(bench_model)\n",
    "    compressed_disk_size = calculate_model_compressed_size_on_disk(bench_model_path)\n",
    "    \n",
    "    print(f\"Test loss: {test_loss}\")\n",
    "    print(f\"Test accuracy: {test_acc}\")\n",
    "    \n",
    "    print(f\"Number of parameters: {num_parameters}\")\n",
    "    print(f\"Compressed disk size (MB): {compressed_disk_size / 1e6}\")\n",
    "    return test_acc, num_parameters, compressed_disk_size\n",
    "\n",
    "def plot_results(acc, num_params, dsk_size):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    data = [acc, num_params, dsk_size]\n",
    "    # boxplots\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        # draw boxplot with seaborn\n",
    "        sns.boxplot(data[i], ax=ax)\n",
    "        # add title\n",
    "        ax.set_title(f\"{['Accuracy', 'Number of parameters', 'Compressed disk size'][i]}\")\n",
    "\n",
    "    # super title\n",
    "    fig.suptitle(\"Pruning results - accuracy, # of parameters, compressed disk size\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization - pruning & quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for evaluating tflite models\n",
    "def evaluate_model(interpreter, test_dataset):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a TensorFlow Lite model on a given test dataset.\n",
    "\n",
    "    Args:\n",
    "        interpreter (tf.lite.Interpreter): A TensorFlow Lite interpreter object with already allocated tensors.\n",
    "        test_dataset (tf.data.Dataset): A TensorFlow dataset object containing test images and labels.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (accuracy, loss) with the model accuracy and loss on the test dataset.\n",
    "    \"\"\"\n",
    "    # Prepare the input and output tensors\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    input_shape = input_details[0]['shape']\n",
    "    input_tensor_index = input_details[0]['index']\n",
    "    output_tensor_index = output_details[0]['index']\n",
    "\n",
    "    # Run the evaluation\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for images, labels in tqdm.tqdm(test_dataset, desc=\"Evaluating model\"):\n",
    "        # Set the input tensor shape to match the batch size\n",
    "        #input_shape[0] = images.shape[0]\n",
    "\n",
    "        # Resize the images to match the input tensor shape\n",
    "        images = tf.image.resize(images, input_shape[1:3])\n",
    "\n",
    "        # Set the input tensor value\n",
    "        interpreter.set_tensor(input_tensor_index, images)\n",
    "\n",
    "        # Run the inference\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Get the output tensor value\n",
    "        output = interpreter.get_tensor(output_tensor_index)\n",
    "\n",
    "        # Compute the batch loss and accuracy\n",
    "        batch_loss = tf.keras.losses.sparse_categorical_crossentropy(labels, output)\n",
    "        batch_accuracy = tf.keras.metrics.sparse_categorical_accuracy(labels, output)\n",
    "\n",
    "        # Update the total loss and accuracy\n",
    "        total_loss += tf.reduce_sum(batch_loss)\n",
    "        total_accuracy += tf.reduce_sum(batch_accuracy)\n",
    "        num_batches += 1\n",
    "\n",
    "    # Compute the average loss and accuracy\n",
    "    average_loss = total_loss / (num_batches * input_shape[0])\n",
    "    average_accuracy = total_accuracy / (num_batches * input_shape[0])\n",
    "\n",
    "    return average_accuracy.numpy() #, average_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "import tqdm\n",
    "\n",
    "results_columns = [\"run_id\", \"optimization_type\",  \"accuracy\", \"num_parameters\", \"compressed_disk_size\"]\n",
    "results = pd.DataFrame(columns=results_columns)\n",
    "do_pruning = False\n",
    "for run_id in tqdm.tqdm(run_ids, desc=\"Runs\"):\n",
    "  print(f\"Run id: {run_id}\")\n",
    "  model, model_path = get_model_from_run(run_id, project_name)\n",
    "  model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "  \n",
    "  base_acc, base_num_param, base_compressed_disk_size = benchmark(model, str(model_path), ds_test) \n",
    "  \n",
    "  base_results = pd.DataFrame(columns=results_columns, data=[[run_id, \"baseline\", base_acc, base_num_param, base_compressed_disk_size]])\n",
    "  results = pd.concat([results, base_results])\n",
    "\n",
    "  print(\"Base model accuracy: \", base_acc)\n",
    "\n",
    "\n",
    "  if do_pruning:\n",
    "    # pruning\n",
    "    pruning_epochs = 5\n",
    "    batch_size = 64\n",
    "    num_train_samples = sum(1 for _ in ds_train)\n",
    "    end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * pruning_epochs\n",
    "    print('End step: ' + str(end_step))\n",
    "\n",
    "    pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.05, final_sparsity=0.45, begin_step=0, end_step=end_step),\n",
    "    }\n",
    "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "    \n",
    "    model_sparse = prune_low_magnitude(model, **pruning_params)\n",
    "    model_sparse.compile(\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "      metrics=['accuracy'],\n",
    "    )\n",
    "    sparse_model_filename = f\"{run_id}_model_sparse.h5\"\n",
    "    print(\"Pruning model...\")\n",
    "    history = model_sparse.fit(\n",
    "        ds_train,\n",
    "        epochs=pruning_epochs,\n",
    "        validation_data=ds_val,\n",
    "        callbacks=[\n",
    "          tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "          # checkpoint\n",
    "          tf.keras.callbacks.ModelCheckpoint(\n",
    "              filepath=sparse_model_filename,\n",
    "              save_weights_only=False,\n",
    "              monitor=\"val_accuracy\",\n",
    "              mode=\"max\",\n",
    "              save_best_only=True,\n",
    "          ),\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    # plot history\n",
    "    plot_history(history, \"Pruning\")\n",
    "    plt.show()\n",
    "    #tf.keras.models.save_model(model_sparse, 'model_sparse.h5', include_optimizer=False)\n",
    "\n",
    "    test_acc, num_param, compressed_disk_size = benchmark(model_sparse, sparse_model_filename, ds_test)\n",
    "    print(\"Pruned model accuracy: \", test_acc)\n",
    "    sparse_results = pd.DataFrame(columns=results_columns, data=[[run_id, \"pruning\", test_acc, num_param, compressed_disk_size]])\n",
    "    results = pd.concat([results, sparse_results])\n",
    "    # send model to wandb\n",
    "    wandb.save(sparse_model_filename)\n",
    "\n",
    "    del model\n",
    "    del model_sparse\n",
    "\n",
    "  \n",
    "  # quantization\n",
    "  model, model_path = get_model_from_run(run_id, project_name)\n",
    "  model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "  quantization_epochs = 2\n",
    "\n",
    "  quantize_model = tfmot.quantization.keras.quantize_model\n",
    "  model_quant = quantize_model(model)\n",
    "  model_quant.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    metrics=['accuracy'],\n",
    "  )\n",
    "  quantization_aware_model_filename = f\"{run_id}_model_quant.h5\"\n",
    "  print(\"Fitting quantization aware model\")\n",
    "  history = model_quant.fit(\n",
    "      ds_train,\n",
    "      epochs=quantization_epochs,\n",
    "      validation_data=ds_val,\n",
    "  )\n",
    "\n",
    "  # plot history\n",
    "  plot_history(history, \"Quantization\")\n",
    "  tf.keras.models.save_model(model_quant, quantization_aware_model_filename, include_optimizer=False)\n",
    "\n",
    "  test_acc, num_param, compressed_disk_size = benchmark(model_quant, quantization_aware_model_filename, ds_test)\n",
    "  print(\"Quantization aware model accuracy: \", test_acc)\n",
    "\n",
    "  quant_aware_results = pd.DataFrame(columns=results_columns, data=[[run_id, \"quantization_aware\", test_acc, num_param, compressed_disk_size]])\n",
    "  results = pd.concat([results, quant_aware_results])\n",
    "\n",
    "  # ------------------ TF Lite ------------------------\n",
    "\n",
    "  tflite_models_dir = pathlib.Path(\"./models/\")\n",
    "  tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "  dynamic_range_model_file = tflite_models_dir / f\"{run_id}_{arch_short_name}_dynamic_range.tflite\"\n",
    "  fp16_model_file = tflite_models_dir / f\"{run_id}_{arch_short_name}_fp16.tflite\"\n",
    "  int8_model_file = tflite_models_dir / f\"{run_id}_{arch_short_name}_int8.tflite\"\n",
    "\n",
    "  def representative_dataset_gen():\n",
    "    for input_value, _ in ds_train_non_normalized:\n",
    "      yield [input_value]\n",
    "\n",
    "  # dynamic range\n",
    "  converter = tf.lite.TFLiteConverter.from_keras_model(model_quant)\n",
    "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "  model_quant_dynamic_range = converter.convert()\n",
    "  dynamic_range_model_file.write_bytes(model_quant_dynamic_range)\n",
    "\n",
    "  # quantization fp16\n",
    "  converter = tf.lite.TFLiteConverter.from_keras_model(model_quant)\n",
    "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "  converter.target_spec.supported_types = [tf.float16]\n",
    "  converter.representative_dataset = representative_dataset_gen\n",
    "  model_quant_fp16 = converter.convert()\n",
    "  fp16_model_file.write_bytes(model_quant_fp16)\n",
    "\n",
    "  # quantization int8\n",
    "  converter = tf.lite.TFLiteConverter.from_keras_model(model_quant)\n",
    "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "  converter.representative_dataset = representative_dataset_gen\n",
    "  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "  converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "  converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "  model_quant_int8 = converter.convert()\n",
    "  int8_model_file.write_bytes(model_quant_int8)\n",
    "\n",
    "  # load dynamic range model\n",
    "  interpreter = tf.lite.Interpreter(model_path=str(dynamic_range_model_file))\n",
    "  # set batch size to 64\n",
    "  interpreter.resize_tensor_input(0, [64, 32, 32, 1])\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  print(\"Evaluating dynamic range model\")\n",
    "  test_acc_dynamic_range = evaluate_model(interpreter, ds_test)\n",
    "  compressed_disk_size_dynamic_range = calculate_model_compressed_size_on_disk(str(dynamic_range_model_file))\n",
    "  \n",
    "  print(\"Dynamic range accuracy: \", test_acc_dynamic_range)\n",
    "\n",
    "  dynamic_range_results = pd.DataFrame(columns=results_columns, data=[[run_id, \"dynamic_range\", test_acc_dynamic_range, 0, compressed_disk_size_dynamic_range]])\n",
    "  results = pd.concat([results, dynamic_range_results])\n",
    "\n",
    "\n",
    "  # load fp16 model\n",
    "  interpreter = tf.lite.Interpreter(model_path=str(fp16_model_file))\n",
    "  interpreter.resize_tensor_input(0, [64, 32, 32, 1])\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  print(\"Evaluating fp16 model\")\n",
    "  test_acc_fp16 = evaluate_model(interpreter, ds_test)\n",
    "  \n",
    "  print(\"FP16 accuracy: \", test_acc_fp16)\n",
    "  compressed_disk_size_fp16 = calculate_model_compressed_size_on_disk(str(fp16_model_file))\n",
    "\n",
    "  fp16_results = pd.DataFrame(columns=results_columns, data=[[run_id, \"fp16\", test_acc_fp16, 0, compressed_disk_size_fp16]])\n",
    "  results = pd.concat([results, fp16_results])\n",
    "\n",
    "  # load int8 model\n",
    "  interpreter = tf.lite.Interpreter(model_path=str(int8_model_file))\n",
    "  interpreter.resize_tensor_input(0, [64, 32, 32, 1])\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  # evaluate interpreter on ds_test: tf.data.Dataset\n",
    "  print(\"Evaluating int8 model\")\n",
    "  test_acc_int8 = evaluate_model(interpreter, ds_test)\n",
    "  print(\"Int8 accuracy: \", test_acc_int8)\n",
    "  compressed_disk_size_int8 = calculate_model_compressed_size_on_disk(str(int8_model_file))\n",
    "\n",
    "  int8_results = pd.DataFrame(columns=results_columns, data=[[run_id, \"int8\", test_acc_int8, 0, compressed_disk_size_int8]])\n",
    "  results = pd.concat([results, int8_results])\n",
    "\n",
    "results_file = f\"optimalization_results_{arch_short_name}.csv\"\n",
    "results.to_csv(results_file)\n",
    "wandb.save(results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "sns.boxplot(x='optimization_type', y='accuracy', data=results, ax=ax)\n",
    "\n",
    "ax.set_title('Accuracies by optimalization type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='optimization_type', y='compressed_disk_size', data=results, ax=ax)\n",
    "\n",
    "ax.set_title('Compressed file size by optimalization type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
