{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils functions\n",
    "\n",
    "import pathlib\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import wandb\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def load_data(run, artifact_name = \"phcd_paper_splits_tfds\") -> List[tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Downloads datasets from a wandb artifact and loads them into a list of tf.data.Datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    artifact = run.use_artifact(f\"master-thesis/{artifact_name}:latest\")\n",
    "    artifact_dir = pathlib.Path(\n",
    "        f\"./artifacts/{artifact.name.replace(':', '-')}\"\n",
    "    ).resolve()\n",
    "    if not artifact_dir.exists():\n",
    "        artifact_dir = artifact.download()\n",
    "        artifact_dir = pathlib.Path(artifact_dir).resolve()\n",
    "\n",
    "    # if tf.__version__ minor is less than 10, use\n",
    "    # tf.data.experimental.load instead of tf.data.Dataset.load\n",
    "\n",
    "    if int(tf.__version__.split(\".\")[1]) < 10:\n",
    "        load_function = tf.data.experimental.load\n",
    "    else:\n",
    "        load_function = tf.data.Dataset.load\n",
    "    \n",
    "    output_list = []\n",
    "    for split in [\"train\", \"test\", \"val\"]:\n",
    "        ds = load_function(str(artifact_dir / split), compression=\"GZIP\")\n",
    "        output_list.append(ds)\n",
    "    \n",
    "    return output_list\n",
    "\n",
    "def get_readable_class_labels(subset = 'phcd_paper'):\n",
    "    if subset == 'phcd_paper':\n",
    "        return ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c',\n",
    "        'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n",
    "        'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C',\n",
    "        'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P',\n",
    "        'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'ą', 'ć', 'ę',\n",
    "        'ł', 'ń', 'ó', 'ś', 'ź', 'ż', 'Ą', 'Ć', 'Ę', 'Ł', 'Ń', 'Ó', 'Ś',\n",
    "        'Ź', 'Ż', '+', '-', ':', ';', '$', '!', '?', '@', '.']\n",
    "    elif subset == 'uppercase':\n",
    "        return ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'Ą', 'Ć', \n",
    "        'Ę', 'Ł', 'Ń', 'Ó', 'Ś', 'Ź', 'Ż']\n",
    "    elif subset == 'lowercase':\n",
    "        return ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "        'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ą', 'ć',\n",
    "        'ę', 'ł', 'ń', 'ó', 'ś', 'ź', 'ż']\n",
    "    elif subset == 'numbers':\n",
    "        return ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    elif subset == 'uppercase_no_diacritics':\n",
    "        return ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
    "        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "    elif subset == 'lowercase_no_diacritics':\n",
    "        return ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "        'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "def calculate_accuracy_per_class(model, test_dataset, test_dataset_name):\n",
    "    '''\n",
    "    Calculates the accuracy per class for a given model and test dataset.\n",
    "\n",
    "    Returns dict with class labels as keys and accuracy as values.\n",
    "    '''\n",
    "        \n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    # get labels\n",
    "    y_true = test_dataset.map(lambda x, y: y).as_numpy_iterator()\n",
    "    y_true = np.concatenate(list(y_true))\n",
    "    # calculate accuracy per class\n",
    "    labels = get_readable_class_labels(test_dataset_name)\n",
    "    class_accuracy = np.zeros(len(labels))\n",
    "    for i, label in enumerate(labels):\n",
    "        class_accuracy[i] = np.sum(y_pred[y_true == i] == i) / np.sum(y_true == i)\n",
    "    return { label: acc for label, acc in zip(labels, class_accuracy) }\n",
    "    \n",
    "\n",
    "def plot_accuracy_per_class(class_accuracy_dict):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    labels = list(class_accuracy_dict.keys())\n",
    "    class_accuracy = list(class_accuracy_dict.values())\n",
    "    plt.bar(labels, class_accuracy)\n",
    "    plt.xticks(labels)\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy per class\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def accuracy_table(class_accuracy_dict):\n",
    "    labels = list(class_accuracy_dict.keys())\n",
    "    class_accuracy = list(class_accuracy_dict.values())\n",
    "    return wandb.Table(columns=[\"Class\", \"Accuracy\"], data=list(zip(labels, class_accuracy)))\n",
    "\n",
    "def get_number_of_classes(ds: tf.data.Dataset) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of classes in a dataset.\n",
    "    \"\"\"\n",
    "    labels_iterator= ds.map(lambda x, y: y).as_numpy_iterator()\n",
    "    labels = np.concatenate(list(labels_iterator))\n",
    "    return len(np.unique(labels))\n",
    "\n",
    "def get_number_of_examples(ds: tf.data.Dataset) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of examples in a dataset.\n",
    "    \"\"\"\n",
    "    return sum(1 for _ in ds)\n",
    "\n",
    "def preprocess_dataset(ds: tf.data.Dataset, batch_size: int, cache: bool = True) -> tf.data.Dataset:\n",
    "    ds = ds.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y))  # normalize\n",
    "    ds = ds.unbatch().batch(batch_size)\n",
    "    if cache:\n",
    "        ds = ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def calculate_model_compressed_size_on_disk(path: str) -> int:\n",
    "    compressed_path = path + \".zip\"\n",
    "    with zipfile.ZipFile(compressed_path, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(path)\n",
    "    return pathlib.Path(compressed_path).stat().st_size    \n",
    "\n",
    "def calculate_model_size_on_disk(path: str) -> int:\n",
    "    return pathlib.Path(path).stat().st_size\n",
    "\n",
    "def calculate_model_num_parameters(model: tf.keras.Model) -> int:\n",
    "    return model.count_params()\n",
    "\n",
    "def calculate_model_flops(summary) -> float:\n",
    "    # from run.summary get GFLOPs or GFLOPS whichever is available\n",
    "    if \"GFLOPs\" in summary.keys():\n",
    "        return summary.get(\"GFLOPs\")\n",
    "    elif \"GFLOPS\" in summary.keys():\n",
    "        return summary.get(\"GFLOPS\")\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "\n",
    "project_name = \"master-thesis\"\n",
    "run_name = \"architecture-2\"\n",
    "\n",
    "def get_runs(project_name, run_name):\n",
    "    \"\"\"\n",
    "    Returns all runs with a given name in a given project.\n",
    "    \"\"\"\n",
    "    api = wandb.Api()\n",
    "    runs = api.runs(\n",
    "        project_name, {\n",
    "            \"$and\": [\n",
    "                {\"displayName\": run_name},\n",
    "                {\"state\": \"finished\"},\n",
    "                {\"tags\": \"phcd_paper_splits_tfds\"}\n",
    "                ]\n",
    "            })\n",
    "    return [run.id for run in runs]\n",
    "\n",
    "def get_model_from_run(run_id, project_name=\"master-thesis\"):\n",
    "    # from project run_id artifacts get file_name\n",
    "    api = wandb.Api()\n",
    "    run = api.run(f\"{project_name}/{run_id}\")\n",
    "    # download file config.yaml\n",
    "    run.file(\"config.yaml\").download(replace=True)\n",
    "    artifact = run.file(\"model_baseline.h5\").download(replace=True)\n",
    "    model = tf.keras.models.load_model(artifact.name, compile=False)\n",
    "    return model, pathlib.Path(artifact.name)\n",
    "\n",
    "run_ids = get_runs(project_name, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = dict(\n",
    "    batch_size=32*2,\n",
    "    epochs=60,    \n",
    "    optimizer=\"adam\"\n",
    ")\n",
    "\n",
    "artifact_base_name = \"phcd_paper\"\n",
    "artifact_name = f\"{artifact_base_name}_splits_tfds\" # \"phcd_paper_splits_tfds\n",
    "run = wandb.init(project=\"master-thesis\", job_type=\"model-optimization\",  config=defaults, tags=[\"optimization\"])\n",
    "\n",
    "# hyperparameters\n",
    "epochs = wandb.config.epochs\n",
    "bs = wandb.config.batch_size\n",
    "\n",
    "ds_train, ds_test, ds_val = load_data(run, artifact_name=artifact_name)\n",
    "\n",
    "num_classes = get_number_of_classes(ds_val)\n",
    "\n",
    "ds_train = preprocess_dataset(ds_train, batch_size=bs)\n",
    "ds_val = preprocess_dataset(ds_val, batch_size=bs)\n",
    "ds_test = preprocess_dataset(ds_test, batch_size=bs, cache=False)\n",
    "ds_test = ds_test.take(1743)\n",
    "#ds_test = preprocess_dataset(ds_test, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# before optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_as_tfrecords(ds: tf.data.Dataset, path):\n",
    "    def serialize_example(image, label):\n",
    "        image = tf.io.serialize_tensor(image)\n",
    "        label = tf.io.serialize_tensor(label)\n",
    "        feature = {\n",
    "            \"image\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[image.numpy()])),\n",
    "            \"label\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[label.numpy()])),\n",
    "        }\n",
    "        example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    writer = tf.data.experimental.TFRecordWriter(path)\n",
    "    writer.write(ds.map(serialize_example))\n",
    "\n",
    "def benchmark(bench_model, bench_model_path, bench_dataset):\n",
    "    test_loss, test_acc = bench_model.evaluate(bench_dataset)\n",
    "    num_parameters = calculate_model_num_parameters(bench_model)\n",
    "    compressed_disk_size = calculate_model_compressed_size_on_disk(bench_model_path)\n",
    "\n",
    "    print(f\"Test accuracy: {test_acc}\")\n",
    "    \n",
    "    print(f\"Number of parameters: {num_parameters}\")\n",
    "    print(f\"Compressed disk size (MB): {compressed_disk_size / 1e6}\")\n",
    "    return test_acc, num_parameters, compressed_disk_size\n",
    "\n",
    "def benchmark_tflite(bench_model, bench_model_path, bench_dataset):\n",
    "    test_loss, test_acc = bench_model.evaluate(bench_dataset)\n",
    "    num_parameters = calculate_model_num_parameters(bench_model)\n",
    "    compressed_disk_size = calculate_model_compressed_size_on_disk(bench_model_path)\n",
    "    \n",
    "    print(f\"Test loss: {test_loss}\")\n",
    "    print(f\"Test accuracy: {test_acc}\")\n",
    "    \n",
    "    print(f\"Number of parameters: {num_parameters}\")\n",
    "    print(f\"Compressed disk size (MB): {compressed_disk_size / 1e6}\")\n",
    "    return test_acc, num_parameters, compressed_disk_size\n",
    "\n",
    "def plot_results(acc, num_params, dsk_size):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    data = [acc, num_params, dsk_size]\n",
    "    # boxplots\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        # draw boxplot with seaborn\n",
    "        sns.boxplot(data[i], ax=ax)\n",
    "        # add title\n",
    "        ax.set_title(f\"{['Accuracy', 'Number of parameters', 'Compressed disk size'][i]}\")\n",
    "\n",
    "    # super title\n",
    "    fig.suptitle(\"Pruning results - accuracy, # of parameters, compressed disk size\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "import tqdm\n",
    "base_accuracies = []\n",
    "base_num_parameters = []\n",
    "base_compressed_disk_sizes = []\n",
    "\n",
    "accuracies = []\n",
    "num_parameters = []\n",
    "compressed_disk_sizes = []\n",
    "\n",
    "for run_id in tqdm.tqdm(run_ids, desc=\"Runs\"):\n",
    "  print(f\"Run id: {run_id}\")\n",
    "  model, model_path = get_model_from_run(run_id, project_name)\n",
    "\n",
    "  prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "  quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "\n",
    "  prune_epochs = 2\n",
    "  pruning_params = {\n",
    "        #'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=0, frequency=100),\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.40, final_sparsity=0.80, begin_step=0, end_step=prune_epochs)\n",
    "    }\n",
    "\n",
    "  callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep()\n",
    "  ]\n",
    "\n",
    "\n",
    "  model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "  \n",
    "  base_acc, base_num_param, base_compressed_disk_size = benchmark(model, str(model_path), ds_test) \n",
    "  base_accuracies.append(base_acc)\n",
    "  base_num_parameters.append(base_num_param)\n",
    "  base_compressed_disk_sizes.append(base_compressed_disk_size)\n",
    "  print(\"Base model accuracy: \", base_acc)\n",
    "\n",
    "  model_sparse = prune_low_magnitude(model, **pruning_params)\n",
    "  model_sparse.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "  model_sparse.fit(\n",
    "      ds_train,\n",
    "      epochs=prune_epochs,\n",
    "      validation_data=ds_val,\n",
    "      callbacks=[\n",
    "        tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  tf.keras.models.save_model(model_sparse, 'model_sparse.h5', include_optimizer=False)\n",
    "\n",
    "  test_acc, num_param, compressed_disk_size = benchmark(model_sparse, 'model_sparse.h5', ds_test)\n",
    "\n",
    "  accuracies.append(test_acc)\n",
    "  num_parameters.append(num_param)\n",
    "  compressed_disk_sizes.append(compressed_disk_size)\n",
    "  del model\n",
    "  del model_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "sns.boxplot([base_accuracies, accuracies], ax=ax)\n",
    "\n",
    "ax.set_title('Accuracies before and after pruning')\n",
    "ax.set_xticklabels(['Before', 'After'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "sns.boxplot([base_num_parameters, num_parameters], ax=ax)\n",
    "\n",
    "ax.set_title('Number of parameters before and after pruning')\n",
    "ax.set_xticklabels(['Before', 'After'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "sns.boxplot([base_compressed_disk_sizes, compressed_disk_sizes], ax=ax)\n",
    "\n",
    "ax.set_title('Compressed disk size before and after pruning')\n",
    "ax.set_xticklabels(['Before', 'After'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    \"base_accuracy\": base_accuracies,\n",
    "    \"accuracy\": accuracies,\n",
    "    \"num_parameters\": num_parameters,\n",
    "    \"compressed_disk_size\": compressed_disk_sizes\n",
    "})\n",
    "\n",
    "results_file = \"pruning_results_arch_1.csv\"\n",
    "\n",
    "results.to_csv(results_file)\n",
    "wandb.save(results_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(interpreter, test_dataset):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a TensorFlow Lite model on a given test dataset.\n",
    "\n",
    "    Args:\n",
    "        interpreter (tf.lite.Interpreter): A TensorFlow Lite interpreter object with already allocated tensors.\n",
    "        test_dataset (tf.data.Dataset): A TensorFlow dataset object containing test images and labels.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (accuracy, loss) with the model accuracy and loss on the test dataset.\n",
    "    \"\"\"\n",
    "    # Prepare the input and output tensors\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    input_shape = input_details[0]['shape']\n",
    "    input_tensor_index = input_details[0]['index']\n",
    "    output_tensor_index = output_details[0]['index']\n",
    "\n",
    "    # Run the evaluation\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for images, labels in tqdm.tqdm(test_dataset, desc=\"Evaluating model\"):\n",
    "        # Set the input tensor shape to match the batch size\n",
    "        #input_shape[0] = images.shape[0]\n",
    "\n",
    "        # Resize the images to match the input tensor shape\n",
    "        images = tf.image.resize(images, input_shape[1:3])\n",
    "\n",
    "        # Set the input tensor value\n",
    "        interpreter.set_tensor(input_tensor_index, images)\n",
    "\n",
    "        # Run the inference\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Get the output tensor value\n",
    "        output = interpreter.get_tensor(output_tensor_index)\n",
    "\n",
    "        # Compute the batch loss and accuracy\n",
    "        batch_loss = tf.keras.losses.sparse_categorical_crossentropy(labels, output)\n",
    "        batch_accuracy = tf.keras.metrics.sparse_categorical_accuracy(labels, output)\n",
    "\n",
    "        # Update the total loss and accuracy\n",
    "        total_loss += tf.reduce_sum(batch_loss)\n",
    "        total_accuracy += tf.reduce_sum(batch_accuracy)\n",
    "        num_batches += 1\n",
    "\n",
    "    # Compute the average loss and accuracy\n",
    "    average_loss = total_loss / (num_batches * input_shape[0])\n",
    "    average_accuracy = total_accuracy / (num_batches * input_shape[0])\n",
    "\n",
    "    return average_accuracy.numpy() #, average_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "import tqdm\n",
    "base_accuracies = []\n",
    "base_num_parameters = []\n",
    "base_compressed_disk_sizes = []\n",
    "\n",
    "accuracies_fp16 = []\n",
    "accuracies_int8 = []\n",
    "compressed_disk_sizes_fp16 = []\n",
    "compressed_disk_sizes_int8 = []\n",
    "\n",
    "for i, run_id in enumerate(run_ids):\n",
    "  print(f\"Running run_id {run_id}, {i+1} out of {len(run_ids)}\")\n",
    "  model, model_path = get_model_from_run(run_id, project_name)\n",
    "\n",
    "  model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "  \n",
    "  base_acc, base_num_param, base_compressed_disk_size = benchmark(model, str(model_path), ds_test) \n",
    "  base_accuracies.append(base_acc)\n",
    "  base_num_parameters.append(base_num_param)\n",
    "  base_compressed_disk_sizes.append(base_compressed_disk_size)\n",
    "  print(f\"Base accuracy: \", base_acc)\n",
    "\n",
    "  # quantization fp16\n",
    "  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "  tflite_model = converter.convert()\n",
    "  \n",
    "  tflite_models_dir = pathlib.Path(\"./models/\")\n",
    "  tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "  fp16_model_file = tflite_models_dir/\"arch-2_fp16.tflite\"\n",
    "  int8_model_file = tflite_models_dir/\"arch-2_int8.tflite\"\n",
    "\n",
    "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "  converter.target_spec.supported_types = [tf.float16]\n",
    "  fp16_model_file.write_bytes(tflite_model)\n",
    "\n",
    "  # quantization int8\n",
    "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "  converter.target_spec.supported_types = [tf.int8]\n",
    "  int8_model_file.write_bytes(tflite_model)\n",
    "\n",
    "  # load fp16 model\n",
    "  interpreter = tf.lite.Interpreter(model_path=str(fp16_model_file))\n",
    "  # set batch size to 64\n",
    "  interpreter.resize_tensor_input(0, [64, 32, 32, 1])\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  # evaluate interpreter on ds_test: tf.data.\n",
    "  print(\"Evaluating fp16 model\")\n",
    "  test_acc_fp16 = evaluate_model(interpreter, ds_test)\n",
    "  accuracies_fp16.append(test_acc_fp16)\n",
    "  print(\"FP16 accuracy: \", test_acc_fp16)\n",
    "  compressed_disk_sizes_fp16.append(calculate_model_compressed_size_on_disk(str(fp16_model_file)))\n",
    "\n",
    "  # load int8 model\n",
    "  interpreter = tf.lite.Interpreter(model_path=str(int8_model_file))\n",
    "  interpreter.resize_tensor_input(0, [64, 32, 32, 1])\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  # evaluate interpreter on ds_test: tf.data.Dataset\n",
    "  print(\"Evaluating int8 model\")\n",
    "  test_acc_int8 = evaluate_model(interpreter, ds_test)\n",
    "  accuracies_int8.append(test_acc_int8)\n",
    "  print(\"Int8 accuracy: \", test_acc_int8)\n",
    "  compressed_disk_sizes_int8.append(calculate_model_compressed_size_on_disk(str(int8_model_file)))\n",
    "\n",
    "  del model\n",
    "  del interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    \"base_accuracy\": base_accuracies,\n",
    "    \"accuracy_fp16\": accuracies_fp16,\n",
    "    \"accuracy_int8\": accuracies_int8,\n",
    "    \"compressed_disk_size_fp16\": compressed_disk_sizes_fp16,\n",
    "    \"compressed_disk_size_int8\": compressed_disk_sizes_int8\n",
    "})\n",
    "\n",
    "results_file = \"quantization_results_arch_1.csv\"\n",
    "\n",
    "results.to_csv(results_file)\n",
    "wandb.save(results_file)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
