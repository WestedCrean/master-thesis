{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils functions\n",
    "\n",
    "import pathlib\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import wandb\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def load_data(run, artifact_name = \"phcd_paper_splits_tfds\") -> List[tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Downloads datasets from a wandb artifact and loads them into a list of tf.data.Datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    artifact = run.use_artifact(f\"master-thesis/{artifact_name}:latest\")\n",
    "    artifact_dir = pathlib.Path(\n",
    "        f\"./artifacts/{artifact.name.replace(':', '-')}\"\n",
    "    ).resolve()\n",
    "    if not artifact_dir.exists():\n",
    "        artifact_dir = artifact.download()\n",
    "        artifact_dir = pathlib.Path(artifact_dir).resolve()\n",
    "\n",
    "    # if tf.__version__ minor is less than 10, use\n",
    "    # tf.data.experimental.load instead of tf.data.Dataset.load\n",
    "\n",
    "    if int(tf.__version__.split(\".\")[1]) < 10:\n",
    "        load_function = tf.data.experimental.load\n",
    "    else:\n",
    "        load_function = tf.data.Dataset.load\n",
    "    \n",
    "    output_list = []\n",
    "    for split in [\"train\", \"test\", \"val\"]:\n",
    "        ds = load_function(str(artifact_dir / split), compression=\"GZIP\")\n",
    "        output_list.append(ds)\n",
    "    \n",
    "    return output_list\n",
    "\n",
    "def get_readable_class_labels(subset = 'phcd_paper'):\n",
    "    if subset == 'phcd_paper':\n",
    "        return ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c',\n",
    "        'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n",
    "        'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C',\n",
    "        'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P',\n",
    "        'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'ą', 'ć', 'ę',\n",
    "        'ł', 'ń', 'ó', 'ś', 'ź', 'ż', 'Ą', 'Ć', 'Ę', 'Ł', 'Ń', 'Ó', 'Ś',\n",
    "        'Ź', 'Ż', '+', '-', ':', ';', '$', '!', '?', '@', '.']\n",
    "    elif subset == 'uppercase':\n",
    "        return ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'Ą', 'Ć', \n",
    "        'Ę', 'Ł', 'Ń', 'Ó', 'Ś', 'Ź', 'Ż']\n",
    "    elif subset == 'lowercase':\n",
    "        return ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "        'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ą', 'ć',\n",
    "        'ę', 'ł', 'ń', 'ó', 'ś', 'ź', 'ż']\n",
    "    elif subset == 'numbers':\n",
    "        return ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    elif subset == 'uppercase_no_diacritics':\n",
    "        return ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
    "        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "    elif subset == 'lowercase_no_diacritics':\n",
    "        return ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "        'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "def calculate_accuracy_per_class(model, test_dataset, test_dataset_name):\n",
    "    '''\n",
    "    Calculates the accuracy per class for a given model and test dataset.\n",
    "\n",
    "    Returns dict with class labels as keys and accuracy as values.\n",
    "    '''\n",
    "        \n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    # get labels\n",
    "    y_true = test_dataset.map(lambda x, y: y).as_numpy_iterator()\n",
    "    y_true = np.concatenate(list(y_true))\n",
    "    # calculate accuracy per class\n",
    "    labels = get_readable_class_labels(test_dataset_name)\n",
    "    class_accuracy = np.zeros(len(labels))\n",
    "    for i, label in enumerate(labels):\n",
    "        class_accuracy[i] = np.sum(y_pred[y_true == i] == i) / np.sum(y_true == i)\n",
    "    return { label: acc for label, acc in zip(labels, class_accuracy) }\n",
    "    \n",
    "\n",
    "def plot_accuracy_per_class(class_accuracy_dict):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    labels = list(class_accuracy_dict.keys())\n",
    "    class_accuracy = list(class_accuracy_dict.values())\n",
    "    plt.bar(labels, class_accuracy)\n",
    "    plt.xticks(labels)\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy per class\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def accuracy_table(class_accuracy_dict):\n",
    "    labels = list(class_accuracy_dict.keys())\n",
    "    class_accuracy = list(class_accuracy_dict.values())\n",
    "    return wandb.Table(columns=[\"Class\", \"Accuracy\"], data=list(zip(labels, class_accuracy)))\n",
    "\n",
    "def get_number_of_classes(ds: tf.data.Dataset) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of classes in a dataset.\n",
    "    \"\"\"\n",
    "    labels_iterator= ds.map(lambda x, y: y).as_numpy_iterator()\n",
    "    labels = np.concatenate(list(labels_iterator))\n",
    "    return len(np.unique(labels))\n",
    "\n",
    "def get_number_of_examples(ds: tf.data.Dataset) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of examples in a dataset.\n",
    "    \"\"\"\n",
    "    return sum(1 for _ in ds)\n",
    "\n",
    "def preprocess_dataset(ds: tf.data.Dataset, batch_size: int, cache: bool = True) -> tf.data.Dataset:\n",
    "    ds = ds.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y))  # normalize\n",
    "    ds = ds.unbatch().batch(batch_size)\n",
    "    if cache:\n",
    "        ds = ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def calculate_model_compressed_size_on_disk(path: str) -> int:\n",
    "    compressed_path = path + \".zip\"\n",
    "    with zipfile.ZipFile(compressed_path, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(path)\n",
    "    return pathlib.Path(compressed_path).stat().st_size    \n",
    "\n",
    "def calculate_model_size_on_disk(path: str) -> int:\n",
    "    return pathlib.Path(path).stat().st_size\n",
    "\n",
    "def calculate_model_num_parameters(model: tf.keras.Model) -> int:\n",
    "    return model.count_params()\n",
    "\n",
    "def calculate_model_flops(summary) -> float:\n",
    "    # from run.summary get GFLOPs or GFLOPS whichever is available\n",
    "    if \"GFLOPs\" in summary.keys():\n",
    "        return summary.get(\"GFLOPs\")\n",
    "    elif \"GFLOPS\" in summary.keys():\n",
    "        return summary.get(\"GFLOPS\")\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 15:48:08.341866: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-03-16 15:48:08.341887: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (wiktor-on-linux): /proc/driver/nvidia/version does not exist\n",
      "2023-03-16 15:48:08.342550: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "\n",
    "project_name = \"master-thesis\"\n",
    "run_id = \"e5ohdjxh\"\n",
    "\n",
    "def get_model_from_run(run_id, project_name=\"master-thesis\"):\n",
    "    # from project run_id artifacts get file_name\n",
    "    api = wandb.Api()\n",
    "    run = api.run(f\"{project_name}/{run_id}\")\n",
    "    artifact = run.file(\"model_baseline.h5\").download(replace=True)\n",
    "    model = tf.keras.models.load_model(artifact.name)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model, pathlib.Path(artifact.name)\n",
    "\n",
    "model, model_path = get_model_from_run(run_id, project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgratkadlafana\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wiktor/code/master-thesis/notebooks/results_analysis/wandb/run-20230316_154810-1wa5bz5b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/gratkadlafana/master-thesis/runs/1wa5bz5b\" target=\"_blank\">devoted-plant-424</a></strong> to <a href=\"https://wandb.ai/gratkadlafana/master-thesis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   9 of 9 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "defaults = dict(\n",
    "    batch_size=32*2,\n",
    "    epochs=60,    \n",
    "    optimizer=\"adam\"\n",
    ")\n",
    "\n",
    "artifact_base_name = \"phcd_paper\"\n",
    "artifact_name = f\"{artifact_base_name}_splits_tfds\" # \"phcd_paper_splits_tfds\n",
    "run = wandb.init(project=\"master-thesis\", job_type=\"optimization\",  config=defaults, tags=[\"optimization\"])\n",
    "\n",
    "# hyperparameters\n",
    "epochs = wandb.config.epochs\n",
    "bs = wandb.config.batch_size\n",
    "\n",
    "ds_train, ds_test, ds_val = load_data(run, artifact_name=artifact_name)\n",
    "\n",
    "num_classes = get_number_of_classes(ds_val)\n",
    "\n",
    "ds_train = preprocess_dataset(ds_train, batch_size=bs)\n",
    "ds_val = preprocess_dataset(ds_val, batch_size=bs)\n",
    "ds_test = preprocess_dataset(ds_test, batch_size=bs, cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# before optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark before optimization:\n"
     ]
    }
   ],
   "source": [
    "def benchmark(bench_model, bench_model_path, benech_dataset):\n",
    "    test_loss, test_acc = bench_model.evaluate(ds_test)\n",
    "    num_parameters = calculate_model_num_parameters(bench_model)\n",
    "    compressed_disk_size = calculate_model_compressed_size_on_disk(bench_model_path)\n",
    "\n",
    "    print(f\"Test accuracy: {test_acc}\")\n",
    "    \n",
    "    print(f\"Number of parameters: {num_parameters}\")\n",
    "    print(f\"Compressed disk size (MB): {compressed_disk_size / 1e6}\")\n",
    "    return test_acc, num_parameters, compressed_disk_size\n",
    "\n",
    "print(f\"Benchmark before optimization:\")\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(acc, num_params, dsk_size):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    data = [acc, num_params, dsk_size]\n",
    "    # boxplots\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        # draw boxplot with seaborn\n",
    "        sns.boxplot(data[i], ax=ax)\n",
    "        # add title\n",
    "        ax.set_title(f\"{['Accuracy', 'Number of parameters', 'Compressed disk size'][i]}\")\n",
    "\n",
    "    # super title\n",
    "    fig.suptitle(\"Pruning results - accuracy, # of parameters, compressed disk size\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wiktor/.pyenv/versions/3.10.9/envs/master-thesis/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6105/6105 [==============================] - 434s 71ms/step - loss: 0.8951 - accuracy: 0.6682 - val_loss: 0.5233 - val_accuracy: 0.7839\n",
      "Epoch 2/2\n",
      "6105/6105 [==============================] - 439s 72ms/step - loss: 0.4625 - accuracy: 0.8107 - val_loss: 0.4018 - val_accuracy: 0.8371\n",
      "Base:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39msave_model(model_sparse, \u001b[39m'\u001b[39m\u001b[39mmodel_sparse.h5\u001b[39m\u001b[39m'\u001b[39m, include_optimizer\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     40\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBase:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m benchmark(model_original, \u001b[39mstr\u001b[39;49m(model_path), ds_test)\n\u001b[1;32m     42\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOptimized:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m test_acc, num_param, compressed_disk_size \u001b[39m=\u001b[39m benchmark(model_sparse, \u001b[39m'\u001b[39m\u001b[39mmodel_sparse.h5\u001b[39m\u001b[39m'\u001b[39m, ds_test)\n",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m, in \u001b[0;36mbenchmark\u001b[0;34m(bench_model, bench_model_path, benech_dataset)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbenchmark\u001b[39m(bench_model, bench_model_path, benech_dataset):\n\u001b[0;32m----> 2\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m bench_model\u001b[39m.\u001b[39;49mevaluate(ds_test)\n\u001b[1;32m      3\u001b[0m     num_parameters \u001b[39m=\u001b[39m calculate_model_num_parameters(bench_model)\n\u001b[1;32m      4\u001b[0m     compressed_disk_size \u001b[39m=\u001b[39m calculate_model_compressed_size_on_disk(bench_model_path)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/envs/master-thesis/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/envs/master-thesis/lib/python3.10/site-packages/keras/engine/training.py:3059\u001b[0m, in \u001b[0;36mModel._assert_compile_was_called\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3053\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assert_compile_was_called\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   3054\u001b[0m   \u001b[39m# Checks whether `compile` has been called. If it has been called,\u001b[39;00m\n\u001b[1;32m   3055\u001b[0m   \u001b[39m# then the optimizer is set. This is different from whether the\u001b[39;00m\n\u001b[1;32m   3056\u001b[0m   \u001b[39m# model is compiled\u001b[39;00m\n\u001b[1;32m   3057\u001b[0m   \u001b[39m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[39;00m\n\u001b[1;32m   3058\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_compiled:\n\u001b[0;32m-> 3059\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mYou must compile your model before \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   3060\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mtraining/testing. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   3061\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mUse `model.compile(optimizer, loss)`.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "prune_epochs = 2\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=0, frequency=100),\n",
    "      # 'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50, final_sparsity=0.90, begin_step=0, end_step=prune_epochs)\n",
    "  }\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep()\n",
    "]\n",
    "\n",
    "accuracies = []\n",
    "num_parameters = []\n",
    "compressed_disk_sizes = []\n",
    "\n",
    "n_tries = 12\n",
    "for i in range(n_tries):\n",
    "  model, model_path = get_model_from_run(run_id, project_name)\n",
    "  # copy model\n",
    "  model_original = tf.keras.models.clone_model(model)\n",
    "  model_sparse = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "  model_original.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "  model_sparse.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "  history = model_sparse.fit(\n",
    "      ds_train,\n",
    "      epochs=prune_epochs,\n",
    "      validation_data=ds_val,\n",
    "      callbacks=[\n",
    "        tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  tf.keras.models.save_model(model_sparse, 'model_sparse.h5', include_optimizer=False)\n",
    "  print(\"Base:\")\n",
    "  \n",
    "  benchmark(model_original, str(model_path), ds_test)\n",
    "  print(\"Optimized:\")\n",
    "  test_acc, num_param, compressed_disk_size = benchmark(model_sparse, 'model_sparse.h5', ds_test)\n",
    "  accuracies.append(test_acc)\n",
    "  num_parameters.append(num_param)\n",
    "  compressed_disk_sizes.append(compressed_disk_size)\n",
    "  plot_results(accuracies, num_parameters, compressed_disk_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    \"accuracy\": accuracies,\n",
    "    \"num_parameters\": num_parameters,\n",
    "    \"compressed_disk_size\": compressed_disk_sizes\n",
    "})\n",
    "\n",
    "results.to_csv(\"pruning_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
