{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRicHmH8vnLY"
   },
   "source": [
    "# Baseline network\n",
    "\n",
    "This notebook trains the baseline network with exact same architecture as the one in paper [Recognition of handwritten Latin characters with diacritics using CNN](https://journals.pan.pl/dlibra/publication/136210/edition/119099/content/bulletin-of-the-polish-academy-of-sciences-technical-sciences-recognition-of-handwritten-latin-characters-with-diacritics-using-cnn-lukasik-edyta-charytanowicz-malgorzata-milosz-marek-tokovarov-michail-kaczorowska-monika-czerwinski-dariusz-zientarski-tomasz-2021-69-no-1?language=en)\n",
    "\n",
    "Model architecture description:\n",
    "\n",
    "\"The architecture of the concrete CNN is shown in Fig. 2.\n",
    "The input is a 32x32 binarized matrix. The input is then prop-\n",
    "agated through 12 adaptable layers. First come two convolu-\n",
    "tional layers having 32 filters with the size of 3x3 and stride 1.\n",
    "Secondly, the output of the convolutional layer is fed to the\n",
    "ReLU function. The output is down-sampled using a max-pool-\n",
    "ing operation with a 2x2 stride. Next, the dropout technique is\n",
    "used with the coefficient 0.25. The four operations (two con-\n",
    "volutions, nonlinearity, max-pooling, and dropout) are repeated,\n",
    "using 64 filters for the convolutional layers. The output of the\n",
    "last layer is then flattened and fed through a fully connected\n",
    "layer with 256 neurons and ReLU nonlinearities, dropped out\n",
    "with the 0.25 coefficient, and a final output layer is fully con-\n",
    "nected with a Softmax activation function. The Adam optimizer\n",
    "and the cross-entropy loss function were used in the network. \n",
    "The output is a probability distribution over 89 classes.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "After training, model is serialized and uploaded to W&B project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XhFWPE1BvnLZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 07:53:51.285714: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import shutil\n",
    "from typing import List\n",
    "\n",
    "def load_data(run, artifact_name = f\"phcd_paper_splits_tfds\") -> List[tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Downloads datasets from a wandb artifact and loads them into a list of tf.data.Datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    artifact = run.use_artifact(f\"master-thesis/{artifact_name}:latest\")\n",
    "    artifact_dir = pathlib.Path(\n",
    "        f\"./artifacts/{artifact.name.replace(':', '-')}\"\n",
    "    ).resolve()\n",
    "    if not artifact_dir.exists():\n",
    "        artifact_dir = artifact.download()\n",
    "        artifact_dir = pathlib.Path(artifact_dir).resolve()\n",
    "\n",
    "    # if tf.__version__ minor is less than 10, use\n",
    "    # tf.data.experimental.load instead of tf.data.Dataset.load\n",
    "\n",
    "    if int(tf.__version__.split(\".\")[1]) < 10:\n",
    "        load_function = tf.data.experimental.load\n",
    "    else:\n",
    "        load_function = tf.data.Dataset.load\n",
    "    \n",
    "    output_list = []\n",
    "    for split in [\"train\", \"test\", \"val\"]:\n",
    "        ds = load_function(str(artifact_dir / split), compression=\"GZIP\")\n",
    "        output_list.append(ds)\n",
    "    \n",
    "    return output_list\n",
    "\n",
    "def get_number_of_classes(ds: tf.data.Dataset) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of classes in a dataset.\n",
    "    \"\"\"\n",
    "    labels_iterator= ds.map(lambda x, y: y).as_numpy_iterator()\n",
    "    labels = np.concatenate(list(labels_iterator))\n",
    "    return len(np.unique(labels))\n",
    "\n",
    "def preprocess_dataset(ds: tf.data.Dataset, batch_size: int, cache: bool = True) -> tf.data.Dataset:\n",
    "    ds = ds.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y))  # normalize\n",
    "    ds = ds.unbatch().batch(batch_size)\n",
    "    if cache:\n",
    "        ds = ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def calculate_model_size_on_disk(path: str) -> int:\n",
    "    return pathlib.Path(path).stat().st_size    \n",
    "\n",
    "def calculate_model_num_parameters(model: tf.keras.Model) -> int:\n",
    "    return model.count_params()\n",
    "\n",
    "def calculate_model_flops(summary) -> float:\n",
    "    # from run.summary get GFLOPs or GFLOPS whichever is available\n",
    "    print(summary.keys())\n",
    "    if \"GFLOPs\" in summary.keys():\n",
    "        return summary.get(\"GFLOPs\")\n",
    "    elif \"GFLOPS\" in summary.keys():\n",
    "        return summary.get(\"GFLOPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rgJaTJFlvnLb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Available devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 07:53:54.546334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-24 07:53:54.559553: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-24 07:53:54.559748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Available devices: \", tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rDmqDY0tvnLc"
   },
   "outputs": [],
   "source": [
    "def train(model_name, defaults, artifact_name=\"phcd_paper_splits_tfds\"):\n",
    "    with wandb.init(project=\"master-thesis\", job_type=\"training\", name=model_name, config=defaults, tags=[artifact_name]) as run:\n",
    "        # hyperparameters\n",
    "        epochs = wandb.config.epochs\n",
    "        bs = wandb.config.batch_size\n",
    "\n",
    "        ds_train, ds_test, ds_val = load_data(run, artifact_name=artifact_name)\n",
    "\n",
    "        num_classes = get_number_of_classes(ds_val)\n",
    "\n",
    "        ds_train = preprocess_dataset(ds_train, batch_size=bs)\n",
    "        ds_val = preprocess_dataset(ds_val, batch_size=bs)\n",
    "        ds_test = preprocess_dataset(ds_test, batch_size=bs, cache=False)\n",
    "\n",
    "        '''\n",
    "        The architecture of the concrete CNN is shown in Fig. 2.\n",
    "        The input is a 32x32 binarized matrix. \n",
    "        The input is then propagated through 12 adaptable layers. \n",
    "        First come two convolutional layers having 32 filters with the size of 3x3 and stride 1.\n",
    "        Secondly, the output of the convolutional layer is fed to the\n",
    "        ReLU function. The output is down-sampled using a max-pool-\n",
    "        ing operation with a 2x2 stride. Next, the dropout technique is\n",
    "        used with the coefficient 0.25. The four operations (two con-\n",
    "        volutions, nonlinearity, max-pooling, and dropout) are repeated,\n",
    "        using 64 filters for the convolutional layers. The output of the\n",
    "        last layer is then flattened and fed through a fully connected\n",
    "        layer with 256 neurons and ReLU nonlinearities, dropped out\n",
    "        with the 0.25 coefficient, and a final output layer is fully con-\n",
    "        nected with a Softmax activation function. The Adam optimizer\n",
    "        and the cross-entropy loss function were used in the network\n",
    "        [24]. The output is a probability distribution over 89 classes.\n",
    "        '''\n",
    "\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(32, 32, 1)),\n",
    "                \n",
    "                # 2 Convolutional layers with 32 filters, 3x3 size, and stride 1\n",
    "                tf.keras.layers.Conv2D(32, (3, 3), strides=1, activation='relu'),\n",
    "                tf.keras.layers.Conv2D(32, (3, 3), strides=1, activation='relu'),\n",
    "                \n",
    "                # Max-pooling operation with 2x2 stride\n",
    "                tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "                # Dropout with coefficient 0.25\n",
    "                tf.keras.layers.Dropout(0.25),\n",
    "                \n",
    "                # Repeat above 4 operations using 64 filters\n",
    "                tf.keras.layers.Conv2D(64, (3, 3), strides=1, activation='relu'),\n",
    "                tf.keras.layers.Conv2D(64, (3, 3), strides=1, activation='relu'),\n",
    "                tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "                tf.keras.layers.Dropout(0.25),\n",
    "                \n",
    "                # Flatten the output and feed through fully connected layer with 256 neurons\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(256, activation='relu'),\n",
    "\n",
    "                # Dropout with coefficient 0.25\n",
    "                tf.keras.layers.Dropout(0.25),\n",
    "                tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "\n",
    "        # save the best model\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=f\"./artifacts/{model_name}.h5\",\n",
    "            save_weights_only=False,\n",
    "            monitor=\"val_accuracy\",\n",
    "            mode=\"max\",\n",
    "            save_best_only=True,\n",
    "        )\n",
    "\n",
    "        wandb_callback = wandb.keras.WandbCallback(\n",
    "            save_model=False,\n",
    "            compute_flops=True,\n",
    "        )\n",
    "\n",
    "        history = model.fit(\n",
    "            ds_train,\n",
    "            epochs=epochs,\n",
    "            validation_data=ds_val,\n",
    "            callbacks=[wandb_callback, checkpoint_callback],\n",
    "        )\n",
    "\n",
    "        # calculate model size on disk, flops and number of parameters\n",
    "\n",
    "        flops = calculate_model_flops(wandb.run.summary)\n",
    "        disk_size = calculate_model_size_on_disk(f\"./artifacts/{model_name}.h5\")\n",
    "        num_parameters = calculate_model_num_parameters(model)\n",
    "\n",
    "        # evaluate model on ds_test and log to wandb\n",
    "        test_loss, test_acc = model.evaluate(ds_test)\n",
    "\n",
    "        wandb.log({\n",
    "            \"test loss\": test_loss, \n",
    "            \"test accuracy\": test_acc, \n",
    "            \"number of parameters\": num_parameters,\n",
    "            \"disk size\": disk_size, \n",
    "            \"model flops\": flops\n",
    "            })\n",
    "            \n",
    "        # save artifact to wandb\n",
    "        artifact = wandb.Artifact(\n",
    "            name=model_name,\n",
    "            type=\"model\"\n",
    "        )\n",
    "\n",
    "        # save best model to artifact\n",
    "        artifact.add_file(f\"./artifacts/{model_name}.h5\")\n",
    "        run.log_artifact(artifact)\n",
    "        run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JMjD5hwsvnLd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgratkadlafana\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20230124_075355-1umlx170</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/gratkadlafana/master-thesis/runs/1umlx170\" target=\"_blank\">baseline-numbers</a></strong> to <a href=\"https://wandb.ai/gratkadlafana/master-thesis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/gratkadlafana/master-thesis\" target=\"_blank\">https://wandb.ai/gratkadlafana/master-thesis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/gratkadlafana/master-thesis/runs/1umlx170\" target=\"_blank\">https://wandb.ai/gratkadlafana/master-thesis/runs/1umlx170</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact numbers_splits_tfds:latest, 50.52MB. 12 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   12 of 12 files downloaded.  \n",
      "Done. 0:0:1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 07:53:59.652209: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-24 07:53:59.653323: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-24 07:53:59.653577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-24 07:53:59.653794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-24 07:54:00.775957: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-24 07:54:00.776521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-24 07:54:00.776534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-01-24 07:54:00.776747: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-24 07:54:00.777043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2075 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 SUPER, pci bus id: 0000:26:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/nn_ops.py:5250: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 07:54:01.259541: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-24 07:54:01.259594: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2023-01-24 07:54:01.259785: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-01-24 07:54:01.261267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-24 07:54:01.261511: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-24 07:54:01.261708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-24 07:54:01.262048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-24 07:54:01.262060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-01-24 07:54:01.262245: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-24 07:54:01.262269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2075 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 SUPER, pci bus id: 0000:26:00.0, compute capability: 7.5\n",
      "/usr/local/lib/python3.8/dist-packages/keras/backend.py:5585: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "2023-01-24 07:54:02.107352: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-01-24 07:54:02.854228: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2023-01-24 07:54:04.755023: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f8658007600 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-24 07:54:04.755073: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce GTX 1650 SUPER, Compute Capability 7.5\n",
      "2023-01-24 07:54:04.778510: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-01-24 07:54:05.034777: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 13s 12ms/step - loss: 0.2237 - accuracy: 0.9292 - val_loss: 0.0485 - val_accuracy: 0.9847\n",
      "Epoch 2/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0623 - accuracy: 0.9820 - val_loss: 0.0383 - val_accuracy: 0.9885\n",
      "Epoch 3/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0421 - accuracy: 0.9875 - val_loss: 0.0218 - val_accuracy: 0.9933\n",
      "Epoch 4/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0368 - accuracy: 0.9889 - val_loss: 0.0158 - val_accuracy: 0.9958\n",
      "Epoch 5/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0273 - accuracy: 0.9913 - val_loss: 0.0189 - val_accuracy: 0.9935\n",
      "Epoch 6/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0267 - accuracy: 0.9917 - val_loss: 0.0122 - val_accuracy: 0.9963\n",
      "Epoch 7/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0204 - accuracy: 0.9938 - val_loss: 0.0096 - val_accuracy: 0.9972\n",
      "Epoch 8/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0200 - accuracy: 0.9938 - val_loss: 0.0058 - val_accuracy: 0.9989\n",
      "Epoch 9/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0178 - accuracy: 0.9946 - val_loss: 0.0040 - val_accuracy: 0.9989\n",
      "Epoch 10/100\n",
      "675/675 [==============================] - 8s 11ms/step - loss: 0.0139 - accuracy: 0.9954 - val_loss: 0.0047 - val_accuracy: 0.9987\n",
      "Epoch 11/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0137 - accuracy: 0.9953 - val_loss: 0.0049 - val_accuracy: 0.9984\n",
      "Epoch 12/100\n",
      "675/675 [==============================] - 8s 11ms/step - loss: 0.0120 - accuracy: 0.9959 - val_loss: 0.0082 - val_accuracy: 0.9982\n",
      "Epoch 13/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0116 - accuracy: 0.9962 - val_loss: 0.0049 - val_accuracy: 0.9987\n",
      "Epoch 14/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0118 - accuracy: 0.9961 - val_loss: 0.0021 - val_accuracy: 0.9989\n",
      "Epoch 15/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0112 - accuracy: 0.9962 - val_loss: 0.0030 - val_accuracy: 0.9990\n",
      "Epoch 16/100\n",
      "675/675 [==============================] - 8s 11ms/step - loss: 0.0088 - accuracy: 0.9970 - val_loss: 0.0025 - val_accuracy: 0.9990\n",
      "Epoch 17/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0097 - accuracy: 0.9969 - val_loss: 0.0034 - val_accuracy: 0.9989\n",
      "Epoch 18/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0078 - accuracy: 0.9973 - val_loss: 0.0026 - val_accuracy: 0.9992\n",
      "Epoch 19/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0093 - accuracy: 0.9972 - val_loss: 0.0025 - val_accuracy: 0.9992\n",
      "Epoch 20/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0091 - accuracy: 0.9973 - val_loss: 0.0016 - val_accuracy: 0.9995\n",
      "Epoch 21/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0083 - accuracy: 0.9973 - val_loss: 0.0032 - val_accuracy: 0.9990\n",
      "Epoch 22/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0077 - accuracy: 0.9975 - val_loss: 0.0019 - val_accuracy: 0.9994\n",
      "Epoch 23/100\n",
      "675/675 [==============================] - 8s 11ms/step - loss: 0.0078 - accuracy: 0.9975 - val_loss: 3.3285e-04 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0072 - accuracy: 0.9976 - val_loss: 0.0014 - val_accuracy: 0.9994\n",
      "Epoch 25/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0055 - accuracy: 0.9981 - val_loss: 0.0028 - val_accuracy: 0.9990\n",
      "Epoch 26/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0091 - accuracy: 0.9973 - val_loss: 2.5473e-04 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0055 - accuracy: 0.9982 - val_loss: 0.0017 - val_accuracy: 0.9995\n",
      "Epoch 28/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9997\n",
      "Epoch 29/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 8.5615e-04 - val_accuracy: 0.9998\n",
      "Epoch 30/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0059 - accuracy: 0.9981 - val_loss: 1.1204e-04 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0060 - accuracy: 0.9978 - val_loss: 0.0018 - val_accuracy: 0.9995\n",
      "Epoch 32/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0064 - accuracy: 0.9978 - val_loss: 0.0012 - val_accuracy: 0.9995\n",
      "Epoch 33/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0077 - accuracy: 0.9976 - val_loss: 3.1513e-04 - val_accuracy: 0.9998\n",
      "Epoch 34/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 9.3603e-04 - val_accuracy: 0.9995\n",
      "Epoch 35/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.0012 - val_accuracy: 0.9998\n",
      "Epoch 36/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0052 - accuracy: 0.9984 - val_loss: 6.1490e-04 - val_accuracy: 0.9997\n",
      "Epoch 37/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0046 - accuracy: 0.9986 - val_loss: 0.0016 - val_accuracy: 0.9992\n",
      "Epoch 38/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0067 - accuracy: 0.9981 - val_loss: 8.3467e-04 - val_accuracy: 0.9997\n",
      "Epoch 39/100\n",
      "675/675 [==============================] - 8s 11ms/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.0023 - val_accuracy: 0.9997\n",
      "Epoch 40/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 3.2375e-04 - val_accuracy: 0.9998\n",
      "Epoch 41/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0051 - accuracy: 0.9984 - val_loss: 5.0568e-04 - val_accuracy: 0.9998\n",
      "Epoch 42/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0022 - val_accuracy: 0.9997\n",
      "Epoch 43/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0055 - accuracy: 0.9984 - val_loss: 1.8469e-04 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0045 - accuracy: 0.9985 - val_loss: 3.6702e-05 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0047 - accuracy: 0.9986 - val_loss: 7.2084e-05 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 3.9714e-04 - val_accuracy: 0.9998\n",
      "Epoch 47/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.0029 - val_accuracy: 0.9994\n",
      "Epoch 48/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0058 - accuracy: 0.9981 - val_loss: 2.9806e-05 - val_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0050 - accuracy: 0.9984 - val_loss: 1.5016e-04 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0043 - accuracy: 0.9985 - val_loss: 5.8211e-04 - val_accuracy: 0.9998\n",
      "Epoch 51/100\n",
      "675/675 [==============================] - 8s 11ms/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 1.4102e-04 - val_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 6.6355e-04 - val_accuracy: 0.9998\n",
      "Epoch 53/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0065 - accuracy: 0.9981 - val_loss: 4.7525e-05 - val_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0047 - accuracy: 0.9986 - val_loss: 4.5511e-05 - val_accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 3.2692e-04 - val_accuracy: 0.9998\n",
      "Epoch 56/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0039 - accuracy: 0.9987 - val_loss: 1.4028e-04 - val_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 7.6191e-04 - val_accuracy: 0.9998\n",
      "Epoch 58/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0062 - accuracy: 0.9984 - val_loss: 3.2952e-04 - val_accuracy: 0.9998\n",
      "Epoch 59/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 5.5256e-04 - val_accuracy: 0.9997\n",
      "Epoch 60/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 1.0785e-04 - val_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 3.3944e-04 - val_accuracy: 0.9998\n",
      "Epoch 62/100\n",
      "675/675 [==============================] - 8s 12ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 1.8329e-05 - val_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0045 - accuracy: 0.9987 - val_loss: 8.6664e-05 - val_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0041 - accuracy: 0.9989 - val_loss: 3.1860e-04 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 1.6431e-04 - val_accuracy: 0.9998\n",
      "Epoch 66/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0047 - accuracy: 0.9989 - val_loss: 7.3831e-05 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 4.1750e-05 - val_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "675/675 [==============================] - 8s 11ms/step - loss: 0.0056 - accuracy: 0.9985 - val_loss: 5.0605e-05 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0044 - accuracy: 0.9989 - val_loss: 2.5200e-05 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 1.1491e-05 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0045 - accuracy: 0.9989 - val_loss: 3.5688e-05 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 3.3540e-06 - val_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 1.1798e-05 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0045 - accuracy: 0.9987 - val_loss: 4.1444e-06 - val_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0055 - accuracy: 0.9987 - val_loss: 1.1519e-04 - val_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0049 - accuracy: 0.9987 - val_loss: 4.3680e-04 - val_accuracy: 0.9998\n",
      "Epoch 77/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 1.2761e-05 - val_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "675/675 [==============================] - 8s 12ms/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 2.7561e-05 - val_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0045 - accuracy: 0.9991 - val_loss: 6.7587e-06 - val_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 1.3340e-05 - val_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "675/675 [==============================] - 9s 13ms/step - loss: 0.0049 - accuracy: 0.9989 - val_loss: 4.8229e-05 - val_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 9.9339e-06 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 2.3451e-04 - val_accuracy: 0.9998\n",
      "Epoch 84/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 3.2917e-05 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "675/675 [==============================] - 8s 12ms/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 5.5493e-06 - val_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0047 - accuracy: 0.9989 - val_loss: 2.0725e-05 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0035 - accuracy: 0.9990 - val_loss: 3.7074e-05 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 2.5860e-06 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0044 - accuracy: 0.9989 - val_loss: 9.7295e-04 - val_accuracy: 0.9995\n",
      "Epoch 90/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0043 - accuracy: 0.9987 - val_loss: 4.8004e-04 - val_accuracy: 0.9997\n",
      "Epoch 91/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 1.6605e-04 - val_accuracy: 0.9998\n",
      "Epoch 92/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0049 - accuracy: 0.9989 - val_loss: 0.0015 - val_accuracy: 0.9997\n",
      "Epoch 93/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 2.5735e-05 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0052 - accuracy: 0.9986 - val_loss: 2.9193e-04 - val_accuracy: 0.9998\n",
      "Epoch 95/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0066 - accuracy: 0.9984 - val_loss: 9.7904e-06 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 3.8161e-06 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 4.5787e-05 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 2.0116e-04 - val_accuracy: 0.9998\n",
      "Epoch 99/100\n",
      "675/675 [==============================] - 7s 10ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 8.7026e-05 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "675/675 [==============================] - 7s 11ms/step - loss: 0.0053 - accuracy: 0.9989 - val_loss: 3.9419e-04 - val_accuracy: 0.9998\n",
      "['GFLOPs', 'graph', 'epoch', '_timestamp', 'loss', 'accuracy', 'val_loss', 'val_accuracy', '_runtime', '_step', 'best_val_loss', 'best_epoch']\n",
      "193/193 [==============================] - 4s 18ms/step - loss: 1.9956e-04 - accuracy: 0.9999\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▇▇▇████████████████████████████████████</td></tr><tr><td>disk size</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>model flops</td><td>▁</td></tr><tr><td>number of parameters</td><td>▁</td></tr><tr><td>test accuracy</td><td>▁</td></tr><tr><td>test loss</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇▇▇██████████████████████████████████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary.keys():</h3><br/><table class=\"wandb\"><tr><td>GFLOPs</td><td>0.01429</td></tr><tr><td>accuracy</td><td>0.99891</td></tr><tr><td>best_epoch</td><td>87</td></tr><tr><td>best_val_loss</td><td>0.0</td></tr><tr><td>disk size</td><td>5788984</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>0.00527</td></tr><tr><td>model flops</td><td>0.01429</td></tr><tr><td>number of parameters</td><td>477418</td></tr><tr><td>test accuracy</td><td>0.99992</td></tr><tr><td>test loss</td><td>0.0002</td></tr><tr><td>val_accuracy</td><td>0.99984</td></tr><tr><td>val_loss</td><td>0.00039</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">baseline-numbers</strong> at: <a href=\"https://wandb.ai/gratkadlafana/master-thesis/runs/1umlx170\" target=\"_blank\">https://wandb.ai/gratkadlafana/master-thesis/runs/1umlx170</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230124_075355-1umlx170/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "defaults = dict(\n",
    "    batch_size=32*2,\n",
    "    epochs=100,    \n",
    "    optimizer=\"adam\"\n",
    ")\n",
    "\n",
    "#train(\"baseline-uppercase-diacritics\", defaults, artifact_name=\"uppercase_splits_tfds\")\n",
    "train(\"baseline-numbers\", defaults, artifact_name=\"numbers_splits_tfds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PcUhYV31vrJi"
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Stop training",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStop training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Stop training"
     ]
    }
   ],
   "source": [
    "raise Exception(\"Stop training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYHU5y_rvrRm"
   },
   "outputs": [],
   "source": [
    "train(\"baseline-uppercase\", defaults, artifact_name=\"uppercase_no_diacritics_splits_tfds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGOJjgPfvrgY"
   },
   "outputs": [],
   "source": [
    "train(\"baseline-lowercase-diacritics\", defaults, artifact_name=\"lowercase_splits_tfds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "v-qOZAQDv-AJ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20230124_081044-v1h7nmtj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/gratkadlafana/master-thesis/runs/v1h7nmtj\" target=\"_blank\">baseline-lowercase</a></strong> to <a href=\"https://wandb.ai/gratkadlafana/master-thesis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/gratkadlafana/master-thesis\" target=\"_blank\">https://wandb.ai/gratkadlafana/master-thesis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/gratkadlafana/master-thesis/runs/v1h7nmtj\" target=\"_blank\">https://wandb.ai/gratkadlafana/master-thesis/runs/v1h7nmtj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact lowercase_no_diacritics_splits_tfds:latest, 82.10MB. 18 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   18 of 18 files downloaded.  \n",
      "Done. 0:0:0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 08:10:49.496457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-24 08:10:49.499152: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2023-01-24 08:10:49.499546: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-01-24 08:10:49.501212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-24 08:10:49.501962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-24 08:10:49.502143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-24 08:10:49.545262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-24 08:10:49.545293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-01-24 08:10:49.545497: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-01-24 08:10:49.545537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2075 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 SUPER, pci bus id: 0000:26:00.0, compute capability: 7.5\n",
      "/usr/local/lib/python3.8/dist-packages/keras/backend.py:5585: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "2023-01-24 08:10:50.522839: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_1/dropout_3/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4/Unknown - 2s 67ms/step - loss: 3.2627 - accuracy: 0.0508WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0178s vs `on_train_batch_end` time: 0.0240s). Check your callbacks.\n",
      "1806/1806 [==============================] - 22s 11ms/step - loss: 0.5630 - accuracy: 0.8318 - val_loss: 0.2716 - val_accuracy: 0.9144\n",
      "Epoch 2/100\n",
      "1806/1806 [==============================] - 20s 11ms/step - loss: 0.3065 - accuracy: 0.9065 - val_loss: 0.2039 - val_accuracy: 0.9371\n",
      "Epoch 3/100\n",
      "1806/1806 [==============================] - 20s 11ms/step - loss: 0.2651 - accuracy: 0.9189 - val_loss: 0.1865 - val_accuracy: 0.9418\n",
      "Epoch 4/100\n",
      "1806/1806 [==============================] - 20s 11ms/step - loss: 0.2419 - accuracy: 0.9259 - val_loss: 0.1666 - val_accuracy: 0.9463\n",
      "Epoch 5/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.2237 - accuracy: 0.9294 - val_loss: 0.1527 - val_accuracy: 0.9496\n",
      "Epoch 6/100\n",
      "1806/1806 [==============================] - 20s 11ms/step - loss: 0.2113 - accuracy: 0.9323 - val_loss: 0.1483 - val_accuracy: 0.9502\n",
      "Epoch 7/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.2011 - accuracy: 0.9357 - val_loss: 0.1397 - val_accuracy: 0.9540\n",
      "Epoch 8/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.1903 - accuracy: 0.9382 - val_loss: 0.1323 - val_accuracy: 0.9562\n",
      "Epoch 9/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.1839 - accuracy: 0.9409 - val_loss: 0.1171 - val_accuracy: 0.9604\n",
      "Epoch 10/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.1754 - accuracy: 0.9424 - val_loss: 0.1161 - val_accuracy: 0.9612\n",
      "Epoch 11/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.1706 - accuracy: 0.9440 - val_loss: 0.1068 - val_accuracy: 0.9647\n",
      "Epoch 12/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.1630 - accuracy: 0.9462 - val_loss: 0.1037 - val_accuracy: 0.9653\n",
      "Epoch 13/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.1590 - accuracy: 0.9462 - val_loss: 0.0985 - val_accuracy: 0.9666\n",
      "Epoch 14/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.1532 - accuracy: 0.9483 - val_loss: 0.0958 - val_accuracy: 0.9667\n",
      "Epoch 15/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.1500 - accuracy: 0.9493 - val_loss: 0.0898 - val_accuracy: 0.9695\n",
      "Epoch 16/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.1487 - accuracy: 0.9499 - val_loss: 0.0860 - val_accuracy: 0.9705\n",
      "Epoch 17/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.1449 - accuracy: 0.9502 - val_loss: 0.0848 - val_accuracy: 0.9709\n",
      "Epoch 18/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.1398 - accuracy: 0.9523 - val_loss: 0.0833 - val_accuracy: 0.9713\n",
      "Epoch 19/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.1385 - accuracy: 0.9528 - val_loss: 0.0779 - val_accuracy: 0.9733\n",
      "Epoch 20/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.1337 - accuracy: 0.9546 - val_loss: 0.0747 - val_accuracy: 0.9738\n",
      "Epoch 21/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.1323 - accuracy: 0.9545 - val_loss: 0.0739 - val_accuracy: 0.9739\n",
      "Epoch 22/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.1290 - accuracy: 0.9552 - val_loss: 0.0682 - val_accuracy: 0.9774\n",
      "Epoch 23/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.1269 - accuracy: 0.9559 - val_loss: 0.0664 - val_accuracy: 0.9779\n",
      "Epoch 24/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.1254 - accuracy: 0.9563 - val_loss: 0.0674 - val_accuracy: 0.9751\n",
      "Epoch 25/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.1236 - accuracy: 0.9573 - val_loss: 0.0623 - val_accuracy: 0.9775\n",
      "Epoch 26/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.1223 - accuracy: 0.9573 - val_loss: 0.0614 - val_accuracy: 0.9789\n",
      "Epoch 27/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.1201 - accuracy: 0.9586 - val_loss: 0.0584 - val_accuracy: 0.9792\n",
      "Epoch 28/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.1192 - accuracy: 0.9583 - val_loss: 0.0570 - val_accuracy: 0.9801\n",
      "Epoch 29/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.1155 - accuracy: 0.9593 - val_loss: 0.0584 - val_accuracy: 0.9799\n",
      "Epoch 30/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.1160 - accuracy: 0.9588 - val_loss: 0.0537 - val_accuracy: 0.9804\n",
      "Epoch 31/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.1130 - accuracy: 0.9601 - val_loss: 0.0523 - val_accuracy: 0.9819\n",
      "Epoch 32/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.1123 - accuracy: 0.9604 - val_loss: 0.0493 - val_accuracy: 0.9839\n",
      "Epoch 33/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.1104 - accuracy: 0.9613 - val_loss: 0.0508 - val_accuracy: 0.9822\n",
      "Epoch 34/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.1101 - accuracy: 0.9612 - val_loss: 0.0498 - val_accuracy: 0.9822\n",
      "Epoch 35/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.1105 - accuracy: 0.9613 - val_loss: 0.0511 - val_accuracy: 0.9815\n",
      "Epoch 36/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.1110 - accuracy: 0.9608 - val_loss: 0.0459 - val_accuracy: 0.9836\n",
      "Epoch 37/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.1087 - accuracy: 0.9618 - val_loss: 0.0462 - val_accuracy: 0.9839\n",
      "Epoch 38/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.1056 - accuracy: 0.9625 - val_loss: 0.0454 - val_accuracy: 0.9853\n",
      "Epoch 39/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.1056 - accuracy: 0.9625 - val_loss: 0.0482 - val_accuracy: 0.9833\n",
      "Epoch 40/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.1056 - accuracy: 0.9624 - val_loss: 0.0440 - val_accuracy: 0.9844\n",
      "Epoch 41/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.1046 - accuracy: 0.9631 - val_loss: 0.0460 - val_accuracy: 0.9836\n",
      "Epoch 42/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.1037 - accuracy: 0.9635 - val_loss: 0.0432 - val_accuracy: 0.9851\n",
      "Epoch 43/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.1022 - accuracy: 0.9642 - val_loss: 0.0409 - val_accuracy: 0.9868\n",
      "Epoch 44/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.1027 - accuracy: 0.9640 - val_loss: 0.0421 - val_accuracy: 0.9853\n",
      "Epoch 45/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.1005 - accuracy: 0.9645 - val_loss: 0.0421 - val_accuracy: 0.9847\n",
      "Epoch 46/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.1024 - accuracy: 0.9634 - val_loss: 0.0395 - val_accuracy: 0.9876\n",
      "Epoch 47/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.0993 - accuracy: 0.9641 - val_loss: 0.0380 - val_accuracy: 0.9875\n",
      "Epoch 48/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.0993 - accuracy: 0.9652 - val_loss: 0.0392 - val_accuracy: 0.9870\n",
      "Epoch 49/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.0985 - accuracy: 0.9653 - val_loss: 0.0431 - val_accuracy: 0.9861\n",
      "Epoch 50/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.0986 - accuracy: 0.9649 - val_loss: 0.0365 - val_accuracy: 0.9884\n",
      "Epoch 51/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.0949 - accuracy: 0.9664 - val_loss: 0.0352 - val_accuracy: 0.9883\n",
      "Epoch 52/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.0961 - accuracy: 0.9660 - val_loss: 0.0328 - val_accuracy: 0.9896\n",
      "Epoch 53/100\n",
      "1806/1806 [==============================] - 20s 11ms/step - loss: 0.0963 - accuracy: 0.9659 - val_loss: 0.0379 - val_accuracy: 0.9880\n",
      "Epoch 54/100\n",
      "1806/1806 [==============================] - 20s 11ms/step - loss: 0.0969 - accuracy: 0.9654 - val_loss: 0.0352 - val_accuracy: 0.9875\n",
      "Epoch 55/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.0958 - accuracy: 0.9655 - val_loss: 0.0373 - val_accuracy: 0.9870\n",
      "Epoch 56/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.0949 - accuracy: 0.9662 - val_loss: 0.0338 - val_accuracy: 0.9890\n",
      "Epoch 57/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.0968 - accuracy: 0.9659 - val_loss: 0.0342 - val_accuracy: 0.9892\n",
      "Epoch 58/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.0950 - accuracy: 0.9664 - val_loss: 0.0357 - val_accuracy: 0.9890\n",
      "Epoch 59/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.0942 - accuracy: 0.9667 - val_loss: 0.0346 - val_accuracy: 0.9891\n",
      "Epoch 60/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.0936 - accuracy: 0.9667 - val_loss: 0.0327 - val_accuracy: 0.9892\n",
      "Epoch 61/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.0930 - accuracy: 0.9667 - val_loss: 0.0329 - val_accuracy: 0.9898\n",
      "Epoch 62/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.0950 - accuracy: 0.9664 - val_loss: 0.0325 - val_accuracy: 0.9901\n",
      "Epoch 63/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.0926 - accuracy: 0.9667 - val_loss: 0.0314 - val_accuracy: 0.9904\n",
      "Epoch 64/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.0926 - accuracy: 0.9675 - val_loss: 0.0328 - val_accuracy: 0.9890\n",
      "Epoch 65/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.0935 - accuracy: 0.9671 - val_loss: 0.0294 - val_accuracy: 0.9899\n",
      "Epoch 66/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.0899 - accuracy: 0.9682 - val_loss: 0.0276 - val_accuracy: 0.9910\n",
      "Epoch 67/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.0892 - accuracy: 0.9683 - val_loss: 0.0284 - val_accuracy: 0.9907\n",
      "Epoch 68/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.0908 - accuracy: 0.9682 - val_loss: 0.0288 - val_accuracy: 0.9899\n",
      "Epoch 69/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.0908 - accuracy: 0.9684 - val_loss: 0.0274 - val_accuracy: 0.9915\n",
      "Epoch 70/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.0873 - accuracy: 0.9688 - val_loss: 0.0292 - val_accuracy: 0.9906\n",
      "Epoch 71/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.0905 - accuracy: 0.9683 - val_loss: 0.0272 - val_accuracy: 0.9915\n",
      "Epoch 72/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.0876 - accuracy: 0.9688 - val_loss: 0.0274 - val_accuracy: 0.9913\n",
      "Epoch 73/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.0870 - accuracy: 0.9695 - val_loss: 0.0301 - val_accuracy: 0.9905\n",
      "Epoch 74/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.0880 - accuracy: 0.9690 - val_loss: 0.0272 - val_accuracy: 0.9915\n",
      "Epoch 75/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.0890 - accuracy: 0.9690 - val_loss: 0.0305 - val_accuracy: 0.9903\n",
      "Epoch 76/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.0875 - accuracy: 0.9693 - val_loss: 0.0306 - val_accuracy: 0.9898\n",
      "Epoch 77/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.0884 - accuracy: 0.9685 - val_loss: 0.0266 - val_accuracy: 0.9913\n",
      "Epoch 78/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.0879 - accuracy: 0.9688 - val_loss: 0.0254 - val_accuracy: 0.9924\n",
      "Epoch 79/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.0881 - accuracy: 0.9688 - val_loss: 0.0257 - val_accuracy: 0.9916\n",
      "Epoch 80/100\n",
      "1806/1806 [==============================] - 18s 10ms/step - loss: 0.0868 - accuracy: 0.9691 - val_loss: 0.0270 - val_accuracy: 0.9919\n",
      "Epoch 81/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.0864 - accuracy: 0.9694 - val_loss: 0.0256 - val_accuracy: 0.9919\n",
      "Epoch 82/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.0859 - accuracy: 0.9696 - val_loss: 0.0249 - val_accuracy: 0.9921\n",
      "Epoch 83/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.0853 - accuracy: 0.9704 - val_loss: 0.0261 - val_accuracy: 0.9915\n",
      "Epoch 84/100\n",
      "1806/1806 [==============================] - 20s 11ms/step - loss: 0.0848 - accuracy: 0.9698 - val_loss: 0.0241 - val_accuracy: 0.9932\n",
      "Epoch 85/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.0856 - accuracy: 0.9697 - val_loss: 0.0261 - val_accuracy: 0.9924\n",
      "Epoch 86/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.0821 - accuracy: 0.9715 - val_loss: 0.0245 - val_accuracy: 0.9918\n",
      "Epoch 87/100\n",
      "1806/1806 [==============================] - 20s 11ms/step - loss: 0.0833 - accuracy: 0.9707 - val_loss: 0.0236 - val_accuracy: 0.9925\n",
      "Epoch 88/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.0859 - accuracy: 0.9703 - val_loss: 0.0275 - val_accuracy: 0.9915\n",
      "Epoch 89/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.0845 - accuracy: 0.9701 - val_loss: 0.0266 - val_accuracy: 0.9920\n",
      "Epoch 90/100\n",
      "1806/1806 [==============================] - 20s 11ms/step - loss: 0.0852 - accuracy: 0.9703 - val_loss: 0.0269 - val_accuracy: 0.9924\n",
      "Epoch 91/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.0840 - accuracy: 0.9705 - val_loss: 0.0256 - val_accuracy: 0.9918\n",
      "Epoch 92/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.0852 - accuracy: 0.9698 - val_loss: 0.0235 - val_accuracy: 0.9921\n",
      "Epoch 93/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.0854 - accuracy: 0.9704 - val_loss: 0.0269 - val_accuracy: 0.9918\n",
      "Epoch 94/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.0822 - accuracy: 0.9707 - val_loss: 0.0200 - val_accuracy: 0.9943\n",
      "Epoch 95/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.0825 - accuracy: 0.9711 - val_loss: 0.0206 - val_accuracy: 0.9941\n",
      "Epoch 96/100\n",
      "1806/1806 [==============================] - 19s 11ms/step - loss: 0.0828 - accuracy: 0.9710 - val_loss: 0.0227 - val_accuracy: 0.9925\n",
      "Epoch 97/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.0857 - accuracy: 0.9702 - val_loss: 0.0247 - val_accuracy: 0.9927\n",
      "Epoch 98/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.0835 - accuracy: 0.9708 - val_loss: 0.0217 - val_accuracy: 0.9923\n",
      "Epoch 99/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.0800 - accuracy: 0.9719 - val_loss: 0.0197 - val_accuracy: 0.9939\n",
      "Epoch 100/100\n",
      "1806/1806 [==============================] - 19s 10ms/step - loss: 0.0799 - accuracy: 0.9718 - val_loss: 0.0203 - val_accuracy: 0.9936\n",
      "['GFLOPs', 'graph', 'epoch', '_timestamp', 'loss', 'accuracy', 'val_loss', 'val_accuracy', '_runtime', '_step', 'best_val_loss', 'best_epoch']\n",
      "516/516 [==============================] - 3s 5ms/step - loss: 0.0205 - accuracy: 0.9936\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▅▆▆▇▇▇▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>disk size</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>model flops</td><td>▁</td></tr><tr><td>number of parameters</td><td>▁</td></tr><tr><td>test accuracy</td><td>▁</td></tr><tr><td>test loss</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>val_loss</td><td>█▆▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary.keys():</h3><br/><table class=\"wandb\"><tr><td>GFLOPs</td><td>0.01429</td></tr><tr><td>accuracy</td><td>0.97177</td></tr><tr><td>best_epoch</td><td>98</td></tr><tr><td>best_val_loss</td><td>0.01967</td></tr><tr><td>disk size</td><td>5838128</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>0.07989</td></tr><tr><td>model flops</td><td>0.01429</td></tr><tr><td>number of parameters</td><td>481530</td></tr><tr><td>test accuracy</td><td>0.99358</td></tr><tr><td>test loss</td><td>0.02048</td></tr><tr><td>val_accuracy</td><td>0.99364</td></tr><tr><td>val_loss</td><td>0.02033</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">baseline-lowercase</strong> at: <a href=\"https://wandb.ai/gratkadlafana/master-thesis/runs/v1h7nmtj\" target=\"_blank\">https://wandb.ai/gratkadlafana/master-thesis/runs/v1h7nmtj</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230124_081044-v1h7nmtj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(\"baseline-lowercase\", defaults, artifact_name=\"lowercase_no_diacritics_splits_tfds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jt9m0TrnwDM9"
   },
   "outputs": [],
   "source": [
    "train(\"baseline\", defaults, artifact_name=\"phcd_paper_splits_tfds\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "df5f531b08a468c0a2a3591ee4fa2c1150ed1bbbe823daf85795b21510fb4a25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
